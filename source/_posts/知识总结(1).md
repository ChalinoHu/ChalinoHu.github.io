---
title: 面试相关知识点总结(一)
date: 2022-05-27 21:39:13
tags: 面试
---



# C++：

## 1.static关键字作用：

- static作用于局部变量时，改变了局部变量的生存周期，会使该变量存在于定义后直到程序运行结束，
- static作用于全局变量和函数，改变了全局变量的作用域，使其只能在定义它的文件中使用
- static作用于类的成员变量和类的成员函数，可以不定义类的对象就可以通过类访问这些静态成员，同时类的静态成员函数只能访问静态成员变量或者静态成员函数
- static函数的函数名（地址）在其他文件中是不可见的， 无法直接调用。因为文件没法获取静态函数文件中函数的地址， 找到不到地址， 当然无从调用起。但是， 有歪门邪道的方法来调用：让文件中的其他函数来暴露（出卖）这个地址。
- **全局变量、文件域的静态变量和类的静态成员变量在main执行之前的静态初始化过程中分配内存并初始化；局部静态变量（一般为函数内的静态变量）在第一次使用时分配内存并初始化。这里的变量包含内置数据类型和自定义类型的对象。**

注意：

- 静态成员变量是在类内进行声明，在类外进行定义和初始化，在类外进行定义和初始化的时候不要出现 static 关键字，

- 静态成员变量相当于类域中的全局变量，被类的所有对象所共享，包括派生类的对象。

- 静态成员变量可以作为成员函数的参数可选参数，而普通成员变量不可以

  ```c++
  class base{ 
  public : 
      static int _staticVar; 
      int _var; 
      void foo1(int i=_staticVar);//正确,_staticVar为静态数据成员 
      void foo2(int i=_var);//错误,_var为普通数据成员 
  };
  ```

- 静态数据成员的类型可以是所属类的类型，而普通数据成员不可以，普通数据成员只可能声明成类的指针或引用

  ```c++
  class base{ 
  public : 
      static base _object1;//正确，静态数据成员 
      base _object2;//错误 
      base *pObject;//正确，指针 
      base &mObject;//正确，引用 
  };
  ```

#### 2.C 和 C++ 区别

- C是面向过程的编程，特点是函数，C++是面向对象的编程，特点是类

- C主要用于嵌入式开发，驱动开发和硬件直接打交道的领域；而C++可以用于应用层的开发，用户界面开发和操作系统打交道领域。

- C++继承了C的底层操作特性，增加了面向对象的机制，增加了泛型编程，异常处理，运算符重载等新特性

  ##### 面向对象的三大特性：

  - 封装：将具体的实现过程和数据封装成一个函数，只能通过接口访问
  - 继承：子类继承父类的特征和行为，子类有父类的非私有方法或成员变量，子类也可对父类的方法进行重写。
  - 多态：不同继承类的对象，对同一消息做出不同响应，基类的指针指向或绑定到派生类的对象，使得基类指针呈现不同表现

#### 3.四种cast转换

C中可以支持任意类型之间的转换，并且不容易区分，且不容易查找

- static_cast静态转换：最常用，但是转换不会检查类型来保证转换安全性，且不能转换掉const，volitale特性，上行转换时安全的，下行转换不安全。
- dynamic_cast动态转换：下行转换时，会首先检查是否转化成功，转化失败会返回0，对于引用还会抛出异常。只能用于含虚函数的类
- reinterpret_cast：重新解释并不是真的转换类型，用于低级转型，基本什么都可以转，但是容易出问题
- const_cast 去掉底层const属性，将const变量转化成非const

这里上行转换和下行转换安全性的解释：

```
class Base
{
    virtual void fun(){}
};

class Derived:public Base
{
};
```

由于需要进行向下转换，因此需要定义一个 **父类类型的指针Base \*P** ，但是由于子类继承与父类，父类指针可以指向父类对象，也可以指向子类对象，这就是重点所在。如果 **P** 指向的确实是子类对象，则dynamic_cast和static_cast都可以转换成功，如下所示：

```
Base *P = new Derived();
Derived *pd1 = static_cast<Derived *>(P);
Derived *pd2 = dynamic_cast<Derived *>(P);
```

以上转换都能成功。

但是，如果 **P** 指向的不是子类对象，而是父类对象，如下所示：

```
Base *P = new Base;
Derived *pd3 = static_cast<Derived *>(P);
Derived *pd4 = dynamic_cast<Derived *>(P);
```

在以上转换中，static_cast转换在编译时不会报错，也可以返回一个子类对象指针（假想），但是这样是不安全的，在运行时可能会有问题，因为子类中包含父类中没有的数据和函数成员，这里需要理解转换的字面意思，转换是什么？转换就是把对象从一种类型转换到另一种类型，如果这时用 pd3 去访问子类中有但父类中没有的成员，就会出现访问越界的错误，导致程序崩溃。而dynamic_cast由于具有运行时类型检查功能，它能检查P的类型，由于上述转换是不合理的，所以它返回NULL。

#### 4.C++/C 指针和引用的区别

- 指针是一个变量，不过存储的是一个地址，指向内存的存储单元，而引用和变量实质是一个东西，不过是一个别名而已
- 可以有const指针，没有const引用
- 指针可以有多级
- 指针可以为空，而引用必须初始化
- 指针初始化后可以改变
- sizeof 引用得到是所指向对象大小，而指针的到指针本身大小
- 指针和引用自增意义不一样，指针是指针运算，而引用是引用对象
- 作为函数参数传递也不一样，指针是作为参数是通过地址进行改变，而引用参数传递传递的是实参本身不是拷贝所以节约时间还可以节约空间
- 引用的底层是一个指针常量，指向这个单元就不再指向别处，且引用由编译器保证初始化
- 引用不能为空（NULL），引用必须与合法的存储单元关联，指针则可以是NULL

#### 5.C++四种智能指针，内部如何实现

- auto_ptr 会因为潜在的内存问题导致程序崩溃，当两个智能指针都指向同一个堆空间时，每个智能指针都会delete一下这个堆空间，这会导致未定义行为。

- unique_ptr

  资源只能被一个指针占有，该指针不能拷贝构造和赋值

- shared_ptr：

  资源可以被多个指针共享，使用计数机制表明资源被几个指针共享。通过use_count查看资源的所有者的个数，当该变量的值变为0后，会自动释放内存空间，从而避免了内存泄漏。

- weak_ptr

  指向share_ptr指向的对象，能够解决由shared_ptr带来的循环引用问题

- 实现：

```C++
#include 
#include 

template 
class SmartPtr
{
private : T *_ptr;
    size_t *_count;

public:
    SmartPtr(T *ptr = nullptr) : _ptr(ptr)
    {
        if (_ptr)
        {
            _count = new size_t(1);
        }
        else
        {
            _count = new size_t(0);
        }
    }

    ~SmartPtr()
    {
        (*this->_count)--;
        if (*this->_count == 0)
        {
            delete this->_ptr;
            delete this->_count;
        }
    }

    SmartPtr(const SmartPtr &ptr) //拷贝构造
    {
        if (this != &ptr)
        {
            this->_ptr = ptr._ptr;
            this->_count = ptr._count;
            (*this->_count)++;
        }
    }

    SmartPtr &operator=(const SmartPtr &ptr) //赋值运算符重载
    {
        if (this->_ptr == ptr._ptr)
        {
            return *this;
        }
        if (this->_ptr)
        {
            (*this->_count)--;
            if (this->_count == 0)
            {
                delete this->_ptr;
                delete this->_count;
            }
        }
        this->_ptr = ptr._ptr;
        this->_count = ptr._count;
        (*this->_count)++;
        return *this;
    }

    T &operator*()
    {
        assert(this->_ptr == nullptr);
        return *(this->_ptr);
    }

    T *operator->()
    {
        assert(this->_ptr == nullptr);
        return this->_ptr;
    }

    size_t use_count()
    {
        return *this->count;
    }
};
```

#### 6.智能指针内存泄露问题和如何解决

[http://senlinzhan.github.io/2015/04/24/%E6%B7%B1%E5%85%A5shared-ptr/](http://senlinzhan.github.io/2015/04/24/深入shared-ptr/)

weak_ptr如何解决shared_ptr的循环引用问题，通过`shared_ptr`创建的两个对象，同时它们的内部均包含`shared_ptr`指向对方。

- 循环引用：该被调用的析构函数没有被调用，从而出现了内存泄漏

- `main`函数退出之前，`Father`和`Son`对象的引用计数都是`2`。

- `son`指针销毁，这时`Son`对象的引用计数是`1`。

- `father`指针销毁，这时`Father`对象的引用计数是`1`。

- 由于`Father`对象和`Son`对象的引用计数都是`1`，这两个对象都不会被销毁，从而发生内存泄露。

- 为避免循环引用导致的内存泄露，就需要使用`weak_ptr`，`weak_ptr`

  并不拥有其指向的对象，也就是说，让`weak_ptr`
  
  指向`shared_ptr`所指向对象，对象的引用计数并不会增加：

  - 使用`weak_ptr`就能解决前面提到的循环引用的问题，方法很简单，只要让`Son`或者`Father`包含的`shared_ptr`改成`weak_ptr`就可以了。
  - `main`函数退出前，`Son`对象的引用计数是`2`，而`Father`的引用计数是`1`。
  - `son`指针销毁，`Son`对象的引用计数变成`1`。
  - `father`指针销毁，`Father`对象的引用计数变成`0`，导致`Father`对象析构，`Father`对象的析构会导致它包含的`son_`指针被销毁，这时`Son`对象的引用计数变成`0`，所以`Son`对象也会被析构。
  - 然而，`weak_ptr`并不是完美的，因为`weak_ptr`不持有对象，所以不能通过`weak_ptr`去访问对象的成员，例如：

#### 7.野指针

访问一个已销毁或者访问受限内存区域的指针，而且不能通过是否为空来避免

##### 原因：

- 指针定义时没有初始化
- 指针释放没有置空
- 指针操作超越变量作用域

##### 规避方法：

- 初始化时将其置为nullptr
- 释放的时候将其置为nullptr

##### 检测工具：

Valgrind软件

#### 8.为什么析构函数必须是虚函数，而默认的析构函数却不是虚函数

- 将可能被继承的父类的析构函数设置为虚函数，可以保证我们new一个子类时，基类指针指向该子类对象，释放该基类指针同时可以释放掉子类空间，防止内存泄露
- 因为虚函数需要额外的虚函数表和虚表指针，占用一定内存空间

#### 9.静态函数和虚函数区别

- 静态函数在编译的时候就已经确定运行时机，虚函数在运行的时候动态绑定。虚函数因为用了虚函数表机制，调用的时候会增加一次内存开销
- 虚函数表：

```C++
class A {
public:
    virtual void vfunc1();
    virtual void vfunc2();
    void func1();
    void func2();
private:
    int m_data1, m_data2;
};
```

​	虚表图：虚表的构建在编译期间就会完成



![img](https://img-blog.csdnimg.cn/20210106133938903.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpaGFvMjE=,size_16,color_FFFFFF,t_70#pic_center)



为了指向对象的虚表，对象内部包含一个虚表指针*_vptr来指向自己所使用的虚表



![img](https://img-blog.csdnimg.cn/20210107204916729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpaGFvMjE=,size_16,color_FFFFFF,t_70#pic_center)



动态绑定即为对象通过虚表调用虚函数，表现出来的是运行时多态，而传统的函数调用称之为静态绑定，即函数调用在编译阶段就确定下来了

而多继承环境下基类的多个派生类所继承的相同的虚函数会放到第一个继承的派生类虚函数表中，此外的派生类则会根据偏移量来寻找虚函数地址。

#### 10.重载和覆盖（重写）



![img](https://img-blog.csdnimg.cn/20200815222458820.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDgyNjM1Ng==,size_16,color_FFFFFF,t_70#pic_center)



- 重载：名称相同而参数形式不同

- 重写：一个方法实现不同的功能，用于子类继承父类重写父类非私有方法，参数列表，返回等必须一致，静态方法不能重写成非静态方法

- 重载是怎么解决冲突的：

  上面的研究都是基于g++编译器，如果用的是vs编译器的话，对应关系跟这个肯定不一样。但是规则是一样的：“**返回类型**+**函数名**+**参数列表**”。

#### 11.函数指针是什么

指向函数可用于代替函数

```C++
int (*p)(int, int) = &max
```

#### 12.fork函数

应用场景：

一种可能见到的场景是在服务器程序中，一个请求到来后，为了避免服务器阻塞，fork出一个子进程处理请求，父进程仍然继续等待请求到来。但这种方式无疑开销会稍大。

另一种最常见的就是执行一个不同的程序，例如我们在shell终端执行一条命令，实际上就是bash（或者其他）调用fork之后，在执行exec族函数。

流程：

如果调用成功，它将返回两次，子进程返回值是0；父进程返回的是非0正值，表示子进程的进程id；如果调用失败将返回-1，并且置errno变量。

创建一个和当前进程映像一样的进程，*子进程*会继承父进程的资源，包括文件设备访问权限、文件描述符等，可再通过exec载入二进制映像，替换当前进程的映像，以往的fork会将所有内核数据结构复制一次，现在会采用写时复制，有需要才会复制

特点：

- fork调用一次，返回两次
- 一个进程可以有多个子进程，但同一时刻最多只有一个父进程
- 子进程继承了父进程很多属性
- 父子进程执行的先后顺序不一定

#### 13.析构函数作用

当对象结束其生命周期，系统会调用，如果没有编写，编译器一般也会自动生成一个析构函数，析构的顺序是：1派生类 2对象成员 3基类析构函数

#### 14.隐式类型转换

- 算术类型转换：1.整形提升 2.符号类型转化为无符号 3.条件判断中，非布尔转化为布尔

- 类类型转换：转换构造函数或者类型转换函数

  转换构造函数：string s = “” 是通过stringconstchar∗sconstchar∗s

  类型转换函数：operator int {return _x};

#### 15.extern”C”，底层是如何实现的

编译器识别后会将这部分代码按C语言进行编译，实现了C++代码调用其他C语言代码

#### 16.new/delete 和 malloc/free区别

- malloc/free是库函数，需要头文件支持，而new/delete是关键字，需要编译器支持
- new申请空间时，无需指定分配空间大小，编译器会根据类型自行计算，malloc则需确定所申请空间大小
- 返回值：new返回对象是对象的指针类型，无需强制类型转换，而malloc返回void*类型，需要转换
- 分配失败时new返回bad_alloc，malloc返回空指针
- new/delete支持重载，malloc/free不能
- new从自由存储区为对象分配内存，而malloc从堆上动态分配
- new可以自定义类型实现，先调用operator new申请空间底层malloc实现底层malloc实现，然后构造函数初始化，最后返回指针，delete首先调用析构函数，然后调用operator delete释放空间底层free实现底层free实现，而malloc/free不可以自定义类型的对象构造和析构

#### 17.select/poll/epoll， epoll的两种模式

- select==>时间复杂度On

  它仅仅知道了，有I/O事件发生了，却并不知道是哪那几个流（可能有一个，多个，甚至全部），我们只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。所以**select具有On的无差别轮询复杂度**，同时处理的流越多，无差别轮询时间就越长。

- poll==>时间复杂度On

  poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态， **但是它没有最大连接数的限制**，原因是它是基于链表来存储的.

- epoll==>时间复杂度O1

  **epoll可以理解为event poll**，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知我们。所以我们说epoll实际上是**事件驱动（每个事件关联上fd）**的，此时我们对这些流的操作都是有意义的。**（复杂度降低到了O1）**

  具体为通过epoll_ wait采用回调，每当检测就绪文件描述符就触发回调，回调函数就将该文件描述符插入内核就绪队列，内核最后在合适时机将就绪事件队列拷贝到用户空间。

- select，poll，epoll都是IO多路复用的机制。I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。**但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的**，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。

- epoll跟select都能提供多路I/O复用的解决方案。在现在的Linux内核里有都能够支持，其中epoll是Linux所特有，而select则应该是POSIX所规定，一般操作系统均有实现

- **select：**

  select本质上是通过设置或者检查存放fd标志位的数据结构来进行下一步处理。这样所带来的缺点是：

  1、 单个进程可监视的fd数量被限制，即能监听端口的大小有限。

  一般来说这个数目和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认是1024个。64位机默认是2048.

  2、 对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低：当套接字比较多的时候，每次select都要通过遍历FD_SETSIZE个Socket来完成调度,不管哪个Socket是活跃的,都遍历一遍。这会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，那就避免了轮询，这正是epoll与kqueue做的。

  3、需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大

- **poll：**

  poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。这个过程经历了多次无谓的遍历。

  **它没有最大连接数的限制**，原因是它是基于链表来存储的，但是同样有一个缺点：

  1、大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。

  2、poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。

- **epoll:**

  epoll对文件描述符有EPOLLLT和EPOLLET两种触发模式，LT是默认的模式，ET是“高速”模式。

  - **LT模式**：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，`应用程序可以不立即处理该事件`。下次调用epoll_wait时，会再次响应应用程序并通知此事件。

    LTleveltriggeredleveltriggered是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。

  - **ET模式**：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，`应用程序必须立即处理该事件`。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。

    ETedge−triggerededge−triggered是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了EWOULDBLOCK错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了EWOULDBLOCK错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪，内核不会发送更多的通知onlyonceonlyonce

  1、**没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）**；
  **2、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数；**
  **即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。**

  3、 内存拷贝，利用mmap文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。

  具体流程：

  - 调用epoll_create函数创建一个epoll句柄结束之后要记得关闭epoll句柄结束之后要记得关闭epoll句柄.
  - 调用epoll_ctl函数将监控的文件描述符进行处理.
  - 调用epoll_wait函数,等待就绪的文件描述符.
  - 总体的工作原理：当我们调用epoll_create系统函数时，会在内核中建一颗红黑树，红黑树的节点就是调用epoll_ctl系统函数时管理的需要监控的事件。此外，还会创建一个链表（即就绪队列），就是用来存储就绪的事件。而当调用epoll_wait系统函数时，就仅仅只需要查看该链表之中有没有数据，有就会将链表上的数据从内核态拷贝到用户态，同时将事件的数量返回给用户，这个事件复杂度是O（1）。若没有就会立即返回，然后继续的等待，若等待的时间超过了timeout也会立即返回。
    问题：那么，内核是怎么维护这个链表（就绪队列）？
    这又涉及到了硬件设备，当调用epoll_ctl进行注册事件时，会做2件事：一是将需要注册的事件挂载到红黑树之中，二是也给每个事件注册一个回调函数，建立一种回调关系。当有事件进入就绪后，就会触发网卡驱动程序发出中断，将对应的事件从红黑树中找到并添加到链表之中。这都是在内核中并结合硬件来实现的，无疑是非常之高效。

#### 18.C++访问权限

public private protected

#### 19.struct和Class区别

- struct中默认访问级别是Public，默认的继承级别也是Public
- class中默认的访问级别是private，默认的也是
- 当class继承struct或者struct继承class时，默认的继承级别取决于calss或struct本身

#### 20.初始化列表的作用

- 可以应对必须使用列表初始化的场合，例如有常量，引用，和没有operator=的类

- 效率较高，省去了调用默认构造函数的过程，直接调用拷贝构造函数

  省掉了创建构造临时对象的过程

- 初始化列表顺序，先初始化先定义的变量

#### 21.C++11新特性，C++20新特性

##### C++11

- 基于范围的for循环

- 自动推断auto decltype

- 匿名函数lambda，内部原理：

  原理：**编译器会把一个lambda表达式生成一个匿名类的匿名对象，并在类中重载函数调用运算符。**

  我们从最简单的lambda表达式入手，从易到难

  ### 2.1 无捕获列表和参数列表

  ```cpp
  auto print = []{cout << "zhangxiang" << endl; };
  ```

  编译器会把这一句翻译成如下情形：

  ```cpp
  //用给定的lambda表达式生成相应的类
  class print_class
  {
  public:
      void operator()(void) const
      {
          cout << "zhangxiang" << endl;
      }
  };
  //用构造的类创建对象，print此时就是一个函数对象
  auto print = print_class();
  ```

  生成类的类名命名规则可以多变，不一定非得这样。

  ### 2.2 无捕获列表但有参数列表

  ```cpp
  auto add = [](int a, int b){return a + b; };
  ```

  编译器会把这一句翻译成如下情形：

  ```cpp
  class add_class
  {
  public:
      auto operator()(int a, int b) const
      {
          return a + b;
      }
  };
  auto add = add_class();
  ```

- 后置返回类型

  auto getSuminta,intbinta,intb->int;

- 空指针常量nullptr

- long long int

- 线程支持

- 元组类型和array

  array 保存在栈内存中，相比堆内存中的vector，我们就能够灵活的访问元素，获得更高的性能；同时真是由于其堆内存存储的特性，有些时候我们还需要自己负责释放这些资源。

  tuple区分于pair，可以任意种类型闭包，用get<>来寻找,但是get内只能用常量

  tuple遍历：

  ```cpp
  template<class Tuple, std::size_t N>
  struct TuplePrinter { 
    static void print(const Tuple& t) 
    { 
      TuplePrinter<Tuple, N - 1>::print(t); 
      std::cout << ", " << std::get<N - 1>(t); 
    }
  };
  
  template<class Tuple>
  struct TuplePrinter<Tuple, 1>{ 
    static void print(const Tuple& t) 
    { 
      std::cout << std::get<0>(t); 
    }
  };
  
  template<class... Args>
  void PrintTuple(const std::tuple<Args...>& t)
  { 
    std::cout << "(";
    TuplePrinter<decltype(t), sizeof...(Args)>::print(t); 
    std::cout << ")\n";
  }
  ```

- 智能指针

- using代替typedef

  using可以用来更简便重定义模板

  1. **template** <**typename** Val>
  2. **using** str_map_t = std::map<std::string, Val>;

- 右值引用 std::move

- 移动构造函数和移动赋值，不用构造临时对象从而提高效率

- constexpr

  constexpr可以用来修饰变量、函数、构造函数。一旦以上任何元素被constexpr修饰，那么等于说是告诉编译器 “请大胆地将我看成编译时就能得出常量值的表达式去优化我”。

- 类型萃取

  提供了编译期计算，查询，判断，转换和选择的帮助类

  type_trits的类型选择功能，在一定程度上可以消除冗长的switch-case或者if-else的语句，降低程序的复杂程度。

- 可变模板参数

  递归调用函数，形参由多至少，一般采用递归方式使用，可参照tuple遍历方式

  ```
  template <class... T>
  ```

##### C++17

结构化绑定： forauto& [k, v] : hashauto& [k, v] : hash

##### C++ 20

- 概念（concept）

  可以方式模板参数不满足鸭子类型（通过对象方法决定类型）的约束时编译器不再报错

- 模块的使用

  不再需要头文件，模块只处理一次，编译更快，并提供import 头文件的方式，将其自动转化为一个模块。

  ```
  concept Addable = requires (T obj) {{obj + obj}->std::same_as<t>;};
  ```


    上述代码的含义即是，就是T类型应该支持加法运算，而且结果类型还是T。

  * 协程

    C++20所提供的协程标准只包含编译器需要实现的底层功能，面向的是库的开发者

    * 解决的问题：

      解决异步编程的麻烦，避免传统线程池+回调模式的复杂性

      * 同步：多个对象具有相对应的关系，其中play为阻塞调用，程序会等待这些事情完成再继续执行接下来步骤

      ```
      function AWeek() {
        // 同步的代码，同步的方式
        print('Monday')
        play_music() // block call
        print('Tuesday')
        browse_moment() // block call
        print('Wednesday')
      }
      ```
      
      
      
      * 异步：asynchronous，处理完成之前就返回调用方法，包括异步IO，异步通信等
      
          * ![img](https://pic3.zhimg.com/80/v2-76750bd11daffa1e29327f2c18c354fe_720w.jpg)
      
        所以异步就是代码还是按照一定序列执行，但中间穿插其他任务，逻辑顺序一致，而协程(Coroutine)就是函数,只不过可以暂停这个函数执行其他的事，再恢复回来
      
        ![preview](https://pic3.zhimg.com/v2-611e746cfd1f80f75c7a58d5104dbc0a_r.jpg)
      
        C++20 是无栈协程，相对于有栈协程，省掉上下文开销，智能手动切换，且不用管理复杂寄存器状态，同时也不能被非协程函数嵌套调用
      
        引入了3个关键字
      
        1.co_yield ：挂起并返回
      
        2.co_await ：挂起
      
        3.co_return ：结束
      
        包含的一些对象：
      
        * promise：当调用协程时，协程自身状态信息（形参，局部变量，自带数据，各个阶段执行点）会保存到堆上的Promise对象中，可在编译器计算出来。
        * Future：与Promise交互的桥梁，即caller(调用者)和callee(自身)的通信
        * Awaitable：可以用co_await 操作符触发动作，从而转移，恢复控制权。

  #### 22.左值引用和右值引用

  * ##### 左值引用：

  ```C++
  int a = 10;
  int &b = a;  // 定义一个左值引用变量
  b = 20;      // 通过左值引用修改引用内存的值
  ```

  左值引用在汇编层面其实和普通的指针是一样的；定义引用变量必须初始化，因为引用其实就是一个别名，需要告诉编译器定义的是谁的引用。

  ```C++
  int &var = 10;
  ```

  上述代码是无法编译通过的，因为10无法进行取地址操作，无法对一个立即数取地址，因为立即数并没有在内存中存储，而是存储在寄存器中，可以通过下述方法解决：

  ```C++
  const int &var = 10;
  ```

  使用常引用来引用常量数字10，因为此刻内存上产生了临时变量保存了10，这个临时变量是可以进行取地址操作的，因此var引用的其实是这个临时变量，相当于下面的操作：

  ```C++
  const int temp = 10; 
  const int &var = temp;
  ```

  根据上述分析，得出如下结论：

  左值引用要求右边的值必须能够取地址，如果无法取地址，可以用常引用；
  但使用常引用后，我们只能通过引用来读取数据，无法去修改数据，因为其被const修饰成常量引用了。

  - ##### 右值引用

    ```text
    类型 && 引用名 = 右值表达式;
    ```

    右值引用是C++ 11新增的特性，所以C++ 98的引用为左值引用。右值引用用来绑定到右值，绑定到右值以后本来会被销毁的右值的生存期会延长至与绑定到它的右值引用的生存期。

    ```C++
    int &&var = 10;
    ```

    在汇编层面右值引用做的事情和常引用是相同的，即产生临时量来存储常量。但是，唯一 一点的区别是，右值引用可以进行读写操作，而常引用只能进行读操作。

    右值引用的存在并不是为了取代左值引用，而是充分利用右值(特别是临时对象)的构造来减少对象构造和析构操作以达到提高效率的目的。

  std::forward只有在它的参数绑定到一个右值上的时候，才转换它的参数到一个右值。

  std::move和std::forward本质都是转换。std::move执行到右值的无条件转换。std::forward只有在它的参数绑定到一个右值上的时候，才转换它的参数到一个右值。

  #### 23.auto  和 decltype，auto有哪些应用场景

  auto 编译器分析表达式的类型，auto的实现原理也是基于模板类型推断

  使用场景：变量的数据类型复杂或者不确定函数返回值类型

  有时候我们希望从表达式中推断出要定义变量的类型，但却不想用表达式的值去初始化变量。还有可能是函数的返回类型为某表达式的的值类型。在这些时候auto显得就无力了。

  所以C++11又引入了第二种类型说明符decltype，它的作用是选择并返回操作数的数据类型。在此过程中，编译器只是分析表达式并得到它的类型，却不进行实际的计算表达式的值。

  #### 24.C++编译过程

  * 编译预处理：处理以 # 开头的指令

    - 将所以#define删除，并**将宏定义展开**。
    - **处理一些条件预编译指令**如#ifndef,#ifdef,#elif,#else,#endif等。将不必要的代码过滤掉。
    - **处理#include预编译指令**，将被包含的文件插入到该预编译指令的位置。这个过程是递归进行的，因为被包含的文件可能也包含其他文件。
    - 预处理过程还会**过滤掉所有注释**/**/和//里面的内容。
    - 另外还会添加行号和文件名标识。
    - 最后会保留#pragma编译器指令，因为编译器需要使用它们。
  * 编译、优化：将源码 .cpp 文件翻译成 .s 汇编代码
  * 汇编：将汇编代码 .s 翻译成机器指令 .o 文件，机器指令（Machine Instructions）是CPU能直接识别并执行的指令，它的表现形式是二进制编码。 机器指令通常由操作码和操作数两部分组成，操作码指出该指令所要完成的操作，即指令的功能，操作数指出参与运算的对象，以及运算结果所存放的位置等。
  * 链接：汇编程序生成的目标文件并不会立即执行，可能有源文件中的函数引用了另一个源文件中定义的符号或者调用了某个库文件中的函数。那链接的目的就是将这些目标文件连接成一个整体，从而生成可执行的程序 .exe 文件。
  * 静态链接：代码从其所在的动态链接库中拷贝到最终的可执行程序中，在该程序被执行时，这些代码会被装入到该进程的虚拟地址空间中
  * 动态链接：代码被放到动态链接库或共享对象的某个目标文件中，链接程序只是在最终的可执行程序中记录了共享对象的名字等一些信息。在程序执行时，动态链接库的全部内容会被映射到运行时相应进行的虚拟地址的空间
  * 二者的优缺点：
    * 静态链接 浪费空间，每个可执行程序都会有目标文件的一个副本，这样如果目标文件进行了更新操作，就需要重新进行编译链接生成可执行程序（更新困难）；优点就是执行的时候运行速度快，因为可执行程序具备了程序运行的所有内容
    * 动态链接：节省内存、更新方便，但是动态链接是在程序运行时，每次执行都需要进行链接，性能会有一定的损失。

  #### 25.include <> 和 ""的区别

  * <>编译器会在系统文件目录下查找
  * 而“ ”编译器首先会在用户目录下查找，然后才会到C++安装目录找

  #### 26.C++内存分布  和 内存管理

  * C++ 内存分区：栈、堆、自由存储区、全局/静态存储区、常量区
    * 栈：存放函数的局部变量，由编译器自动分配和释放
    * 堆：动态申请的内存空间，就是由 malloc 分配的内存块，由程序员控制它的分配和释放，如果程序执行结束还没有释放，操作系统会自动回收
    * 自由存储区：和堆十分相似，存放由 new 分配的内存块，由 delete 释放内存
    * 全局区/静态区：存放全局变量和静态变量
    * 常量存储区：存放的是常量，不允许修改
  * 堆和自由存储区的区别：
    * 自由存储是 C++ 中通过 new 与 delete 动态分配和释放对象的抽象概念，而堆是 C 语言和操作系统的术语，是操作系统维护的一块动态分配内存
    * new 所申请的内存区域在 C++ 中成为自由存储区。藉由堆实现的自由存储，可以说 new 所申请的内存区域在堆上
    * 堆和自由存储区有区别，并非等价。使用 new 来分配内存，程序员也可以通过重载操作符，改用其他内存来实现自由存储，例如全局变量做的对象池，这时自由存储区就区别于堆了。

  #### 27.malloc实现原理

  C/C++分配内存函数为： malloc，tcmalloc，jemalloc

  linux向用户提供申请的内存有brk（sbrk）和mmap函数

  * brk，sbrk 扩展heap的上界，第一个函数参数设置为新的brk上界地址，成功返回0，失败返回-1。第二个函数参数为需要的内存大小然后返回heap新的上界brk地址。如果sbrk的参数为0，则返回的为原来的brk地址。

    ```C
    #include <unistd.h>
    int brk( const void *addr )
    void* sbrk ( intptr_t incr );
    
    ```

  * mmap函数

    ```C
    #include <sys mman.h="">
    void *mmap(void *addr, size\_t length, int prot, int flags, int fd, off\_t offset);
    int munmap(void *addr, size_t length);
    ```

    mmap函数第一种用法是映射磁盘文件到内存中；而malloc使用的mmap函数的第二种用法，即匿名映射，匿名映射不映射磁盘文件，而是向映射区申请一块内存。munmap函数是用于释放内存，第一个参数为内存首地址，第二个参数为内存的长度。接下来看下mmap函数的参数。

    在说mmap之前我们先说一下普通的读写文件的原理，进程调用read或是write后会陷入内核，因为这两个函数都是系统调用，进入系统调用后，内核开始读写文件，假设内核在读取文件，内核首先把文件读入自己的内核空间，读完之后进程在内核回归用户态，内核把读入内核内存的数据再copy进入进程的用户态内存空间。实际上我们同一份文件内容相当于读了两次，先读入内核空间，再从内核空间读入用户空间。

    Linux提供了内存映射函数mmap, 它把文件内容映射到一段内存上(准确说是虚拟内存上), 通过对这段内存的读取和修改, 实现对文件的读取和修改,mmap()系统调用使得进程之间可以通过映射一个普通的文件实现共享内存。普通文件映射到进程地址空间后，进程可以向访问内存的方式对文件进行访问，不需要其他系统调用(read,write)去操作。

    ![mmap映射区和shm共享内存的区别总结1](https://res-static.hc-cdn.cn/fms/img/e1688f893feb27a2f4b7827271d9cb331603764805798)

    * prot:期望的内存保护标志,不能与文件的打开模式冲突。是以下的某个值,可以通过 or 运算合理地组合在一起。PROT_EXEC(页内容可以被执行);PROT_READ(页内容可以被读取);PROT_WRITE(页可以被写入);PROT_NONE(页不可访问).

    * flags:指定映射对象的类型,映射选项和映射页是否可以共享。它的值可以是一个或者多个以下位的组合体

    - MAP_FIXED //使用指定的映射起始地址，如果由start和len参数指定的内存区重叠于现存的映射空间，重叠部分将会被丢弃。如果指定的起始地址不可用，操作将会失败。并且起始地址必须落在页的边界上。

  - MAP_SHARED //与其它所有映射这个对象的进程共享映射空间。对共享区的写入，相当于输出到文件。直到msync()或者munmap()被调用，文件实际上不会被更新。

    - MAP_PRIVATE //建立一个写入时拷贝的私有映射。内存区域的写入不会影响到原文件。这个标志和以上标志是互斥的，只能使用其中一个。
    - MAP_LOCKED //锁定映射区的页面，从而防止页面被交换出内存。
    - MAP_ANONYMOUS //匿名映射，映射区不与任何文件关联。

    * fd为映射的文件，如果是匿名映射，可以设为-1；

    * offset为被映射文件内容的起点偏移；

  当申请小内存的时，malloc使用sbrk分配内存；当申请大内存时，使用mmap函数申请内存；但是这只是分配了虚拟内存，还没有映射到物理内存，当访问申请的内存时，才会因为缺页异常，内核分配物理内存。

  * 但是由于brk/sbrk/mmap属于系统调用，如果每次申请内存，都调用这三个函数中的一个，那么每次都要产生系统调用开销，这是非常影响性能的；其次，这样申请的内存容易产生碎片，因为堆是从低地址到高地址，如果低地址的内存没有被释放，高地址的内存就不能被回收。

  * 鉴于此，malloc采用的是内存池的实现方式，malloc内存池实现方式更类似于STL分配器和memcached的内存池，先申请一大块内存，然后将内存分成不同大小的内存块，然后用户申请内存时，直接从内存池中选择一块相近的内存块即可。

  * malloc函数的实质体现在：它有一个将可用的内存块连接为一个长长的列表的所谓空闲链表。调用malloc函数时，它沿`连接表`寻找一个大到足以满足用户请求所需要的内存块。然后，将该内存块一分为二（一块的大小与用户请求的大小相等，另一块的大小就是剩下的字节）。接下来，将分配给用户的那块内存传给用户，并将剩下的那块（如果有的话）返回到连接表上。

  * 这里注意，malloc找到的内存块大小一定是会大于等于我们需要的内存大小，下面会提到如果所有的内存块都比要求的小会怎么办？

    调用free函数时，它将用户释放的内存块连接到空闲链上。到最后，空闲链会被切成很多的小内存片段，如果这时用户申请一个大的内存片段，那么空闲链上可能没有可以满足用户要求的片段了。于是，malloc函数请求延时，并开始在空闲链上翻箱倒柜地检查各内存片段，对它们进行整理，**将相邻的小空闲块合并成较大的内存块**

    glibc维护了`不止一个`不定长的内存块链表，而是好几个，每一个这种链表负责一个大小范围，这种做法有效**减少了分配大内存时的遍历开销**，类似于`哈希`的方式，将很大的范围的数据散列到有限的几个小的范围内而不是所有数据都放在一起，虽然最终还是要在小的范围内查找，但是最起码省去了很多的开销，如果只有一个不定长链表那么就要全部遍历，如果分成3个，就省去了2/3的开销，总之这个策略十分类似于散列。

    glibc另外的策略就是不止维护一类空闲链表，而是另外再维护一个缓冲链表和一个高速缓冲链表，在分配的时候首先在高速缓存中查找，失败之后再在空闲链表查找，如果找到的内存块比较大，那么将切割之后的剩余内存块插入到缓存链表，如果空闲链表查找失败那么就往缓存链表中查找. 如果还是没有合适的空闲块，就向内存申请比请求数更大的内存块，然后把剩下的内存放入链表中。

    在对内存块进行了 free 调用之后，我们需要做的是诸如将它们标记为未被使用的等事情，并且，在调用 malloc 时，我们要能够定位未被使用的内存块。因此， **malloc返回的每块内存的起始处首先要有这个结构**：

    这就解释了，为什么在程序中free之后，但是堆的内存还是没有释放。

    ```
    内存控制块结构定义
    struct mem_control_block {
        int is_available;
        int size;
    };
    ```

    现在，您可能会认为当程序调用 malloc 时这会引发问题 —— 它们如何知道这个结构？答案是它们不必知道；在返回指针之前，我们会将其移动到这个结构之后，把它隐藏起来。这使得返回的指针指向没有用于任何其他用途的内存。那样，从调用程序的角度来看，它们所得到的全部是空闲的、开放的内存。然后，当通过 free() 将该指针传递回来时，**我们只需要倒退几个内存字节就可以再次找到这个结构**。

    delete识别要删除内容大小也是相同的原理，new[]会比申请时多一些字节，保存了数字大小。

    **new关键字在空间不足导致内存分配失败时：**

    会调用全局的new_handler类型的函数（包含在头文件中）来处理调用失败的情形，这样就可能在new_handler函数中有机会腾出足够的空间使得可以申请足够的空间，但C++编译器不知道具体怎么操作，所以就有了个默认处理，即在new_handler函数里抛出异常std::bad_alloc异常。
    new_handler的类型是：void (*new_handler)()，它是一个函数指针。
    由于编译器不知道如何整理内存 碎片，所以就可以自定义new_handler函数在不同环境中做不同的内存整理工作。

  #### 28.什么是内存泄露，内存泄露检查工具

  * 工具：Valgrind
  * 申请的内存没有被正确释放，导致后续程序这块内存被永远占用

  #### 29.volatile和explicit

  * 在多线程下的应用： 有些变量是用volatile关键字声明的。当两个线程都要用到某一个变量且该变量的值会被改变时，应该用volatile声明，该关键字的作用是**防止优化编译器把变量从内存装入CPU寄存器中。**如果变量被装入寄存器，那么两个线程有可能一个使用内存中的变量，一个使用寄存器中的变量，这会造成程序的错误执行。volatile的意思是**让编译器每次操作该变量时一定要从内存中真正取出，而不是使用已经存在寄存器中的值，**
  * **explicit关键字的作用就是防止类构造函数的隐式自动转换.**

  #### 30.define的作用缺点

  define在预编译期间执行，而using，typedef则是在编译期间。

  而且因为define是预编译期间替换，无类型检查，写错也不会发现

  #### 31.inline作用，优缺点

  注意，inline仅仅是对编译器的建议，最后是否真正内联看编译器

  * 优点：

    1）inline定义的内联函数，函数代码被放入符号表中，在使用时进行替换（像宏一样展开），效率很高。

    2）类的内联函数也是函数。编绎器在调用一个内联函数，首先会检查参数问题，保证调用正确，像对待真正函数一样，消除了隐患及局限性。

    3）inline可以作为类的成员函数，可以使用所在类的保护成员及私有成员。

    * (1) 隐喻式：定义在类中的成员函数缺省都是内联的，如果在类定义时就在类内给出函数定义，那当然最好。
    * (2) 明确声明：如果在类中未给出成员函数定义，而又想内联该函数的话，那在类外要加上inline，否则就认为不是内联的。

  * 缺点：

    内联函数以复制为代价，活动产函数开销

    1)如果函数的代码较长，使用内联将消耗过多内存

    2)如果函数体内有循环，那么执行函数代码时间比调用开销大。

  #### 32.strcpy存在什么安全问题？

  #### 手写strcpy并做优化。[源码](https://www.nowcoder.com/jump/super-jump/word?word=源码)为什么这么实现？

  * 一是若str1没有足够存储空间存'\0',无法确定strcpy跑到哪里

  * 二是假如不足，可能会覆盖其他空间，这个是致命的

    因此一般采用strncpy

    * strcpy源代码：

    ```C++
    char* strcpy(char *strSrc, char *strDest)
    {
    	if ((strSrc == NULL) || (strDest == NULL))
    		return NULL;
    	char *strDestCopy = strDest;
    	while ((*strDest++ = *strSrc++) != '\0');
    		return strDestCopy;
    }
    ```

    * strncpy代码：

      ```C
      char * strncpy(char *dest, const char *src, size_t copySize)
      {
        size_t size = __strnlen(src, copySize);
        if (size != copySize)
          memset(dest + size, '\0', copySize - size);
        return memcpy(dest, src, size);
      }
      ```

      strnlen()也是'GNU libc'里的库函数返回'strlen(src)'和'copySize'的最小值

  #### 33.memcpy存在的问题，memmove的问题。

  * memcpy存在问题：内存重叠可能会导致拷贝数据不正确，解决办法memmove(可以允许内存重叠)

  ```C++
  void *Memcpy(void *dst, const void *src, size_t size)
  {
      char *psrc;
      char *pdst;
      if(NULL == dst || NULL == src)
      {
          return NULL;
      }
      if((src < dst) && (char *)src + size > (char *)dst) // 自后向前拷贝（重叠了）
      {
          psrc = (char *)src + size - 1;
          pdst = (char *)dst + size - 1;
          while(size--)
          {
              *pdst-- = *psrc--;
          }
      }
      else
      {
          psrc = (char *)src;
          pdst = (char *)dst;
          while(size--)
          {
              *pdst++ = *psrc++;
          }
      }
      return dst;
  }
  ```

  * memset

    ```C
    void *(memset) (void *s,int c,size_t n)  
    {  
        const unsigned char uc = c;  
        unsigned char *su;  
        for(su = s;0 < n;++su,--n)  
            *su = uc;  
        return s;  
    }
    ```

    

  #### 34.C++和Java的区别

  * 二者在语言特性上有很大的区别：
    * 指针：C++ 可以直接操作指针，容易产生内存泄漏以及非法指针引用的问题；JAVA 并不是没有指针，虚拟机(JVM)内部还是使用了指针，只是编程人员不能直接使用指针，不能通过指针来直接访问内存，并且 JAVA 增加了内存管理机制
    * 多重继承：C++ 支持多重继承，允许多个父类派生一个类，虽然功能很强大，但是如果使用的不当会造成很多问题，例如：菱形继承；JAVA 不支持多重继承，但允许一个类可以继承多个接口，可以实现 C++ 多重继承的功能，但又避免了多重继承带来的许多不便
    * 数据类型和类：C++ 可以将变量或函数定义成全局，但是JAVA是完全面向对象的语言，除了基本的数据类型之外，其他的都作为类的对象，包括数组。
  * 垃圾回收：
    * JAVA 语言一个显著的特点就是垃圾回收机制，编程人员无需考虑内存管理的问题，可以有效的防止内存泄漏，有效的使用空闲的内存
    * JAVA 所有的对象都是用 new 操作符建立在内存堆栈上，类似于 C++ 中的 new 操作符，但是当要释放该申请的内存空间时，JAVA 自动进行内存回收操作，C++ 需要程序员自己释放内存空间，并且 JAVA 中的内存回收是以线程的方式在后台运行的，利用空闲时间。
  * 应用场景：
    * java 运行在虚拟机上，和开发平台无关，C++ 直接编译成可执行文件，是否跨平台在于用到的编译器的特性是否有多平台的支持
    * C++ 可以直接编译成可执行文件，运行效率比 JAVA 高
    * JAVA 主要用来开发 web 应用
    * C++ 主要用在嵌入式开发、网络、并发编程的方面

  #### 35.深拷贝和浅拷贝

  * 如果一个类拥有资源，该类的对象发生了复制，如果资源发生了重新分配，就是深拷贝，否则就是浅拷贝

  * 深拷贝：该对象和原对象占用不同的存储空间，即拷贝位于stack域（栈）中的内容，又拷贝类中位于heap域（堆）中的内容
  * 浅拷贝：该对象和原对象占用同一块内存区域，仅拷贝类中位于stack域（栈）中的内容
  * 当类的成员变量中有指针时，使用深拷贝安全，如果两个对象指向同一块内存空间，当其中一个对象的删除后，该块内存空间就会被释放，另外一个对象指向的就是垃圾内存。

  #### 36.哪些运算符不能重载：

  不能重载的运算符（只有5个）：

  * . 成员访问运算符 ，保证访问成员的功能不能被改变
  * .* 成员指针访问运算符，保证访问成员的功能不能被改变
  * :: 域运算符，运算对象是类型而不是变量
  * sizeof 长度运算符，运算对象是类型而不是变量或一般表达式，不具备重载特征
  * ?: 条件运算符，运算对象是类型而不是变量或一般表达式，不具备重载特征
    允许重载的运算符：

  ![在这里插入图片描述](https://pic.leetcode-cn.com/1607952042-BICAFT-file_1607952046148)

  #### 37.静态函数能否定义为虚函数

  * 静态成员函数不能被声明为虚函数：

    * 静态成员不属于任何类对象或类实例，所以即使加上virtual也没有任何意义
    * 静态与非静态成员函数之间一个最主要的区别就是：静态成员函数没有this指针。调用类中的虚函数时，是通过虚表以及指向虚表的指针vptr才能完成虚函数的调用，并且只能用this指针来访问。对于静态成员函数没有this指针，无法访问vptr.

  * 静态成员函数不能为const函数：

    * 当声明一个类（Test）的非静态成员函数为const时，this指针相当于Test const *，对于非const成员函数，this指针相当于Test *. 但是static成员函数没有this指针，所以用const来修饰static没有任何意义。（volatile的道理也是如此）

      

  #### 38.模板特化和模板偏特化

  https://harttle.land/2015/10/03/cpp-template.html

  #### 39.空类默认生成哪些函数：

  #### 40.条件编译ifdef和 ifndef作用：

  条件编译，如果已经生成了这个头文件，就不用再编译生成一次了

  ### 41.构造函数不能是虚函数

  1. 从vptr角度解释

     ​    虚函数的调用是通过虚函数表来查找的，而虚函数表由类的实例化对象的vptr指针(vptr可以参考C++的虚函数表指针vptr)指向，该指针存放在对象的内部空间中，需要调用构造函数完成初始化。如果构造函数是虚函数，那么调用构造函数就需要去找vptr，但此时vptr还没有初始化！

  2. 从多态角度解释
     虚函数主要是实现多态，在运行时才可以明确调用对象，根据传入的对象类型来调用函数，例如通过父类的指针或者引用来调用它的时候可以变成调用子类的那个成员函数。而构造函数是在创建对象时自己主动调用的，不可能通过父类的指针或者引用去调用。那使用虚函数也没有实际意义。
     　　在调用构造函数时还不能确定对象的真实类型（由于子类会调父类的构造函数）；并且构造函数的作用是提供初始化，在对象生命期仅仅运行一次，不是对象的动态行为，没有必要成为虚函数。

  ### 42.C和Python的区别

  1、语言类型不同。

  Python是一种动态类型语言，又是强类型语言。它们确定一个变量的类型是在您第一次给它赋值的时候。C 是静态类型语言，一种在编译期间就确定数据类型的语言。大多数静态类型语言是通过要求在使用任一变量之前声明其数据类型来保证这一点的。

  2、对象机制不同。

  Python中所有的数据，都是由对象或者对象之间的关系表示的，函数是对象，字符串是对象，每个东西都是对象的概念。每一个对象都有三种属性： 实体，类型和值。

  C中没有对象这个概念，只有“数据的表示”，比如说，如果有两个int变量a和b，想比较大小，可以用a ＝＝ b来判断，但是如果是两个字符串变量a和b，就不得不用strcmp来比较了，因为此时，a和b本质上是指向字符串的指针，如果直接用＝＝比较， 那比较的实际是指针中存储的值地址。

  3、变量类型不同。

  python六个标准的数据类型：Number数字、String字符串、List列表、Tuple元祖、Sets集合、Dictionary字典，数字类型有四种类型：整数、布尔型、浮点数和复数。C语言也可分四类：基本类型、枚举类型、void的类型、派生类型。基本类型：整数类型、浮点类型。

  4、函数库的使用方法不同。

  在C语言中使用那个库函数，需要引入头文件用include引入，而在python中需要引入别的模块或者函数时需要用import引入。

  两者的不同机制是C语言中include是告诉预处理器，这个include指定的文件的内容，要当作本地源文件出现过，而python中的import可以通过简单的import 导入，或者是 import numpy as np 。

  5、全局变量不同。

  在C语言中，声明全局变量，如果值是恒定的，那么可以直接用#define声明，如果只是声明全局，并且变量的值是可变的，那么直接类似int a 就可以了。

  在python中，声明全局变量时，需要加上global，类似global a，在函数里面使用的时候需要先声明global a ，否则直接用a那么python会重新创建一个新的本地对象并将新的值赋值给他，原来的全局变量的值并不变化。

  ## 43.const

  C++的编译器通常不为普通const常量分配存储空间，而是将它们保存在符号表中，这使得它成为一个编译期间的常量，没有了存储与读内存的操作，使得它的效率也很高

  ## 44.const和define区别

  ### (1) 编译器处理方式不同

   　#define宏是在预处理阶段展开。
   　const常量是编译运行阶段使用。

  ###  (2) 类型和安全检查不同

   　#define宏没有类型，不做任何类型检查，仅仅是展开。
   　const常量有具体的类型，在编译阶段会执行类型检查。

  ###  (3) 存储方式不同

   　#define宏仅仅是展开，有多少地方使用，就展开多少次，不会分配内存。（宏定义不分配内存，变量定义分配内存。）
   　const常量会在内存中分配(可以是堆中也可以是栈中)。

  ###  (4) const  可以节省空间，避免不必要的内存分配。

   例如：

   \#define NUM 3.14159  //常量宏
   const doulbe Num = 3.14159; //此时并未将Pi放入ROM中 ......
   double i = Num; //此时为Pi分配内存，以后不再分配！
   double  I= NUM; //编译期间进行宏替换，分配内存
   double j = Num; //没有内存分配
   double J = NUM; //再进行宏替换，又一次分配内存！
   const定义常量从汇编的角度来看，只是给出了对应的内存地址，而不是象#define一样给出的是立即数，所以，const定义的常量在程序运行过程中只有一份拷贝（因为是全局的只读变量，存在静态区），而 #define定义的常量在内存中有若干个拷贝。

  ###  (5) 提高了效率。

   编译器通常不为普通const常量分配存储空间，而是将它们保存在符号表中，这使得它成为一个编译期间的常量，没有了存储与读内存的操作，使得它的效率也很高。

  ###  (6) 宏替换

  只作替换，不做计算，不做表达式求解;

   宏预编译时就替换了，程序运行时，并不分配内存。

  ## 44.有的操作符重载函数只能是友元函数

  运算符重载为成员函数，第一个参数必须是本类的对象。而<<和>>的第一个操作数一定是ostream类型，所以<<只能重载为友元函数。 1 ## 45.模板实例化 ### 实例化 是指在编译或链接时生成函数模板或类模板的具体实例源代码。实例化有两种形式，分别为显式实例化和隐式实例化。模板并非函数定义，实例式函数定义。 #### 1.1 显式实例化（explicit instantiation） 显式实例化意味着可以直接命令编译器创建特定的实例，有两种显式声明的方式。 比如存在这么一个模板函数 template <typename t="">
  void Swap(T &a, T &b)

  第一种方式是声明所需的种类，用<>符号来指示类型，并在声明前加上关键词template，如下：
  template void Swap<int>(int &, int &);

  第二种方式是直接在程序中使用函数创建，如下：

  Swap<int>(a,b);

  显式实例化直接使用了具体的函数定义，而不是让程序去自动判断。

  #### 1.2 隐式实例化（implicit instantiation）

  隐式实例化比较简单，就是最正常的调用，Swap(a,b)，直接导致程序生成一个Swap()的实例，该实例使用的类型即参数a和b的类型，编译器根据参数来定义函数实例。

  ## 46.动多态和静多态

  ### 通过继承实现的多态是绑定的和动态的：

  绑定的含义是： 对于参与多态行为的类型， 它们（具有多态行为）的接口是在公共基类的设计中就预先确定的（有时候也把绑定这个概念称为入侵的或者插入的） 。

  动态的含义是： 接口的绑定是在运行期（动态） 完成的。

  ### 通过模板实现的多态是非绑定的和静态的：

  非绑定的含义是： 对于参与多态行为的类型， 它们的接口是没有预先确定的（有时也称这个概念为非入侵的或者非插入的） 。

  静态的含义是： 接口的绑定是在编译期（静态） 完成的。

  ## 47.char short int long所占字节

  **windows操作系统，32位机**中，

  char：  1个字节

  short：  2个字节

  int：    4个字节

  long：  4个字节

  **windows操作系统，64位机**中,

  char：  1个字节

  short：  2个字节

  int：    4个字节

  long：  4个字节

  ## 48.C++支持多继承，子类到父类转换的访问权限：

  C++是支持多继承的。

  1.如果子类从父类继承时使用的继承限定符是public，那么

  * 父类的public成员成为子类的public成员，允许类以外的代码访问这些成员；
  * 父类的private成员仍旧是父类的private成员，子类成员不可以访问这些成员；
  * 父类的protected成员成为子类的protected成员，只允许子类成员访问；

  2.如果子类从父类继承时使用的继承限定符是private，那么

  * 父类的public成员成为子类的private成员，只允许子类成员访问；
  * 父类的private成员仍旧是父类的private成员，子类成员不可以访问这些成员；
  * 父类的protected成员成为子类的private成员，只允许子类成员访问；

  3.如果子类从父类继承时使用的继承限定符是protected，那么

  * 父类的public成员成为子类的protected成员，只允许子类成员访问；
  * 父类的private成员仍旧是父类的private成员，子类成员不可以访问这些成员；
  * 父类的public成员成为子类的protected成员，只允许子类成员访问；

  ## 49.不用第三个变量交换两个变量：

  int a=10,b=12; //a=1010^b=1100;
  a=a^b; //a=0110^b=1100;
  b=a^b; //a=0110^b=1010;
  a=a^b; //a=1100=12;b=1010;

   

  此算法能够实现是由异或运算的特点决定的，通过异或运算能够使数据中的某些位翻转，其他位不变。这就意味着任意一个数与任意一个给定的值连续异或两次，值不变。

  # STL：

  #### 1.STL有哪几部分组成

  容器：各种数据结构，从实现的角度来看，STL容器是一种class template

  空间配置器：负责动态空间的配置和管理

  迭代器：在23个设计模式中，有一种是迭代器模式（提供一种方法，使之能够依序访问某个容器所含的各个元素，而无需暴露该容器的内部表述方式），其行为类似于智能指针；STL的设计中，将数据容器和算法分开，彼此独立设计，通过迭代器(容器和算法的胶粘剂)将他们撮合在一起。

  算法：从实现角度来看，STL算法是一种function template

  仿函数：也叫函数对象，行为类似于函数；从实现角度来看，仿函数是重载了operator()的class或者class template 。
  仿函数分类：以操作数的个数划分：一元仿函数、二元仿函数； 以功能划分：算术运算、关系运算、逻辑运算。

  配接器：用来修饰容器或者迭代器接口

  * STL并不是线程安全的，都需要进行加锁处理

  #### 2.hash,set和map的实现和区别

  * ##### hash:  哈希表是采用开链法来完成的，（vector + list）

    * 底层键值序列采用 vector 实现，vector 的大小取的是质数，且相邻质数的大小约为 2 倍关系，当创建 hashtable 时，会自动选取一个接近所创建大小的质数作为当前 hashtable 的大小；
    
    * 对应键的值序列采用单向 list 实现；
    
    * 当 hashtable 的键 vector 的大小重新分配的时候，原键的值 list 也会重新分配，因为 vector 重建了相当于键增加了，那么原来的值对应的键可能就不同于原来分配的键，这样就需要重新确定值的键。
    
    * 扩容机制：
    
      通过负载因子来确定是否要扩容，哈希表初始化的大小为2的幂值，当所占空间大于负载因子*完整空间大小，就去申请2倍空间。

  #### 3.map和unordered_map的实现和区别

  * 底层实现不同：

    * unordered_map 底层实现是一个哈希表，元素无序
    * map 底层实现是红黑树，其内部所有的元素都是有序的，因此对 map 的所有操作，其实都是对红黑树的操作
  * 优缺点：

    * unordered_map：查找效率高；但是建立哈希表比较耗费时间

    * map：内部元素有序，查找和删除操作都是 logn 的时间复杂度；但是维护红黑树的存储结构需要占用一定的内存空间
  * 适用情况：
    对于要求内部元素有序的使用 map，对于要求查找效率的用 unordered_map
  * map实现：
    * map以红黑树作为底层机制，红黑树是平衡二叉树的一种，在要求上比AVL树更宽泛。通过map的迭代器只能修改其实值，不能修改其键值，所以map的迭代器既不能是const也不是mutable
    * 红黑树满足以下几个条件：
      每个节点不是红色就是黑色
      根节点是黑色
      红色节点的子节点必须是黑色（不能有连续的红节点）
      从根节点到NULL的任何路径所含的黑节点数目相同
      叶子节点是黑色的NULL节点（注：这里不是我们常说的二叉树中的叶节点，是它的子节点（NULL））

  #### list实现和区别

  * vector：

    * vector 是一个动态数组，底层实现是一段连续的线性内存空间。
      扩容的本质：当 vector 实际所占用的内存空间和容量相等时，如果再往其中添加元素需要进行扩容。其步骤如下：
      * 首先，申请一块更大的存储空间，一般是增加当前容量的 50% 或者 100%，和编译器有关；
      * 然后，将旧内存空间的内容，按照原来的顺序放到新的空间中
      * 最后，将旧内存空间的内容释放掉，本质上其存储空间不会释放，只是删除了里面的内容。

  * 从 vector 扩容的原理也可以看出：vector 容器释放后，与其相关的指针、引用以及迭代器会失效的原因。

  * Vector的性能影响主要是在插入元素方面：插入元素需要考虑元素的移动问题和是否需要扩容的问题，频繁的调用 push_back() 也是扩容的问题对性能的影响，可更改为emplace_back()

    两者导致差异的原因是插入方法不同，push_back()会先拷贝当前元素，再通过移动或拷贝将其放入，然后销毁原元素，而emplace_back()则是直接在vector中创建

    避免大规模性能消耗的也可以通过一次性申请够大空间即可。

  * list：

    - list底层存储机制，list以节点为单位存放数据，节点的地址在内存中不一定连续，每次插入或者删除数据时，就配置或者释放一个元素的空间
    - list自带排序算法：链表排序

  * list deque vector对比：

    * vector数据在内存中连续排列，所以随机存取元素的速度最快。但是在除尾部以外的位置删除或者添加元素的时候速度很慢
    * list数据是链式存储，不能随机存取。其优势在于在任意位置添加和删除元素
    * deque通过链接若干片连续的数据实现的，均衡了以上两容器的特

    

  #### 5.STL迭代器的作用，为什么有指针还要用迭代器

  * 迭代器不是指针，是类模板，表现的像指针，模拟了指针的功能，重载了指针的一些操作符，->, * , ++, --等；
  * 迭代器封装了指针，是一个可遍历STL容器内全部或者部分元素的对象，本质上封装了原声指针，比指针更高级，相当于只能指针
  * 迭代器返回的是引用，而不是对象的值
  * vector动态增加空间时，并不是在原空间之后增加新的空间，而是以原来大小的两倍或者原空间加上实际所需的空间的大小另外配置一片较大的空间，释放原来的空间。由于操作改变了空间，所以原来的迭代器失效
  * 在进行insert时，如果在p位置插入新的元素。当容器有剩余空间，不需要重新分配空间时，p之前的迭代器都有效，p之后的迭代器都失效；当容器重新分配了内存空间，那么所有的迭代器都失效
  * 进行erase时，erase的位置在p处，p之前的迭代器都有效且p指向下一个元素位置（如果p在尾元素处，p指向无效end无效），p之后的迭代器都无效。

  #### 6.resize和reserve的区别

  * resize(size_type n, value_type val = value_type())：改变的是当前容器内元素的数量，也就是改变的size()。如果n小于当前容器的元素数量，则容器中只会取前n个元素，多余的会被移除；否则会在当前元素的最后插入n - size()个元素，元素的值为其传入的参数，如果未传入，则是默认的
  * reserve(size_type n)：改变的是容器的容量，也就是capacity()。如果n大于当前的容量，就会分配空间扩增容量；否则，将不会做任何处理

  #### 7.STL的内存优化

  在C++中动态分配内存和释放内存分别用new和delete这两个关键字。

  * new内含两阶段操作：
    1.调用::operator new配置内存
    2.调用Foo::Foo()构造对象的内容

  * delete内含两阶段操作
    1.调用Foo::~Foo()将对象析构
    2.调用::operator delete释放内存

  * STL allocator将以上两阶段的操作由以下几个函数来完成
    内存配置：alloc:allocate()负责
    内存释放：allo::deallocate()负责
    对象构造：::construct()负责
    对象析构：::destory()负责

  * 内存分配与释放分别是由::operator new()和::operator delete()来完成，底层是调用malloc()和free()这两个函数来完成内存的配置和释放。

  * 双层级配置器：
    考虑到分配小的空间时可能会造成内存碎片问题，以及小块内存的频繁的申请和释放的性能问题，SGI STL设计了双层级配置器，默认使用第二级配置器。

    * 第一级配置器直接使用malloc()和free()进行内存空间的分配和释放
    * 第二级配置器视情况采取不同的策略：当配置区块超过128bytes时，调用第一级配置器；当配置区块小于128bytes时，采用memory pool(内存池)的方式，通过空闲链表来管理内存，第二级配置器会自动将内存的需求量上调为8的倍数，并维护16个自由链表，自由链表是一个指针数组，数组大小为16，每个数组的元素代表所挂区块的大小，free_list[0] = 8, free_list[1] = 16,以此类推（8，16，24，32，40，48，56，64，72，80，88，96，104，112，120，128）

  * 一级配置器

    * SGI以malloc来配置内存，配置成功直接返回

    * 配置失败，调用内存不足处理函数oom_malloc，如果客户端没有设置内存不足的处理机制，直接抛出bad_alloc异常或者直接终止程序

    * 如果客户端设置了内存不足的处理机制，就会一直调用这个处理机制，直到内存分配成功。这里如果内存不足的处理机制设置的不好的话，存在死循环的危险

      ![在这里插入图片描述](https://pic.leetcode-cn.com/1606303841-SHqQKL-file_1606303850218)

  * 二级配置器：内存池 + 空闲链表

    * 先判断分配内存的大小，如果大于128bytes就调用一级配置器，否则通过内存池来分配，将所需内存上调至8的倍数，然后再去自由链表中相应的节点中去找，如果该节点下面挂有未使用的内存，则摘下来直接使用这部分内寸。否则调用refill(size_t n)去内存池中申请

    * 向内存池中申请时STL默认一次申请20个，将多余的空间挂在自由链表上。refill函数中使用chunk_alloc(size_t n, size_t& nobjs)函数去内存池中申请。
      如果申请成功，回到refill函数。这时，如果nobjs = 1,表明内存池只够分配一个，返回这个地址就可以；如果大于一个，需要将剩余的挂到自由链表上。

    * 如果chunk_alloc(size_t n, size_t& nobjs)失败，

      如果内存池剩余的空间足够 nobjs * n这么大，直接分配返回就OK。如果剩余的空间leftAlloc的范围n<=leftalloc<=nobjs*n，就分配 2 228 nobjs="(leftAlloc)" n个空间返回就可以。 
      
      * 如果剩余的空间连一个n都不够，需要向heap申请内存，申请之前需要将内存池中剩余的内存挂在到自由链表上。 如果申请成功，就再调用chunk_alloc进行分配 如果失败，去看自由链表中有没有比n大的空间，如果有就将这块空间放到内存池中，再调用chunk_alloc进行分配；否则调用一级配置器，交给内存不足处理机制处理 ![在这里插入图片描述](https: pic.leetcode-cn.com 1606303841-lawnhf-file_1606303850223) 空间配置器存在的问题： 自由链表所挂的区块都是8的整数倍，当我们需要非8的整数倍的区块，往往造成浪费 配置器所有的方法和成员都是静态的，那么他们放在静态区。只有程序结束后，才释放内存，这样会导致在程序运行的过程中，自由链表一直占用内存，自己的进程可以使用，其他的进程却用不了。 #### 8.string是存于堆中还是栈中 由于是动态内存，所以存放在堆上 9.常用容器及其特点： string：与vector相似的容器，专门用于存储字符。随机访问快，在尾位置插入 删除速度快 array：固定大小数组。支持快速随机访问，不能添加或者删除元素 vector：可变大小的数组。底层数据结构为数组，支持快速随机访问，在尾部之外的位置插入或者删除元素可能很慢 list：双向链表。底层数据结构为双向链表，支持双向顺序访问。在list任何位置插入 删除速度很快 forward_list：单向链表。支持单项顺序访问。在forward_list任何位置插入 deque：双端队列。底层数据结构为一个中央控制器和多个缓冲区，支持快速随机访问，在头尾位置插入 stack：栈。底层用deque实现，封闭头部，在尾部进行插入和删除元素 queue：队列。底层用deque实现 priority_queue：优先队列。底层用vector实现，堆heap为处理规则来管理底层容器的实现 set：集合。底层为红黑树，元素有序，不重复 multiset：底层为红黑树，元素有序，可重复 map：底层为红黑树，键有序，不重复 multimap：底层为红黑树，键有序，可重复 hash_set：底层为哈希表，无序，不重复 hash_multiset：底层为哈希表，无序，可重复 hash_map：底层为哈希表，无序，不重复 hash_multiap：底层为哈希表，无序，可重复 首先vector属于顺序容器，其元素与存储位置与操作操作有关；set属于关联容器，其元素相当于键值。set能够保证它里面所有的元素都是不重复的（multiset除外）。 其次，由于存储结构不同，vector擅长于解决某个位置是什么值的问题，而set擅长于解决，某个元素在那个位置的问题，知道元素的内容，查找它的位置。因此vector特别好的支持随机访问，而set不支持（不支持下标访问） ### 10.stl迭代器失效情况 迭代器失效类型： - 由于插入元素，使得容器元素整体`迁移`导致存放原容器元素的空间不再有效，从而使得指向原空间的迭代器失效； 由于删除元素，使得某些元素`次序`发生变化导致原本指向某元素的迭代器不再指向期望指向的元素。 11.deque的原理 双端队列(deque)是一种支持向两端高效地插入数据、支持随机访问的容器。其内部实现原理如下：双端队列的数据被表示为一个分段数组，容器中的元素分段存放在一个个大小固定的数组中，此外容器还需要维护一个存放这些数组首地址的索引数组。参见下图 ![](https: ivanzz1001.github.io records assets img cplusplus cpp_stl_iterator.jpg)] 由于分段数组的大小是固定的，并且它们的首地址被连续存放在索引数组中，因此可以对其进行随机访问，但效率比vector低很多。 向两端加入新元素时，如果这一端的分段数组未满，则可以直接加入； 如果这一端的分段数组已满，只需创建新的分段数组，并把该分段数组的地址加入到索引数组中即可。无论哪种情况，都不需要对已有元素进行移动，因此在双端队列的两端加入新的元素都具有较高的效率。 当删除双端队列容器两端的元素时，由于不需要发生元素的移动，效率也是非常高的。 双端队列中间插入元素时，需要将插入点到某一端之间的所有元素向容器的这一端移动，因此向中间插入元素效率较低，而且往往插入位置越靠近中间，效率越低。删除队列中元素时，情况也类似，由于被删元素到某一端之间的所有元素都要向中间移动，删除的位置越靠近中间，效率越低。 # 操作系统 1.进程和线程的概念，为什么还要有线程，有什么区别 进程：是并发执行的程序在执行过程中分配和管理资源的基本单位，是一个动态概念，竞争计算机系统资源的基本单位。 线程：是进程的一个执行单元，是进程内科调度实体。比进程更小的独立运行的基本单位。线程也被称为轻量级进程。 协程：是一种比线程更加轻量级的存在。一个线程也可以拥有多个协程。其执行过程更类似于子例程，或者说不带返回值的函数调用。 区别： 地址空间：线程共享本进程的地址空间，而进程之间是独立的地址空间。 资源：线程共享本进程的资源如内存、i o、cpu等，不利于资源的管理和保护，而进程之间的资源是独立的，能很好的进行资源管理和保护。 健壮性：多进程要比多线程健壮，一个进程崩溃后，在保护模式下不会对其他进程产生影响，但是一个线程崩溃整个进程都死掉。 执行过程：每个独立的进程有一个程序运行的入口、顺序执行序列和程序入口，执行开销大。但是线程不能独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制，执行开销小。 可并发性：两者均可并发执行。 切换时：进程切换时，消耗的资源大，效率高。所以涉及到频繁的切换时，使用线程要好于进程。同样如果要求同时进行并且又要共享某些变量的并发操作，只能用线程不能用进程。 其他：线程是处理器调度的基本单位，但是进程不是。 进程结构：  linux进程结构：可由三部分组成：代码段、数据段、堆栈段。也就是程序、数据、进程控制块pcb（process control block）组成。进程控制块是进程存在的惟一标识，系统通过pcb的存在而感知进程的存在。 系统通过pcb对进程进行管理和调度。pcb包括创建进程、执行程序、退出进程以及改变进程的优先级等。而进程中的pcb用一个名为task_struct的结构体来表示，定义在include linux sched.h中，每当创建一新进程时，便在内存中申请一个空的task_struct结构，填入所需信息，同时，指向该结构的指针也被加入到task数组中，所有进程控制块都存储在task[]数组中。 2.进程间通信方式，各自讲一下 **1. 管道 匿名管道(pipe)** 管道是半双工的，数据只能向一个方向流动；需要双方通信时，需要建立起两个管道。 只能用于父子进程或者兄弟进程之间(具有亲缘关系的进程); 单独构成一种独立的文件系统：管道对于管道两端的进程而言，就是一个文件，但它不是普通的文件，它不属于某种文件系统，而是自立门户，单独构成一种文件系统，并且只存在与内存中。 数据的读出和写入：一个进程向管道中写的内容被管道另一端的进程读出。写入的内容每次都添加在管道缓冲区的末尾，并且每次都是从缓冲区的头部读出数据。 ![img](https: upload-images.jianshu.io upload_images 1281379-05378521a7b41af4.png?imagemogr2 auto-orient strip|imageview2 w format webp)  进程间管道通信模型 **管道的实质：** 管道的实质是一个内核缓冲区，进程以先进先出的方式从缓冲区存取数据，管道一端的进程顺序的将数据写入缓冲区，另一端的进程则顺序的读出数据。 该缓冲区可以看做是一个循环队列，读和写的位置都是自动增长的，不能随意改变，一个数据只能被读一次，读出来以后在缓冲区就不复存在了。 当缓冲区读空或者写满时，有一定的规则控制相应的读进程或者写进程进入等待队列，当空的缓冲区有新数据写入或者满的缓冲区有数据读出来时，就唤醒等待队列中的进程继续读写。 **管道的局限：** 管道的主要局限性正体现在它的特点上： 只支持单向数据流； 只能用于具有亲缘关系的进程之间； 没有名字； 管道的缓冲区是有限的（管道制存在于内存中，在管道创建时，为缓冲区分配一个页面大小）； 管道所传送的是无格式字节流，这就要求管道的读出方和写入方必须事先约定好数据的格式，比如多少字节算作一个消息（或命令、或记录）等等； 2.**匿名管道** 由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道(fifo)。 有名管道不同于匿名管道之处在于它提供了一个路径名与之关联，**以有名管道的文件形式存在于文件系统中**，这样，**即使与有名管道的创建进程不存在亲缘关系的进程，只要可以访问该路径，就能够彼此通过有名管道相互通信**，因此，通过有名管道不相关的进程也能交换数据。值的注意的是，有名管道严格遵循**先进先出(first in first out)**,对匿名管道及有名管道的读总是从开始处返回数据，对它们的写则把数据添加到末尾。它们不支持诸如lseek()等文件定位操作。**有名管道的名字存在于文件系统中，内容存放在内存中。**> **匿名管道和有名管道总结：**
  > （1）管道是特殊类型的文件，在满足先入先出的原则条件下可以进行读写，但不能进行定位读写。
  > （2）匿名管道是单向的，只能在有亲缘关系的进程间通信；有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。
  > （3）**无名管道阻塞问题：**无名管道无需显示打开，创建时直接返回文件描述符，在读写时需要确定对方的存在，否则将退出。如果当前进程向无名管道的一端写数据，必须确定另一端有某一进程。如果写入无名管道的数据超过其最大值，写操作将阻塞，如果管道中没有数据，读操作将阻塞，如果管道发现另一端断开，将自动退出。
  > （4）**有名管道阻塞问题：**有名管道在打开时需要确实对方的存在，否则将阻塞。即以读方式打开某管道，在此之前必须一个进程以写方式打开管道，否则阻塞。此外，可以以读写（O_RDWR）模式打开有名管道，即当前进程读，当前进程写，不会阻塞。

  3.**信号**

  信号是一种比较复杂的通信方式,用于通知接收进程某个事件已经发生。

  - 信号是Linux系统中用于进程间互相通信或者操作的一种机制，信号可以在任何时候发给某一进程，而无需知道该进程的状态。
  - 如果该进程当前并未处于执行状态，则该信号就有内核保存起来，知道该进程回复执行并传递给它为止。
  - 如果一个信号被进程设置为阻塞，则该信号的传递被延迟，直到其阻塞被取消是才被传递给进程。

  > **Linux系统中常用信号：**
  > （1）**SIGHUP：**用户从终端注销，所有已启动进程都将收到该进程。系统缺省状态下对该信号的处理是终止进程。
  > （2）**SIGINT：**程序终止信号。程序运行过程中，按`Ctrl+C`键将产生该信号。
  > （3）**SIGQUIT：**程序退出信号。程序运行过程中，按`Ctrl+\\`键将产生该信号。
  > （4）**SIGBUS和SIGSEGV：**进程访问非法地址。
  > （5）**SIGFPE：**运算中出现致命错误，如除零操作、数据溢出等。
  > （6）**SIGKILL：**用户终止进程执行信号。shell下执行`kill -9`发送该信号。
  > （7）**SIGTERM：**结束进程信号。shell下执行`kill 进程pid`发送该信号。
  > （8）**SIGALRM：**定时器信号。
  > （9）**SIGCLD：**子进程退出信号。如果其父进程没有忽略该信号也没有处理该信号，则子进程退出后将形成僵尸进程。

  **信号来源**
   信号是软件层次上对中断机制的一种模拟，是一种异步通信方式，，信号可以在用户空间进程和内核之间直接交互，内核可以利用信号来通知用户空间的进程发生了哪些系统事件，信号事件主要有两个来源：

  - 硬件来源：用户按键输入`Ctrl+C`退出、硬件异常如无效的存储访问等。
  - 软件终止：终止进程信号、其他进程调用kill函数、软件异常产生信号。

  **信号生命周期和处理流程**
   （1）信号被某个进程产生，并设置此信号传递的对象（一般为对应进程的pid），然后传递给操作系统；
   （2）操作系统根据接收进程的设置（是否阻塞）而选择性的发送给接收者，如果接收者阻塞该信号（且该信号是可以阻塞的），操作系统将暂时保留该信号，而不传递，直到该进程解除了对此信号的阻塞（如果对应进程已经退出，则丢弃此信号），如果对应进程没有阻塞，操作系统将传递此信号。
   （3）目的进程接收到此信号后，将根据当前进程对此信号设置的预处理方式，暂时终止当前代码的执行，保护上下文（主要包括临时寄存器数据，当前程序位置以及当前CPU的状态）、转而执行中断服务程序，执行完成后在回复到中断的位置。当然，对于抢占式内核，在中断返回时还将引发新的调度。

  ![img](https:////upload-images.jianshu.io/upload_images/1281379-3eed8cca67aa9f55.png?imageMogr2/auto-orient/strip|imageView2/2/w/889/format/webp)

  ​													信号的生命周期

  **4. 消息(Message)队列**

  - 消息队列是存放在内核中的消息链表，每个消息队列由消息队列标识符表示。
  - 与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。
  - 另外与管道不同的是，消息队列在某个进程往一个队列写入消息之前，并不需要另外某个进程在该队列上等待消息的到达。

  > **消息队列特点总结：**
  > （1）消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识.
  > （2）消息队列允许一个或多个进程向它写入与读取消息.
  > （3）管道和消息队列的通信数据都是先进先出的原则。
  > （4）消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比FIFO更有优势。
  > （5）消息队列克服了信号承载信息量少，管道只能承载无格式字 节流以及缓冲区大小受限等缺。
  > （6）目前主要有两种类型的消息队列：POSIX消息队列以及System V消息队列，系统V消息队列目前被大量使用。系统V消息队列是随内核持续的，只有在内核重起或者人工删除时，该消息队列才会被删除。

  **5. 共享内存(share memory)**

  - 使得多个进程可以可以直接读写同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。
  - 为了在多个进程间交换信息，内核专门留出了一块内存区，可以由需要访问的进程将其映射到自己的私有地址空间。进程就可以直接读写这一块内存而不需要进行数据的拷贝，从而大大提高效率。
  - 由于多个进程共享一段内存，因此需要依靠某种同步机制（如信号量）来达到进程间的同步及互斥。

  **6. 信号量(semaphore)**
   信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。
   为了获得共享资源，进程需要执行下列操作：
   （1）**创建一个信号量**：这要求调用者指定初始值，对于二值信号量来说，它通常是1，也可是0。
   （2）**等待一个信号量**：该操作会测试这个信号量的值，如果小于0，就阻塞。也称为P操作。
   （3）**挂出一个信号量**：该操作将信号量的值加1，也称为V操作。

  为了正确地实现信号量，信号量值的测试及减1操作应当是原子操作。为此，信号量通常是在内核中实现的。Linux环境中，有三种类型：**Posix（[可移植性操作系统接口](https://link.jianshu.com?t=http://baike.baidu.com/link?url=hYEo6ngm9MlqsQHT3h28baIDxEooeSPX6wr_FdGF-F8mf7wDp2xJWIDtQWGEDxthtPNiJtlsw460g1_N0txJYa)）有名信号量（使用Posix IPC名字标识）**、**Posix基于内存的信号量（存放在共享内存区中）**、**System V信号量（在内核中维护）**。这三种信号量都可用于进程间或线程间的同步。

  > **信号量与普通整型变量的区别：**
  > （1）信号量是非负整型变量，除了初始化之外，它只能通过两个标准原子操作：wait(semap) , signal(semap) ; 来进行访问；
  > （2）操作也被成为PV原语（P来源于荷兰语proberen"测试"，V来源于荷兰语verhogen"增加"，P表示通过的意思，V表示释放的意思），而普通整型变量则可以在任何语句块中被访问；

  > **信号量与互斥量之间的区别：**
  > （1）互斥量用于线程的互斥，信号量用于线程的同步。这是互斥量和信号量的根本区别，也就是互斥和同步之间的区别。
  > **互斥：**是指某一资源同时只允许一个访问者对其进行访问，具有唯一性和排它性。但互斥无法限制访问者对资源的访问顺序，即访问是无序的。
  > **同步：**是指在互斥的基础上（大多数情况），通过其它机制实现访问者对资源的有序访问。
  > 在大多数情况下，同步已经实现了互斥，特别是所有写入资源的情况必定是互斥的。少数情况是指可以允许多个访问者同时访问资源
  > （2）互斥量值只能为0/1，信号量值可以为非负整数。
  > 也就是说，一个互斥量只能用于一个资源的互斥访问，它不能实现多个资源的多线程互斥问题。信号量可以实现多个同类资源的多线程互斥和同步。当信号量为单值信号量是，也可以完成一个资源的互斥访问。
  > （3）互斥量的加锁和解锁必须由同一线程分别对应使用，信号量可以由一个线程释放，另一个线程得到。

  

  **7. 套接字(socket)**
   套接字是一种通信机制，凭借这种机制，客户/服务器（即要进行通信的进程）系统的开发工作既可以在本地单机上进行，也可以跨网络进行。也就是说它可以让不在同一台计算机但通过网络连接计算机上的进程进行通信。

  ![img](https:////upload-images.jianshu.io/upload_images/1281379-2db1deb0115ec4f2.png?imageMogr2/auto-orient/strip|imageView2/2/w/319/format/webp)

  ​								Socket是应用层和传输层之间的桥梁

  套接字是支持TCP/IP的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。

  **套接字特性**
   套接字的特性由3个属性确定，它们分别是：域、端口号、协议类型。
   **（1）套接字的域**
   它指定套接字通信中使用的网络介质，最常见的套接字域有两种：
   **一是AF_INET，它指的是Internet网络。**当客户使用套接字进行跨网络的连接时，它就需要用到服务器计算机的IP地址和端口来指定一台联网机器上的某个特定服务，所以在使用socket作为通信的终点，服务器应用程序必须在开始通信之前绑定一个端口，服务器在指定的端口等待客户的连接。
   **另一个域AF_UNIX，表示UNIX文件系统，**它就是文件输入/输出，而它的地址就是文件名。
   **（2）套接字的端口号**
   每一个基于TCP/IP网络通讯的程序(进程)都被赋予了唯一的端口和端口号，端口是一个信息缓冲区，用于保留Socket中的输入/输出信息，端口号是一个16位无符号整数，范围是0-65535，以区别主机上的每一个程序（端口号就像房屋中的房间号），低于256的端口号保留给标准应用程序，比如pop3的端口号就是110，每一个套接字都组合进了IP地址、端口，这样形成的整体就可以区别每一个套接字。
   **（3）套接字协议类型**
   因特网提供三种通信机制，
   **一是流套接字，**流套接字在域中通过TCP/IP连接实现，同时也是AF_UNIX中常用的套接字类型。流套接字提供的是一个有序、可靠、双向字节流的连接，因此发送的数据可以确保不会丢失、重复或乱序到达，而且它还有一定的出错后重新发送的机制。
   **二个是数据报套接字，**它不需要建立连接和维持一个连接，它们在域中通常是通过UDP/IP协议实现的。它对可以发送的数据的长度有限制，数据报作为一个单独的网络消息被传输,它可能会丢失、复制或错乱到达，UDP不是一个可靠的协议，但是它的速度比较高，因为它并一需要总是要建立和维持一个连接。
   **三是原始套接字，**原始套接字允许对较低层次的协议直接访问，比如IP、 ICMP协议，它常用于检验新的协议实现，或者访问现有服务中配置的新设备，因为RAW SOCKET可以自如地控制Windows下的多种协议，能够对网络底层的传输机制进行控制，所以可以应用原始套接字来操纵网络层和传输层应用。比如，我们可以通过RAW SOCKET来接收发向本机的ICMP、IGMP协议包，或者接收TCP/IP栈不能够处理的IP包，也可以用来发送一些自定包头或自定协议的IP包。网络监听技术很大程度上依赖于SOCKET_RAW。

  > **原始套接字与标准套接字的区别在于：**
  > 原始套接字可以读写内核没有处理的IP数据包，而流套接字只能读取TCP协议的数据，数据报套接字只能读取UDP协议的数据。因此，如果要访问其他协议发送数据必须使用原始套接字。

  **套接字通信的建立**

  ![img](https:////upload-images.jianshu.io/upload_images/1281379-2575b81bbab6b67b.png?imageMogr2/auto-orient/strip|imageView2/2/w/437/format/webp)

  Socket通信基本流程

  

  ** 服务器端**
   （1）首先服务器应用程序用系统调用socket来创建一个套接字，它是系统分配给该服务器进程的类似文件描述符的资源，它不能与其他的进程共享。
   （2）然后，服务器进程会给套接字起个名字，我们使用系统调用bind来给套接字命名。然后服务器进程就开始等待客户连接到这个套接字。
   （3）接下来，系统调用listen来创建一个队列并将其用于存放来自客户的进入连接。
   （4）最后，服务器通过系统调用accept来接受客户的连接。它会创建一个与原有的命名套接不同的新套接字，这个套接字只用于与这个特定客户端进行通信，而命名套接字（即原先的套接字）则被保留下来继续处理来自其他客户的连接（建立客户端和服务端的用于通信的流，进行通信）。

  **客户端**
   （1）客户应用程序首先调用socket来创建一个未命名的套接字，然后将服务器的命名套接字作为一个地址来调用connect与服务器建立连接。
   （2）一旦连接建立，我们就可以像使用底层的文件描述符那样用套接字来实现双向数据的通信（通过流进行数据传输）

  Socket 传输的特点：

  * 优点

   1)  传输数据为字节级，传输数据可自定义，数据量小（对于手机应用讲：费用低）

   2）传输数据时间短，性能高

   3）适合于客户端和服务器端之间信息实时交互

   4）可以加密,数据安全性强

  * 缺点：

  1）需对传输的数据进行解析，转化成应用级的数据

  2）对开发人员的开发水平要求高

  3）相对于Http协议传输，增加了开发量

  #### 3.线程的通信方式

  * 锁机制：包括互斥锁、条件变量、读写锁
    * 互斥锁提供了以排他方式防止数据结构被并发修改的方法。
    * 读写锁允许多个线程同时读共享数据，而对写操作是互斥的。
    * 条件变量可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。

  * 信号量机制(Semaphore)：包括无名线程信号量和命名线程信号量

  * 信号机制(Signal)：类似进程间的信号处理

  #### 4.虚拟内存的概念和好处

  正是因为 **虚拟内存** 的存在，通过 **虚拟内存** 可以让程序可以拥有超过系统物理内存大小的可用内存空间。另外，**虚拟内存为每个进程提供了一个一致的、私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉（每个进程拥有一片连续完整的内存空间）**。这样会更加有效地管理内存并减少出错。

  **虚拟内存**是计算机系统内存管理的一种技术，我们可以手动设置自己电脑的虚拟内存。不要单纯认为虚拟内存只是“使用硬盘空间来扩展内存“的技术。**虚拟内存的重要意义是它定义了一个连续的虚拟地址空间**，并且 **把内存扩展到硬盘空间**。

  ![virtual-memory-system](https://img.draveness.me/2020-04-09-15863627859701-virtual-memory-system.png)

  主存储是相对比较稀缺的资源，虽然顺序读取只比磁盘快 1 个数量级，但是它能提供极快的随机访问速度，充分利用内存的随机访问速度是改善程序执行效率的有效方式。

  操作系统以页为单位管理内存，当进程发现需要访问的数据不在内存时，操作系统可能会将数据以页的方式加载到内存中，这个过程是由上图中的内存管理单元（MMU）完成的。操作系统的虚拟内存作为一个抽象层，起到了以下三个非常关键的作用：

  - 虚拟内存可以利用内存起到缓存的作用，提高进程访问磁盘的速度；
  - 虚拟内存可以为进程提供独立的内存空间，简化程序的链接、加载过程并通过动态库共享内存；
  - 虚拟内存可以控制进程对物理内存的访问，隔离不同进程的访问权限，提高系统的安全性；

  我们可以将虚拟内存看作是在磁盘上一片空间，当这片空间中的一部分访问比较频繁时，**该部分数据会以页为单位被缓存到主存中**以加速 CPU 访问数据的性能，虚拟内存利用空间较大的磁盘存储作为『内存』并使用主存储缓存进行加速，**让上层认为操作系统的内存很大而且很快，然而区域很大的磁盘并不快，而很快的内存也并不大**。

  #### 5.缺页中断，缺页置换算法

  * 缺页中断（英语：Page fault，又名硬错误、硬中断、分页错误、寻页缺失、缺页中断、页故障等）指的是当软件试图访问已映射在[虚拟](https://baike.baidu.com/item/虚拟)[地址空间](https://baike.baidu.com/item/地址空间)中，但是目前并未被加载在[物理内存](https://baike.baidu.com/item/物理内存)中的一个[分页](https://baike.baidu.com/item/分页)时，由[中央处理器](https://baike.baidu.com/item/中央处理器)的内存管理单元所发出的[中断](https://baike.baidu.com/item/中断)。

  * 算法：

    * OPT（最优置换）　置换以后不再被访问，或者在将来最迟才回被访问的页面，缺页中断率最低。但是该算法需要依据以后各业的使用情况，而当一个进程还未运行完成是，很难估计哪一个页面是以后不再使用或在最长时间以后才会用到的页面。所以**该算法是不能实现的**

    * FIFO（先进先出）但会出现Belady异常，即分配物理块数增大而页故障不减反增。

    * LRU（最近最久未使用）

    * Clock时钟置换，即逐出的页面都是最近没有使用的那个。我们给每一个页面设置一个标记位u，u=1表示最近有使用u=0则表示该页面最近没有被使用，应该被逐出。

      * 改进型：在之前的CLOCK算法上面除了***\*使用位(used bit)\****，还增加了一个***\*修改位(modified bit)\****，有些地方也叫做dirty bit。现在每一页有两个状态，分别是***\*(使用位，修改位)\****，可分为以下四种情况考虑：

        **(0,0)**：最近没有使用使用也没有修改，最佳状态！

        **(0,1)**：修改过但最近没有使用，将会被写

        **(1,0)**：使用过但没有被修改，下一轮将再次被用

        **(1,1)**：使用过也修改过，下一轮页面置换最后的选择

        页面替换的顺序：

        1. 从指针当前的位置开始寻找主存中满足(使用位，修改位)为**(0,0)**的页面；
        2. 如果第1步没有找到满足条件的，接着寻找状态为**(0,1)**页面；
        3. 如果依然没有找到，指针回到最初的位置，将集合中所有页面的使用位设置成0。重复第1步，并且如果有必要，重复第2步，这样一定可以找到将要替换的页面。

  #### 6.并发和并行

  * 并发：一个处理器同时处理多个任务
  * 并行：多个处理器或者多核的处理器同时处理多个不同的任务

  #### 7.线程同步方式

  进程同步方法：

  * 事件： 通过通知操作的方式来保持线程的同步，还可以方便实现对多个线程的优先级比较的操作 .

  * 临界区（Critical Section）:通过对多线程的串行化来访问公共资源或一段代码，速度快，适合控制数据访问。

    * 优点：保证在某一时刻只有一个线程能访问数据的简便办法
    * 缺点：虽然临界区同步速度很快，但却只能用来同步本进程内的线程，而不可用来同步多个进程中的线程。

  * 互斥量（Mutex）:为协调共同对一个共享资源的单独访问而设计的。

    互斥量跟临界区很相似，比临界区复杂，互斥对象只有一个，只有拥有互斥对象的线程才具有访问资源的权限。

    * 优点：使用互斥不仅仅能够在同一应用程序不同线程中实现资源的安全共享，而且可以在不同应用程序的线程之间实现对资源的安全共享。

    * 缺点：①互斥量是可以命名的，也就是说它可以跨越进程使用，所以创建互斥量需要的资源更多，所以如果只为了在进程内部是用的话使用临界区会带来速度上的优势并能够减少资源占用量。因为互斥量是跨进程的互斥量一旦被创建，就可以通过名字打开它。

      ②通过互斥量可以指定资源被独占的方式使用，但如果有下面一种情况通过互斥量就无法处理，比如现在一位用户购买了一份三个并发访问许可的数据库系统，可以根据用户购买的访问许可数量来决定有多少个线程/进程能同时进行数据库操作，这时候如果利用互斥量就没有办法完成这个要求，信号量对象可以说是一种资源计数器。

  * 信号量：它允许多个线程在同一时刻访问同一资源，但是需要限制在同一时刻访问此资源的最大线程数目 .信号量对象对线程的同步方式与前面几种方法不同，信号允许多个线程同时使用共享资源，这与操作系统中的PV操作相同。它指出了同时访问共享资源的线程最大数目。它允许多个线程在同一时刻访问同一资源，但是需要限制在同一时刻访问此资源的最大线程数目。

    PV操作及信号量的概念都是由荷兰科学家E.W.Dijkstra提出的。信号量S是一个整数，S大于等于零时代表可供并发进程使用的资源实体数，但S小于零时则表示正在等待使用共享资源的进程数。

    * P操作申请资源：
      （1）S减1；
      　　（2）若S减1后仍大于等于零，则进程继续执行；
      　　（3）若S减1后小于零，则该进程被阻塞后进入与该信号相对应的队列中，然后转入进程调度。

    * V操作 释放资源：
      （1）S加1；
      　　（2）若相加结果大于零，则进程继续执行；
      　　（3）若相加结果小于等于零，则从该信号的等待队列中唤醒一个等待进程，然后再返回原进程继续执行或转入进程调度。

  #### 8上下文切换

  就是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。

  而这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。

  #### 9.死锁发生的条件和解决方法

  * 互斥条件：一个资源每次只能被一个进程使用。　（资源本身的特点，不可避免）

  * 请求与保持条件：一个进程因请求资源而等待时，不会释放已分配的资源。

  * 不剥夺条件：进程已获得的资源，在未使用之前，不能被强行剥夺。

  * 循环等待条件：若干个进程之间形成头尾相连的循环等待资源的关系。
  * 预防死锁：
    * **资源一次性分配**：破坏请求和保持条件。　当某个资源只在进程结束时使用一小会，那么在进程运行期间，这个资源都被占用，资源利用率很低。比较好的方法是，进程开始时，只申请和使用进程启动的资源，在运行过程中不断申请新的资源，同时释放已经使用完的资源。
    * **可剥夺资源**：当进程新申请的资源不满足时，释放已经分配的资源。破坏不可剥夺条件。　在使用某些资源，比如打印机时，当强制剥夺已分配资源的时候，会导致打印机资源打印的信息不连续的问题。
    * **资源有序分配**：系统给进程编号，按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。

  * 死锁避免：银行家算法：分配资源前先评估风险，会不会在分配后导致死锁。即分配给一个进程资源的时候，该进程能否全部返还占用的资源。
  * 死锁检测：建立资源分配表和进程等待表。
  * 解除死锁：从其他进程强制剥夺资源给死锁进程。可以直接撤销死锁进程，或撤销代价最小的进程。

  #### 10.多进程和多线程应用场景

  * 多进程优点：

    ①编程相对容易；通常不需要考虑锁和同步资源的问题。 
    ②更强的容错性：比起多线程的一个好处是一个进程崩溃了不会影响其他进程。 
    ③有内核保证的隔离：数据和错误隔离。 对于使用如C/C++这些语言编写的本地代码，错误隔离是非常有用的：采用多进程架构的程序一般可以做到一定程度的自恢复；（master守护进程监控所有worker进程，发现进程挂掉后将其重启）

  * 多线程优点：

    ①创建速度快，方便高效的数据共享 
    共享数据：多线程间可以共享同一虚拟地址空间；多进程间的数据共享就需要用到共享内存、信号量等IPC技术。
    ②较轻的上下文切换开销 - 不用切换地址空间，不用更改寄存器，不用刷新TLB。 
    ③提供非均质的服务。如果全都是计算任务，但每个任务的耗时不都为1s，而是1ms-1s之间波动；这样，多线程相比多进程的优势就体现出来，它能有效降低“简单任务被复杂任务压住”的概率。

  * 多线程模型适用于I/O密集型场景，因为I/O密集型场景因为I/O阻塞导致频繁切换，线程只占用栈，程序计数器，一组寄存器等少量资源，切换效率高，单机多核分布式；多进程模型适用于需要频繁的计算场景，多机分布式，网络通信

  #### 11.结构体和类字节对齐

  ##### 结构体：

  * 结构体的大小并不是各数据成员之和，而是按照一定的对齐数进行对齐存储，最后结构体的大小也是按照一定的对齐数进行对齐。

  * 对齐规则：

    第一个成员在与结构体变量偏移量为0的地址
    其他成员变量要对齐到对齐数字的整数倍地址处
    对齐数= 编译器默认的对齐数（vs:8,linux:4）与该成员大小较小的值
    结构体总的大小为默认对齐数的整数倍
    注意一点：当内存中的变量变换了顺序后，那么该结构体对象或者该类的对象所占的内存空间也会发生变化。

  ```C
  #include <iostream>
  using namespace std;
  
  class MyStruct
  {
      char dda;    //偏移量为0，满足对齐方式，dda占用1个字节；
      double dda1; //下一个可用的地址的偏移量为1，不是sizeof(double)=8的倍数，需要补足7个字节才能使偏移量变为8（满足对齐方式），因此VC自动填充7个字节，dda1存放在偏移量为8的地址上，它占用8个字节。
      int type;    //下一个可用的地址的偏移量为16，是sizeof(int)=4的倍数，满足int的对齐方式，所以不需要VC自动填充，type存放在偏移量为16的地址上，它占用4个字节。
      //共占20个字节，不是默认偏移量（8）的整数倍，需要进行字节填充，最终占24个字节
  };
  
  class MyStruct1{
      double d1;//偏移量0，满足对齐方式，d1占用8个字节
      char d2;//偏移量为8，满足对其方式（是1的倍数），占用1个字节
      int d3;//偏移量为9，不满足对其方式（不是4的倍数），需要补3个字节，9+3+4=16
      //最终，占16个字节，是默认偏移量（8）的整数倍，不用补充字节
  };
  int main()
  {
      cout << sizeof(MyStruct) << endl;
      cout << sizeof(MyStruct1) << endl;
      return 0;
  }
  ```

  * 进行内存对齐的原因：
    * 平台的移植性好，并不是所有的硬件平台都能存放任意地址的数据，某些平台只能在某些地址访问特定类型的数据，否则就会出现异常
    * cpu处理效率高：cpu并不是把内存看成以字节为单位，而是以块为单位，cpu在读取内存的时候是一块一块的读取的

  ##### 类：

  类本身没有大小，这里类的大小是指：类对象所占的大小。类的大小遵循结构体的对齐规则

  * 类的大小与普通成员函数和静态成员无关（包括：普通成员函数、静态成员函数、静态数据成员、静态常量数据成员），与普通数据成员有关
  * 虚函数对类的大小有影响，是因为虚函数指针的影响
  * 虚继承对类的大小有影响，是因为虚基表指针带来的影响
  * 空类的大小是 1

  ```C++
  #include <bits stdc++.h="">
  using namespace std;
  
  class A{
      //静态成员
      static int a;
      const static int b;    
      static int fun1(){}
  
      //普通成员函数
      void fun(){}
  };
  
  //普通数据成员
  class B{
      int a;    
  };
  
  //虚函数
  class C{
      virtual int fun(){}
  };
  
  //结构体对齐原则
  class D{
      int a;
      virtual int fun(){}
  };
  int main(){
      cout << sizeof(A) << endl;//1
      cout << sizeof(B) << endl;//4:表示一个整型变量的大小
      cout << sizeof(C) << endl;//8：虚函数表的指针的大小
      cout << sizeof(D) << endl;//16：整型变量的大小+虚函数指针的大小+对齐原则（4）
      return 0;
  }
  ```

  

  #### 12.内存管理，段和页和段页

  * 在基本的分页概念中，我们把程序分成**等长**的小块。这些小块叫做“**页（Page）**”，同样内存也被我们分成了和页面同样大小的”**页框（Frame）“，**一个页可以装到一个页框里。在执行程序的时候我们根据一个页表去查找某个页面在内存的某个页框中，由此完成了逻辑到物理的映射。

  ![preview](https://pic2.zhimg.com/v2-e2fabad9ee175e9892f907af06f9fe4e_r.jpg?source=1940ef5c)

  * 分段和分页有很多类似的地方，但是最大的区别在于分页对于用户来说是没什么逻辑意义的，分页是为了完成离散存储，所有的页面大小都一样，对程序员来说这就像碎纸机一样，出来的东西没有完整意义。但是分段不一样，分段不定长，分页由系统完成，分段有时在编译过程中会指定划分，因此可以保留部分逻辑特征，容易实现分段共享。

  ![img](https://img-blog.csdn.net/20130519103349824)

  * 段页式：

    (1) 用分段方法来分配和管理虚拟存储器。程序的地址空间按逻辑单位分成基本独立的段，而每一段有自己的段名，再把每段分成固定大小的若干页。

      (2) 用分页方法来分配和管理实存。即把整个主存分成与上述页大小相等的存储块，可装入作业的任何一页。程序对内存的调入或调出是按页进行的。但它又可按段实现共享和保护。

    ![img](https://img-blog.csdn.net/20130519103644850)

  #### 13.TLB

  是一种内存高速缓存，用于将虚拟内存的最新转换存储到物理地址，以加快检索速度。当程序引用虚拟内存地址时，将在CPU中开始搜索。首先，检查指令缓存。如果所需的内存不在这些非常快的高速缓存中，则系统必须查找内存的物理地址。此时，将检查TLB以快速参考物理内存中的位置。

  #### 14.递归锁

  * 可重入锁指的是同一线程外层函数获得锁之后，内层递归函数仍然能获取该锁的代码，在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁也即是说线程可以进入任何一个它已经拥有的锁所同步着的代码块。

    **可重入锁优点:可避免死锁(不会出现外方法进入后调用一个锁方法,没有锁,一直尝试获得锁陷入死锁状态)**
    
    内核是通过计数器来实现的

  #### 15.用户态和内核态

  * **内核态**：操作系统的模式，如果用户或某程序进入了内核态，那么它的权限就会不受约束，可以做任何事。操作系统向外提供系统调用接口方便进行用户态到内核态的转变。

  * **用户态**：一般用户的模式，用户或某程序在此状态下只能调用用户代码，权限受约束。当用户想调用系统接口，执行内核代码，就要从用户态变成内核态。

  #### 16.宏内核微内核

  ![image-20210121203553811](C:\Users\17362\AppData\Roaming\Typora\typora-user-images\image-20210121203553811.png)

  #### 17.C++的锁，互斥锁，条件变量，自旋锁，读写锁

  当已经有一个线程加锁后，其他线程加锁则就会失败，互斥锁和自旋锁对于加锁失败后的处理方式是不一样的：

  - **互斥锁**加锁失败后，线程会**释放 CPU** ，给其他线程；
  - **自旋锁**加锁失败后，线程会**忙等待**，直到它拿到锁；

  互斥锁是一种「独占锁」，比如当线程 A 加锁成功后，此时互斥锁已经被线程 A 独占了，只要线程 A 没有释放手中的锁，线程 B 加锁就会失败，于是就会释放 CPU 让给其他线程，**既然线程 B 释放掉了 CPU，自然线程 B 加锁的代码就会被阻塞**。

  **对于互斥锁加锁失败而阻塞的现象，是由操作系统内核实现的**。当加锁失败时，内核会将线程置为「睡眠」状态，等到锁被释放后，内核会在合适的时机唤醒线程，当这个线程成功获取到锁后，于是就可以继续执行。如下图：

  ![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%94%81/%E4%BA%92%E6%96%A5%E9%94%81%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png)

  所以，互斥锁加锁失败时，会从用户态陷入到内核态，让内核帮我们切换线程，虽然简化了使用锁的难度，但是存在一定的性能开销成本。

  那这个开销成本是什么呢？会有**两次线程上下文切换的成本**：

  - 当线程加锁失败时，内核会把线程的状态从「运行」状态设置为「睡眠」状态，然后把 CPU 切换给其他线程运行；
  - 接着，当锁被释放时，之前「睡眠」状态的线程会变为「就绪」状态，然后内核会在合适的时间，把 CPU 切换给该线程运行。

  线程的上下文切换的是什么？当两个线程是属于同一个进程，**因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。**

  上下切换的耗时有大佬统计过，大概在几十纳秒到几微秒之间，如果你锁住的代码执行时间比较短，那可能上下文切换的时间都比你锁住的代码执行时间还要长。

  所以，**如果你能确定被锁住的代码执行时间很短，就不应该用互斥锁，而应该选用自旋锁，否则使用互斥锁。**

  自旋锁是通过 CPU 提供的 `CAS` 函数（*Compare And Swap*），在「用户态」完成加锁和解锁操作，不会主动产生线程上下文切换，所以相比互斥锁来说，会快一些，开销也小一些。

  一般加锁的过程，包含两个步骤：

  - 第一步，查看锁的状态，如果锁是空闲的，则执行第二步；
  - 第二步，将锁设置为当前线程持有；

  CAS 函数就把这两个步骤合并成一条硬件级指令，形成**原子指令**，这样就保证了这两个步骤是不可分割的，要么一次性执行完两个步骤，要么两个步骤都不执行。

  使用自旋锁的时候，当发生多线程竞争锁的情况，加锁失败的线程会「忙等待」，直到它拿到锁。这里的「忙等待」可以用 `while` 循环等待实现，不过最好是使用 CPU 提供的 `PAUSE` 指令来实现「忙等待」，因为可以减少循环等待时的耗电量。

  自旋锁是最比较简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。**需要注意，在单核 CPU 上，需要抢占式的调度器（即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。**

  自旋锁开销少，在多核系统下一般不会主动产生线程切换，适合异步、协程等在用户态切换请求的编程方式，但如果被锁住的代码执行时间过长，自旋的线程会长时间占用 CPU 资源，所以自旋的时间和被锁住的代码执行的时间是成「正比」的关系，我们需要清楚的知道这一点。

  自旋锁与互斥锁使用层面比较相似，但实现层面上完全不同：**当加锁失败时，互斥锁用「线程切换」来应对，自旋锁则用「忙等待」来应对**。

  它俩是锁的最基本处理方式，更高级的锁都会选择其中一个来实现，比如读写锁既可以选择互斥锁实现，也可以基于自旋锁实现。不同，条件变量是用来等待而不是用来上锁的。条件变量用来自动阻塞一个线程，直 到某特殊情况发生为止。通常条件变量和互斥锁同时使用。

  ### 条件变量

  条件变量使我们可以睡眠等待某种条件出现。条件变量是利用线程间共享的全局变量进行同步 的一种机制：

  ### 读写锁

  与互斥量类似，不过读写锁允许更改的并行性，也叫共享互斥锁。互斥量要么是锁住状态，要么就是不加锁状态，而且一次只有一个线程可以对其加锁。读写锁可以有3种状态：读模式下加锁状态、写模式加锁状态、不加锁状态。

  所以，**读写锁适用于能明确区分读操作和写操作的场景**。

  读写锁的工作原理是：

  - 当「写锁」没有被线程持有时，多个线程能够并发地持有读锁，这大大提高了共享资源的访问效率，因为「读锁」是用于读取共享资源的场景，所以多个线程同时持有读锁也不会破坏共享资源的数据。
  - 但是，一旦「写锁」被线程持有后，读线程的获取读锁的操作会被阻塞，而且其他写线程的获取写锁的操作也会被阻塞。

  所以说，写锁是独占锁，因为任何时刻只能有一个线程持有写锁，类似互斥锁和自旋锁，而读锁是共享锁，因为读锁可以被多个线程同时持有。

  知道了读写锁的工作原理后，我们可以发现，**读写锁在读多写少的场景，能发挥出优势**。

  另外，根据实现的不同，读写锁可以分为「读优先锁」和「写优先锁」。

  读优先锁期望的是，读锁能被更多的线程持有，以便提高读线程的并发性，它的工作方式是：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程中，后续来的读线程 C 仍然可以成功获取读锁，最后直到读线程 A 和 C 释放读锁后，写线程 B 才可以成功获取写锁。如下图：

  ![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%94%81/%E8%AF%BB%E4%BC%98%E5%85%88%E9%94%81%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png)

  而写优先锁是优先服务写线程，其工作方式是：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程中，后续来的读线程 C 获取读锁时会失败，于是读线程 C 将被阻塞在获取读锁的操作，这样只要读线程 A 释放读锁后，写线程 B 就可以成功获取读锁。如下图：

  ![img](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%94%81/%E5%86%99%E4%BC%98%E5%85%88%E9%94%81%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png)

  读优先锁对于读线程并发性更好，但也不是没有问题。我们试想一下，如果一直有读线程获取读锁，那么写线程将永远获取不到写锁，这就造成了写线程「饥饿」的现象。

  写优先锁可以保证写线程不会饿死，但是如果一直有写线程获取写锁，读线程也会被「饿死」。

  既然不管优先读锁还是写锁，对方可能会出现饿死问题，那么我们就不偏袒任何一方，搞个「公平读写锁」。

  **公平读写锁比较简单的一种方式是：用队列把获取锁的线程排队，不管是写线程还是读线程都按照先进先出的原则加锁即可，这样读线程仍然可以并发，也不会出现「饥饿」的现象。**

  #### 18.系统调用

  * 在用户空间和内核空间之间，有一个叫做Syscall(系统调用, system call)的中间层，是连接用户态和内核态的桥梁。这样即提高了内核的安全型，也便于移植，只需实现同一套接口即可。Linux系统，用户空间通过向内核空间发出Syscall，产生软中断，从而让程序陷入内核态，执行相应的操作。对于每个系统调用都会有一个对应的系统调用号，比很多操作系统要少很多。

  * 安全性与稳定性：内核驻留在受保护的地址空间，用户空间程序无法直接执行内核代码，也无法访问内核数据，通过系统调用可以**受限**访问硬件设备。

  * 性能：Linux上下文切换时间很短，以及系统调用处理过程非常精简，内核优化得好，所以性能上往往比很多其他操作系统执行要好。

  #### 19.僵尸进程，孤儿进程，守护进程，僵尸进程如何解决

  * 守护进城：

    守护进程，也就是通常说的Daemon进程，是Linux中的后台服务进程。它是一个生存期较长的进程，通常独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。守护进程是脱离于终端并且在后台运行的进程。守护进程脱离于终端是为了避免进程在执行过程中的信息在任何终端上显示并且进程也不会被任何终端所产生的终端信息所打断。

  * 僵尸进程：

    僵尸进程产生过程 一个进程在调用exit命令结束自己的生命的时候，其实它并没有真正的被销毁，而是留下一个称为僵尸进程（Zombie）的数据结构（系统调用exit，它的作用是使进程退出，但也仅仅限于将一个正常的进程变成一个僵尸进程，并不能将其完全销毁），无法正常结束，此时即使是root身份kill -9也不能杀死僵死进程。补救办法是杀死僵尸进程的父进程(僵死进程的父进程必然存在)，僵死进程成为”孤儿进程”，过继给1号进程init，init始终会负责清理僵死进程。

  * 孤儿进程

    一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。由于孤儿进程会被init进程给收养，所以孤儿进程不会对系统造成危害。

  #### 20.5种IO模型

  **同步模型（synchronous IO）**

  * 阻塞IO（bloking IO）执行系统调用不能做其他事情

  * 非阻塞IO（non-blocking IO）阻塞后不停轮询问

  * 多路复用IO（multiplexing IO）阻塞后会开另外一条路来查询

  * 信号驱动式IO（signal-driven IO）

  * **异步IO（asynchronous IO）**

    ## 1.阻塞I/O模型

    > 张三去火车站买票，排队三天买到一张退票。
    > 耗费：在车站吃喝拉撒睡 3天，其他事一件没干。
    > 优点: 简单，谁都能排队等
    > 缺点: 耗费时间

    ## 2.非阻塞I/O模型

    > 张三去火车站买票，隔12小时去火车站问有没有退票，三天后买到一张票。
    > 耗费：往返车站6次，路上6小时，其他时间做了好多事。
    > 优点: 不需要一直等，可以省出时间做其它事
    > 缺点: 需要往返多次

    ## 3.I/O复用模型

    ## (1) select/poll

    > 张三去火车站买票，委托黄牛，然后每隔6小时电话黄牛询问，黄牛三天内买到票，然后张三去火车站交钱领票。
    > 耗费：往返车站2次，路上2小时，黄牛手续费100元，打电话17次
    > 优点: 不需要自己等，
    > 缺点: 需要看黄牛手上所有票，而且可能从黄牛手上拿错票

    ## (2) epoll

    > 张三去火车站买票，委托黄牛，黄牛买到后即通知张三去领，然后张三去火车站交钱领票。
    > 耗费：往返车站2次，路上2小时，黄牛手续费100元，通知一次
    > 优点: 不需要等，不会拿错票
    > 缺点: 需要其它资源(黄牛手续费)，还需要自己去取票

    ### IO多路复用模式

    #### Reactor模式
    
    Reactor事件处理机制为：主程序将事件以及对应事件处理的方法在Reactor上进行注册, 如果相应的事件发生，Reactor将会主动调用事件注册的接口，即回调函数。在网络编程中，这个事件就是网络I/O操作。

  　　![img](https://nimg.ws.126.net/?url=http%3A%2F%2Fdingyue.ws.126.net%2F2021%2F0322%2F002a76a8j00qqd6jz000ic000hs008ac.jpg&thumbnail=650x2147483647&quality=80&type=jpg)

  　　通过Reactor的方式，服务器可以将用户线程轮询IO操作状态的工作统一交给 handle_events事件循环进行处理，用户线程注册事件处理器之后可以继续执行做其他的工作。

　　Reactor线程负责调用内核的 select 函数检查socket状态。当有socket被激活时，则通知相应的用户线程（或执行用户线程的回调函数），执行 handle_event 进行数据读取、处理的工作。

#### Proactor模式

　　Proactor模式完全将IO处理和业务分离，使用异步IO模型，即内核完成数据处理后主动通知给应用处理。

　　![img](https://nimg.ws.126.net/?url=http%3A%2F%2Fdingyue.ws.126.net%2F2021%2F0322%2F2c104657j00qqd6jz000gc000hs007qc.jpg&thumbnail=650x2147483647&quality=80&type=jpg)

  


  　　异步IO模型中，用户线程直接使用内核提供的异步IO API发起 read 请求，且发起后立即返回，继续执行用户线程代码。当 read 请求的数据到达时，由内核负责读取socket中的数据，并写入用户指定的缓冲区中。

## 4. 信号驱动I/O模型

> 张三去火车站买票，给售票员留下电话，有票后，售票员电话通知张三，然后张三去火车站交钱领票。
> 耗费：往返车站2次，路上2小时，通知一次
> 优点: 不需要黄牛
> 缺点: 需要其它资源，需要自己去取票

## 5.异步I/O模型

> 张三去火车站买票，给售票员留下电话，有票后，售票员电话通知张三并快递送票上门。
> 耗费：往返车站1次，路上1小时，通知一次，快递一次
> 优点: 不需要自己取票
> 缺点: 需要其它资源 (售票员协助) (快递费)

1同2的区别是：自己轮询
2同3的区别是：委托黄牛
3同4的区别是：电话代替黄牛
4同5的区别是：通知后是自取还是送票上门

## (1) 阻塞式 I/O ( blocking IO )

> 在每一个连接创建时，都需要一个用户线程来处理，并且在 I/O 操作没有就绪或结束时，线程会被挂起，进入阻塞等待状态。
> 在整个 socket 通信工作流程中，socket 的默认状态是阻塞的。也就是说，当发出一个不能立即完成的套接字调用时，其进程将被阻塞，被系统挂起，进入睡眠状态，一直等待相应的操作响应。
> 阻塞式 I/O 就成为了导致性能瓶颈的根本原因。

  >同步： 事情一件件的做，做完一件返回一件，做不完不回复也不返回。
  >
  >阻塞; 阻塞调用是指调用结果返回之前，当前线程会被挂起（线程进入非可执行状态，在这个状态下，cpu不会给线程分配时间片，即线程暂停运行）。函数只有在得到结果之后才会返回。

**connect 阻塞**

> connect 阻塞： 当客户端发起 TCP 连接请求，通过系统调用 connect 函数，TCP 连接的建立需要完成三次握手过程，客户端需要等待服务端发送回来的 ACK 以及 SYN 信号，同样服务端也需要阻塞等待客户端确认连接的 ACK 信号，这就意味着 TCP 的每个 connect 都会阻塞等待，直到确认连接。
>
> 非阻塞模式下，调用connect后会立刻返回EINPROGRESS，同时三次握手还在进行
>
> ```cpp
> int socfd = socket();
> //set non-blocking
> int ret = connect(sockfd, ...);
> if(ret < 0)
> {
>  return -1;
> }
> if(errno == EINPRPGRESS)
> {
>  fd_set rdset, wrset;
>  FD_SET(sockfd, rdset);
>  FD_SET(sockfd, wrset);
>  ret = select(sockfd+1, &rdset, &wrset, NULL, timeout);
>  if(FD_ISSET(sockfd, &wrset))
>  {
>      if(FD_ISSET(sockfd, rdset))
>      {
>          //清楚sock error
>      }
>      //成功连接
>      return sockfd.
>  }
> }
> ```
>
> 非阻塞connect的意义在于提高并发度。阻塞connect下，完成一个三次握手需要耗费一个RTT时间。RTT时间波动很大。从局域网内的几时毫秒到广域网的几十秒。阻塞模式下，进程被connect阻塞住，什么都干不了。非阻塞下，我们可以让select或者epoll来监听listenfd，直到完成三次连接再继续进行数据的手法。

**accept 阻塞**

  > accept 阻塞 : 一个阻塞的 socket 通信的服务端接收外来连接，会调用 accept 函数，如果没有新的连接到达，调用进程将被挂起，进入阻塞状态。
  >
  > 假设一种情况,当客户度端发起连接,完成了三次握手之后.但是此时服务器端因为很忙碌,还没有调用accept函数来确认这个连接.此时客户端使用close函数关闭了连接,注意此时客户端发送的是RST报文而不是FIN报文,所以该链接就在内核中就被断开了,但是此时用户层并不知道这种变化,如果此时服务器端经过epoll触发的listenfd调用阻塞的accept,那么由于连接已经被取消,所以accept就会被阻塞,那么其他注册在epoll中的fd就不能被即使处理,所以我们需要将listenfd设置为非阻塞的,保证完成accept时非阻塞的.
  >
  > 当设置accept是非阻塞的时,如果有连接存在,那么accept正常返回.如果时没有连接存在,或者像上述所说的连接被取消了,那么会返回不同的错误码,我们只需要在accept返回错误后检查这些错误码,如果是这两种情况,把它当做特殊情况不用处理即可.
  >
  > 冷知识：当服务端不调用accept时，客户端还是会connect成功的，因为listen函数会生成accept全连接队列已经存储上了所有连接，而accept函数只起到接受连接的作用

**read、write 阻塞**

> read、write 阻塞 : 当一个 socket 连接创建成功之后，服务端用 fork 函数创建一个子进程， 调用 read 函数等待客户端的数据写入，如果没有数据写入，调用子进程将被挂起，进入阻塞状态。

## (2) 非阻塞式 I/O ( non-blocking IO )

> 非阻塞式I/O 解决了阻塞的问题。

> 内核在没有准备好数据的时候会返回错误码，而调用程序不会休眠，而是不断轮询询问内核数据是否准备好。数据准备好时，函数成功返回。

使用用户线程轮询查看一个 I/O 操作的状态，在大量请求的情况下，非阻塞式IO的轮询会耗费大量cpu。

## (3) I/O 复用

> 类似与非阻塞，只不过轮询不是由用户线程去执行，而是由内核去轮询，内核监听程序监听到数据准备好后，调用内核函数复制数据到用户态。

> Linux 提供了 I/O 复用函数 select/poll/epoll，进程将一个或多个读操作通过系统调用函数，阻塞在函数操作上。这样，系统内核就可以帮我们侦测多个读操作是否处于就绪状态。

Linux 提供了 I/O 复用函数 select/poll/epoll，进程将一个或多个读操作通过系统调用函数，阻塞在函数操作上。这样，系统内核就可以帮我们侦测多个读操作是否处于就绪状态。

> select 会修改传入的参数数组，这个对于一个需要调用很多次的函数，是非常不友好的。
> select 如果任何一个sock(I/O stream)出现了数据，select 仅仅会返回，但是并不会告诉你是哪个sock上有数据，于是你只能自己一个一个的找，10几个sock可能还好，要是几万的sock每次都找一遍，这个无谓的开销就颇有海天盛筵的豪气了。
> select 只能监视1024个链接，linux 定义在头文件中的，参见FD_SETSIZE。
> select 不是线程安全的

> poll 修复了select的很多问题，比如
> poll 去掉了1024个链接的限制
> poll 从设计上来说，不再修改传入数组

> epoll 可以说是I/O 多路复用最新的一个实现，epoll 修复了poll 和select绝大部分问题, 比如：
> epoll 现在是线程安全的。
> epoll 现在不仅告诉你sock组里面数据，还会告诉你具体哪个sock有数据，你不用自己去找了。

epoll只有linux支持

## (4) 信号驱动式I/O（ signal-driven IO ）

> 首先我们允许Socket进行信号驱动IO，并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据。

## (5) 异步非阻塞 I/O（ asynchronous IO ）

> 相对于同步IO，异步IO不是顺序执行。用户进程进行aio_read系统调用之后，无论内核数据是否准备好，都会直接返回给用户进程，然后用户态进程可以去做别的事情。等到socket数据准备好了，内核直接复制数据给进程，然后从内核向进程发送通知。IO两个阶段，进程都是非阻塞的。

  #### 21.线程池

  ##### 线程池基本概念

  线程池是一种线程使用模式，其思想非常类似于内存池，均是从性能出发而开发出来的一种优化技巧。线程池主要用在需要频繁执行较短的任务的情况下，由于短时间内如果进行大量线程的创建与销毁带来的开销是不可接受，因此通常预先创建好一些工作线程，然后在需要使用时直接将任务分派给空闲线程即可，同时可以根据任务数量动态增加减少内存池中的线程数量，在尽量将少资源占用的情况下获得较好的性能。

  ##### 线程池特点

  - 线程池数量限制

  由于线程自身会占用系统资源，带来系统开销，因此个数并不是越多越好，但是如果太少将会降低并发性能，使得任务长期得不到服务，因此线程池数量需要有一个合理值。

  - 适用于大量较短暂任务

  一般而言，线程数量会小于并发任务量，因此如果任务时间很长，甚至和进程生命周期处在同一个数量级上，此时直接使用普通线程即可，无需使用线程池。

  - 线程数量选取

  一般而言，对于IO密集型线程，线程数量一般大于CPU数量，而计算密集型线程数量则取和CPU相同比较合理。

  - 根据任务情况动态增减线程池中线程数量

  初始线程池数量一般不直接创建到最大数量，因此在运行中需要根据任务情况动态进行线程的创建和销毁。

  ##### 线程池基本结构

  线程池的目的以及特点，可总结出线程池以及任务需要实现以下功能：

  ```
  1. 线程池初始化：用于创建一定数量的线程
  2. 线程池增加线程数量：用于任务过多时增加线程数量
  3. 线程池销毁：销毁线程池
  4. 往线程池中增加任务
  ```

  拒绝策略：

  1、直接丢弃（DiscardPolicy）

  2、丢弃队列中最老的任务(DiscardOldestPolicy)。

  3、抛异常(AbortPolicy)

  4、将任务分给调用线程来执行(CallerRunsPolicy)。

  #### 22.中断

  - 按中断源进行分类：发出中断请求的设备称为中断源。按中断源的不同，中断可分为

    内中断：即程序运行错误引起的中断

    外中断：即由外部设备、接口卡引起的中断

    软件中断：由写在程序中的语句引起的中断程序的执行，称为软件中断

  - 允许/禁止(开/关)中断： CPU通过指令限制某些设备发出中断请求，称为屏蔽中断。从CPU要不要接收中断即能不能限制某些中断发生的角度 ，中断可分为

    可屏蔽中断 ：可被CPU通过指令限制某些设备发出中断请求的中断， 那是不是意味着进中断时disable整个中断，其实disable的都是可屏蔽中断？

    不可屏蔽中断：不允许屏蔽的中断如电源掉电

  #### 23.写时复制

  在fork之后exec之前两个进程用的是相同的物理空间（内存区），子进程的代码段、数据段、堆栈都是指向父进程的物理空间，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个。当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间，如果不是因为exec，内核会给子进程的数据段、堆栈段分配相应的物理空间（至此两者有各自的进程空间，互不影响），而代码段继续共享父进程的物理空间（两者的代码完全相同）。而如果是因为exec，由于两者执行的代码不同，子进程的代码段也会分配单独的物理空间。   

  写时复制技术：内核只为新生成的子进程创建虚拟空间结构，它们来复制于父进程的虚拟究竟结构，但是不为这些段分配物理内存，它们共享父进程的物理空间，当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间

  vfork()：这个做法更加火爆，内核连子进程的虚拟地址空间结构也不创建了，直接共享了父进程的虚拟空间，当然了，这种做法就顺水推舟的共享了父进程的物理空间

  #### 24.RAFT的工作原理

  分布式一致性协议

  https://wingsxdu.com/post/algorithms/raft/#gsc.tab=0

  Raft 是分布式领域中的强一致性算法，当其中某个节点收到客户端的一组指令时，它必须与其它节点沟通，以保证所有的节点以相同的顺序收到相同的指令，最终所有的节点会产生一致的结果，就像是一台机器一样。

  - 强领导者（Strong Leader）：Raft 使用一种比其他算法更强的领导形式。例如，日志条目只从领导者发送向其他服务器。这样就简化了对日志复制的管理，使得 Raft 更易于理解。
  - 领导选取（Leader Selection）：Raft 使用随机定时器来选取领导者。这种方式仅仅是在所有算法都需要实现的心跳机制上增加了一点变化，它使得在解决冲突时更简单和快速。
  - 成员变化（Membership Change）：Raft 为了调整集群中成员关系使用了新的联合一致性（joint consensus）的方法，这种方法中大多数不同配置的机器在转换关系的时候会交迭（overlap）。这使得在配置改变的时候，集群能够继续操作。

  #### 25.cache命中率如何提高，结构，组相联

  * **1. 尽量绑定线程到一个CPU上**

    这样就避免了数据缓存在多个cpu cache中。

    **2. 使用Per cpu变量，避免共享变量**

    ​    让每个CPU访问独立的变量或者per cpu的变量，来避免加锁，用空间来换取时间。这是一种很常见的多核编程技巧，一般的简单实现，都是使用数组来实现，其中数组的个数为CPU的个数。

    **3. 共享变量结构定义为Cache line对齐**

    ​    Cache取数据是按照cache line为单位（我们的系统下64Byte），数据跨越两个cache line，就意味着两次load或者两次store。如果数据结构是cache line对齐的，就有可能减少一次读写。数据结构的首地址cache line对齐，意味着可能有内存浪费（特别是数组这样连续分配的数据结构），所以需要在空间和时间两方面权衡。

    例如a[8]，然后每个线程访问固定一个下标来访问，并且存在写操作，这样会使效率暴降，除非把a的数据类型是按CPU的cache line大小定义的。

    ​     我们在看一些开源代码或者内核代码的时候，经常会看到一些关键的数据结构都是按照cache line对其的，比如，内核的**skb**结构是两个cache line大小，牺牲一定的空间来换取时间。

    4. **频繁更新和不频繁更新的数据分开存放，读数据和写数据分开**

    实际操作具有一定的难度。

    5. **数据的分布尽可能的按照访问顺序定义**

    实际操作具有一定的难度。

    6. **减少使用全局变量，增强数据访问的局部性**

       程序的运行存在时间和空间上的局部性，前者是指只要内存中的值被换入缓存，今后一段时间内会被多次引用，后者是指该内存附近的值也被换入缓存。函数操作数据最好都在栈上，因为局部性的数据很可能被载入到cache了，频繁访问局部性数据也会使得对应的cache数据不被频繁替换出cache（在64位机器上，局部变量会优先放到寄存器中），而全局变量存储在全局数据段，在一个被反复调用的函数体内，引用该变量需要对缓存多次换入换出。

        循环体内的代码要尽量精简，因为指令是放在L1指令缓存(I-Cache)里的，而L1指令缓存只有几K字节大小，如果对某段代码需要多次读取，而这段代码又跨越一个L1缓存大小，那么缓存优势将荡然无存。

    增强数据访问的局部性也是减少cache miss的最直接方法，编译器所作的优化很多也只是在不改变程序逻辑的情况下增加数据访问局部性。但编译器这方面能做的很有限。

  * 组相联：将Cache空间分成大小相同的组，主存的一个数据块可以装到组内的任一个位置，即组间采取直接映射，组内采取全相联映射。 如果把Cache分成Q组，每组有R块，那么有： i= j % q 其中i为缓存的组号，j为主存块号主存地址分为三个字段：

  #### 26.零拷贝，对象复用

  零拷贝：不需要cpu参与在内存之间复制数据的操作，减少上下文切换和CPU拷贝时间

  对象的复用：享元模式，通过创建一个对象池，以避免对象的重复创建

  #### 27.大端小端问题（字节序）

  对于整型、长整型等数据类型，Big endian 认为第一个字节是最高位字节（按照从低地址到高地址的顺序存放数据的高位字节到低位字节）；(PowerPC处理器)

  而 Little endian 则相反，它认为第一个字节是最低位字节（按照从低地址到高地址的顺序存放据的低位字节到高位字节）。(Intel8086系列)

  ```C
      int i = 1;   
      char *p = (char *)&i;   
      if(*p == 1)     
            printf("Little Endian"); 
      else
            printf("Big Endian");
  ```

    大小端存储问题，如果小端方式中（i占至少两个字节的长度）则i所分配的内存最小地址那个字节中就存着1，其他字节是0.大端的话则1在i的最高地址字节处存放，char是一个字节，所以强制将char型量p指向i则p指向的一定是i的最低地址，那么就可以判断p中的值是不是1来确定是不是小端。

  为什么会有小端字节序？

  答案是，计算机电路先处理低位字节，效率比较高，因为计算都是从低位开始的。所以，计算机的内部处理都是小端字节序。

  #### 28.程序的内存分布：

  ![浅谈程序中的text段、data段和bss段](https://pic1.zhimg.com/v2-2290195e73159ad0bd364cd3e926c370_1440w.jpg?source=172ae18b)

  一个程序本质上都是由 bss段、data段、text段三个段组成——这是计算机程序设计中重要的基本概念。而且在嵌入式系统的设计中也非常重要，牵涉到嵌入式系统运行时的内存大小分配，存储单元占用空间大小的问题。

  在采用段式内存管理的架构中（比如intel的80x86系统）

  **bss段**（Block Started by Symbol segment）通常是指用来存放程序中未初始化的全局变量的一块内存区域，一般在初始化时bss 段部分将会清零（bss段属于静态内存分配，即程序一开始就将其清零了）。

  比如，在C语言程序编译完成之后，已初始化的全局变量保存在.data 段中，未初始化的全局变量保存在.bss 段中。

  **text段**: 用于存放程序代码的区域， 编译时确定， 只读。更进一步讲是存放处理器的机器指令，当各个源文件单独编译之后生成目标文件，经连接器链接各个目标文件并解决各个源文件之间函数的引用，与此同时，还得将所有目标文件中的.text段合在一起，但不是简单的将它们“堆”在一起就完事，还需要处理各个段之间的函数引用问题。

  在嵌入式系统中，如果处理器是带MMU（MemoryManagement Unit，内存管理单元），那么当我们的可执行程序被加载到内存以后，通常都会将.text段所在的内存空间设置为只读，以保护.text中的代码不会被意外的改写（比如在程序出错时）。当然，如果没有MMU就无法获得这种代码保护功能。

  **data段** :用于存放在编译阶段(而非运行时)就能确定的数据，可读可写。也是通常所说的静态存储区，赋了初值的全局变量、常量和静态变量都存放在这个域。

  ## 29.惊群效应

  惊群效应（thundering herd）是指多进程（多线程）在同时阻塞等待同一个事件的时候（休眠状态），如果等待的这个事件发生，那么他就会唤醒等待的所有进程（或者线程），但是最终却只能有一个进程（线程）获得这个时间的“控制权”，对该事件进行处理，而其他进程（线程）获取“控制权”失败，只能重新进入休眠状态，这种现象和性能浪费就叫做惊群效应。

  Linux 2.6 版本之后，通过引入一个标记位 WQ_FLAG_EXCLUSIVE，解决掉了 accept 惊群效应。

  **NGINX**不让多个进程在同一时间监听接受连接的socket，而是让每个进程轮流监听**，这样当有连接过来的时候，就只有一个进程在监听那肯定就没有惊群的问题。具体做法是：利用一把进程间锁，每个进程中都**尝试**获得这把锁，如果获取成功将监听socket加入wait集合中，并设置超时等待连接到来，没有获得所的进程则将监听socket从wait集合去除。

  ## 30.同一进程内哪些内容共享，哪些不共享

  **线程共享的环境包括：进程代码段、进程的公有数据(利用这些共享的数据，线程很容易的实现相互之间的通讯)、进程打开的文件描述符、信号的处理器、进程的当前目录和进程用户ID与进程组ID。**  

  ​    进程拥有这许多共性的同时，还拥有自己的个性。有了这些个性，线程才能实现并发性。这些个性包括：

     1.线程ID
      每个线程都有自己的线程ID，这个ID在本进程中是唯一的。进程用此来标识线程。

  ​    2.寄存器组的值
  ​     由于线程间是并发运行的，每个线程有自己不同的运行线索，当从一个线程切换到另一个线程上  时，必须将原有的线程的寄存器集合的状态保存，以将来该线程在被重新切换到时能得以恢复。

  ​    3.线程的堆栈
  ​     堆栈是保证线程独立运行所必须的。线程函数可以调用函数，而被调用函数中又是可以层层嵌套的，所以线程必须拥有自己的函数堆栈，  使得函数调用可以正常执行，不受其他线程的影响。

     4.错误返回码
       由于同一个进程中有很多个线程在同时运行，可能某个线程进行系统调用后设置了errno值，而在该  线程还没有处理这个错误，另外一个线程就在此时被调度器投入运行，这样错误值就有可能被修改。
  所以，不同的线程应该拥有自己的错误返回码变量。

  5.线程的信号屏蔽码
        由于每个线程所感兴趣的信号不同，所以线程的信号屏蔽码应该由线程自己管理。但所有的线程都共享同样的信号处理器。 

  6.线程的优先级
        由于线程需要像进程那样能够被调度，那么就必须要有可供调度使用的参数，这个参数就是线程的优先级。 

  ## 31.皮特森算法

  Peterson算法是一个实现互斥锁的并发程序设计算法，核心就是三个标志位是怎样控制两个方法对临界区的访问，

  算法使用两个控制变量flag与turn. 其中flag[n]的值为真，表示ID号为n的进程希望进入该临界区. 标量turn保存有权访问共享资源的进程的ID号.

  ## 32.三个线程依次打印1，2，3

  **竞争型**：每个线程都抢着去打印，如果发现不该自己打印，则准备下一轮抢。由于大家都是竞争的，因此需要用锁机制来保护。

  **协同型**：当前线程线程打印之后通知下一个线程去打印，这种需要确认好第一个线程打印时机。由于是协同型的因此可以不用锁机制来保护，但是需要一个通知机制。

  ### 竞争型打印

  多个线程竞争型打印，优势是代码简单易懂，劣势是线程争抢是CPU调度进行的，可能该某个线程打印时结果该线程迟迟未被CPU调度，结果其他线程被CPU调度到但是由于不能执行打印操作而继续争抢，造成CPU性能浪费。

  3 个线程会抢锁，但是 state 的初始值为 0，所以第一次执行 if 语句的内容只能是 **线程 A**，然后还在 for 循环之内，此时 `state = 1`，只有 **线程 B** 才满足 `1% 3 == 1`，所以第二个执行的是 B，同理只有 **线程 C** 才满足 `2% 3 == 2`，所以第三个执行的是 C，执行完 ABC 之后，才去执行第二次 for 循环，

  ### 协同型打印

  多个线程协同型打印，优势是各个线程使用“通知”机制进行协同分工，理论上执行效率较高，不过要使用对应的“通知”机制。关于如何“通知”，第一种是可使用Java对象的 wait/notify 或者Conditon对象的 await/signal ，第二种是以事件或者提交任务的方式（比如通过提交“待打印数字”这个任务给下一个线程）。

  ## 33.一个游戏服务器一般是用多线程还是多进程

  我们的游戏服务器组是按多进程的方式设计的。强调多进程，是想提另外一点，我们每个进程上是单线程的。所以，我们在设计中，系统的复杂点在于进程间如何交换数据；而不需要考虑线程间的数据锁问题。

  如果肆意的做进程间通讯，在进程数量不断增加后，会使系统混乱不可控。经过分析后，我决定做如下的限制：

  1. 如果一个进程需要和多个服务器做双向通讯，那么这个进程不能处理复杂的逻辑，而只是过滤和转发数据用。即，这样的一个进程 S ，只会把进程 A 发过来的数据转发到 B ；或把进程 B 发过来的数据转发到 A 。或者从一端发过来的数据，经过简单的协议分析后，可以分发到不同的地方。例如，把客户端发过来的数据包中的聊天信息分离处理，交到聊天进程处理。
  2. 有逻辑处理的进程上的数据流一定是单向的，它可以从多个数据源读取数据，但是处理后一定反馈到另外的地方，而不需要和数据源做逻辑上的交互。
  3. 每个进程尽可能的保持单个输入点，或是单个输出点。
  4. 所有费时的操作均发到独立的进程，以队列方式处理。
  5. 按功能和场景划分进程，单一服务和单一场景中不再分离出多个进程做负载均衡。

  性能问题上，我是这样考虑的：

  我们应该充分利用多核的优势，这会是日后的发展方向。让每个进程要么处理大流量小计算量的工作；要么处理小流量大计算量的工作。这样多个进程放在一台物理机器上可以更加充分的利用机器的资源。

  单线程多进程的设计，个人认为更能发挥多核的优势。这是因为没有了锁，每个线程都可以以最大吞吐量工作。增加的负担只是进程间的数据复制，在网游这种复杂逻辑的系统中，一般不会比逻辑计算更早成为瓶颈。

  ## 34.该如何实现并发

  * Thread类定义了多线程，通过多线程可以实现并发或并行。
  * 在CPU比较繁忙，资源不足的时候（开启了很多进程），操作系统只为一个含有多线程的进程分配仅有的CPU资源，这些线程就会为自己尽量多抢时间片，这就是通过多线程实现并发，线程之间会竞争CPU资源争取执行机会。
  * 在CPU资源比较充足的时候，一个进程内的多线程，可以被分配到不同的CPU资源，这就是通过多线程实现并行。
  * 至于多线程实现的是并发还是并行？上面所说，所写多线程可能被分配到一个CPU内核中执行，也可能被分配到不同CPU执行，分配过程是操作系统所为，不可人为控制。所有，如果有人问我我所写的多线程是并发还是并行的？我会说，都有可能。
  * 不管并发还是并行，都提高了程序对CPU资源的利用率，最大限度地利用CPU资源。

  ## 35.现代高并发服务器都是怎么实现的：

  ### 1.多进程

  这种方法的优点就在于：

  - 编程简单，非常容易理解
  - 由于各个进程的地址空间是相互隔离的，因此一个进程崩溃后并不会影响其它进程
  - 充分利用多核资源

  多进程并行处理的优点和明显，但是缺点同样明显：

  - 各个进程地址空间相互隔离，这一优点也会变成缺点，那就是进程间要想通信就会变得比较困难，你需要借助进程间通信(IPC，interprocesscommunications)机制，想一想你现在知道哪些进程间通信机制，然后让你用代码实现呢？显然，进程间通信编程相对复杂，而且性能也是一大问题。
  - 创建进程开销是比线程要大的，频繁的创建销毁进程无疑会加重系统负担。

  ![img](https://static001.geekbang.org/infoq/83/8341a8aed82dab16066b0240bf255b6a.png)

  ### 2.多线程

  由于线程共享进程地址空间，因此线程间通信天然不需要借助任何通信机制，直接读取内存就好了。

  线程创建销毁的开销也变小了，要知道线程就像寄居蟹一样，房子(地址空间)都是进程的，自己只是一个租客，因此非常的轻量级，创建销毁的开销也非常小。

  ![img](https://static001.geekbang.org/infoq/dd/dd8825c737b5b7a802907ca28c5acd63.png)

  由于线程共享进程地址空间，这在为线程间通信带来便利的同时也带来了无尽的麻烦。

  正是由于线程间共享地址空间，因此一个线程崩溃会导致整个进程崩溃退出，同时线程间通信简直太简单了，简单到线程间通信只需要直接读取内存就可以了，也简单到出现问题也极其容易，死锁、线程间的同步互斥、等等，这些极容易产生bug，无数程序员宝贵的时间就有相当一部分用来解决多线程带来的无尽问题。

  虽然线程也有缺点，但是相比多进程来说，线程更有优势，但想单纯的利用多线程就能解决高并发问题也是不切实际的。

  因为虽然线程创建开销相比进程小，但依然也是有开销的，对于动辄数万数十万的链接的高并发服务器来说，创建数万个线程会有性能问题，这包括内存占用、线程间切换，也就是调度的开销。

  ### 3.IO复用

  ![img](https://static001.geekbang.org/infoq/00/008e52f5b7aba2806a22c59299fc2069.png)

  ### 4.事件驱动Event Loop

  这一技术需要两种原料：

  - event
  - 处理event的函数，这一函数通常被称为event handler

  剩下的就简单了：

  你只需要安静的等待event到来就好，当event到来之后，检查一下event的类型，并根据该类型找到对应的event处理函数，也就是event handler，然后直接调用该event handler就好了。

  ![img](https://static001.geekbang.org/infoq/31/31445ec4946d10c14458e3562d98c866.png)

  只需要运行在一个线程或者进程中，只需要这一个event loop就可以同时处理多个用户请求。

  原因很简单，对于web服务器来说，处理一个用户请求时大部分时间其实都用在了I/O操作上，像数据库读写、文件读写、网络读写等。当一个请求到来，简单处理之后可能就需要查询数据库等I/O操作，我们知道I/O是非常慢的，当发起I/O后我们大可以不用等待该I/O操作完成就可以继续处理接下来的用户请求。

  ![img](https://static001.geekbang.org/infoq/e6/e62d2c0655d35ef2781dd7de3d2772cd.png)

  可以分为以下 6 点：

  - 系统拆分
  - 缓存
  - MQ
  - 分库分表
  - 读写分离
  - ElasticSearch

  ![high-concurrency-system-design](https://www.javazhiyin.com/wp-content/uploads/2018/12/high-concurrency-system-design.png)

  ### 系统拆分

  将一个系统拆分为多个子系统，用 dubbo 来搞。然后每个系统连一个数据库，这样本来就一个库，现在多个数据库，不也可以扛高并发么。

  ### 缓存

  缓存，必须得用缓存。大部分的高并发场景，都是**读多写少**，那你完全可以在数据库和缓存里都写一份，然后读的时候大量走缓存不就得了。毕竟人家 redis 轻轻松松单机几万的并发。所以你可以考虑考虑你的项目里，那些承载主要请求的**读场景，怎么用缓存来抗高并发**。

  ### MQ

  MQ，必须得用 MQ。可能你还是会出现高并发写的场景，比如说一个业务操作里要频繁搞数据库几十次，增删改增删改，疯了。那高并发绝对搞挂你的系统，你要是用 redis 来承载写那肯定不行，人家是缓存，数据随时就被 LRU 了，数据格式还无比简单，没有事务支持。所以该用 mysql 还得用 mysql 啊。那你咋办？用 MQ 吧，大量的写请求灌入 MQ 里，排队慢慢玩儿，**后边系统消费后慢慢写**，控制在 mysql 承载范围之内。所以你得考虑考虑你的项目里，那些承载复杂写业务逻辑的场景里，如何用 MQ 来异步写，提升并发性。MQ 单机抗几万并发也是 ok 的，这个之前还特意说过。

  ### 分库分表

  分库分表，可能到了最后数据库层面还是免不了抗高并发的要求，好吧，那么就将一个数据库拆分为多个库，多个库来扛更高的并发；然后将一个表**拆分为多个表**，每个表的数据量保持少一点，提高 sql 跑的性能。

  ### 读写分离

  读写分离，这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都集中在一个库上吧，可以搞个主从架构，**主库写**入，**从库读**取，搞一个读写分离。**读流量太多**的时候，还可以**加更多的从库**。

  ### ElasticSearch

  Elasticsearch，简称 es。es 是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为动不动就可以扩容加机器来扛更高的并发。那么一些比较简单的查询、统计类的操作，可以考虑用 es 来承载，还有一些全文搜索类的操作，也可以考虑用 es 来承载。

  https://www.javazhiyin.com/

  # 36.while(true) 和 CPU占有率

  **linux系统是时间片调度算法，微观上所有可运行进程都是串行，不管进程中作何操作，该进程的时间片一到就切换到下一进程，那为什么一个空循环进程CPU占用率还这么高。**

  CPU的确会切换到别的进程。可以这样理解：当切换到别的进程时，别的进程告诉系统自己没什么事情要做，不需要那么多的时间，这个时候系统就会切换到下一个进程，直到回到这个死循环的进程上，而这个进程无论什么时候都再循环，所以一直会报告有事情要做，系统就会把尽可能多的时间分给他——实际上是这个死循环任务用光了别的进程节省下来的时间。

  可以在while(true)语句中加入别的语句，比如sleep，CPU占有率就会下降很多

  **因为CPU检测到这句后，如果任务队列没有其他任务就会进入休眠状态，于是CPU占有率便降了下来。**

  加入一些Printf函数也可以有一定作用，因为printf会把信息输出到终端，会有IO操作，会让CPU等待

  # 37.32位系统最大只能支持4GB内存原因

  **问题：**

  - 自己装的是4G内存条，可是操作系统显示的内存却是3.75G
  - 自己装的是8G内存条，可是操作系统显示的内存也是3.75G

  在使用计算机时，其支持的最大内存是由操作系统和硬件两方面决定的。

  **硬件方面：**

  地址总线，是用来传输数据所在地址的，而32位系统一般有32根地址总线，那么所能传输的最大数据地址就是2的32次方 ，这里所指的地址是真实的数据地址，即物理地址，CPU在执行指令时需要先将指令的逻辑地址变换为物理地址才能执行。

  |              | 地址总线数目 |                              最大支持内存 |
  | ------------ | :----------: | ----------------------------------------: |
  | **32位系统** |      32      |                            2的32次方 = 4G |
  | **64位系统** |    36或40    | 2的36次方 = 64G 或 2的40次方=  1024G = 1T |

  **系统方面：**

  用户在使用计算机时能够访问的最大内存不单是由CPU地址总线的位数决定的，还需要考虑操作系统的实现。实际上用户在使用计算机时，进程访问到的地址都是逻辑地址，并不是真实的物理地址，逻辑地址是由操作系统提供的，并维护了逻辑地址和物理地址的映射。

  对于32位的windows操作系统，提供的逻辑地址寻址范围是4G，但是对于这4G的逻辑地址，又要划分出来一份给CPU寄存器、ROM的这些物理地址进行映射，那么剩下和内存条的物理地址进行映射的空间肯定没有4G了,如下图所示:

  ![img](https:////upload-images.jianshu.io/upload_images/1329440-ea55740fd9874f7c.png?imageMogr2/auto-orient/strip|imageView2/2/w/600/format/webp)

  其实操作系统显示的内存3.75G，是逻辑地址。

  # 38.Ctrl + C发生了什么事情：

  用户输入命令，在Shell下启动一个前台进程。

  用户按下Ctrl-C，这个键盘输入产生一个硬件中断。

  如果CPU当前正在执行这个进程的代码，则该进程的用户空间代码暂停执行，CPU从用户态切换到内核态处理硬件中断。

  终端驱动程序将Ctrl-C解释成一个SIGINT信号，记在该进程的PCB中（也可以说发送了一个SIGINT信号给该进程）。

  当某个时刻要从内核返回到该进程的用户空间代码继续执行之前，首先处理PCB中记录的信号，发现有一个SIGINT信号待处理，而这个信号的默认处理动作是终止进程，所以直接终止进程而不再返回它的用户空间代码执行。

  注意，Ctrl-C产生的信号只能发给前台进程。一个命令后面加个&可以放到后台运行，这样Shell不必等待进程结束就可以接受新的命令，启动新的进程。Shell可以同时运行一个前台进程和任意多个后台进程，只有前台进程才能接到像Ctrl-C这种控制键产生的信号。前台进程在运行过程中用户随时可能按下Ctrl-C而产生一个信号，也就是说该进程的用户空间代码执行到任何地方都有可能收到SIGINT信号而终止，所以信号相对于进程的控制流程来说是异步（Asynchronous）的。

  # 39.两个线程分别读写和删除一个文件会发生什么：

  学操作系统原理的时候，我们知道，linux是通过link的数量来控制文件删除，只有当一个文件不存在任何link的时候，这个文件才会被删除。

  而每个文件都会有2个link计数器-- i_count 和 i_nlink。i_count的意义是当前使用者的数量，也就是打开文件进程的个数。i_nlink的意义是介质连接的数量；或者可以理解为 i_count是内存引用计数器，i_nlink是硬盘引用计数器。再换句话说，当文件被某个进程引用时，i_count 就会增加；当创建文件的硬连接的时候，i_nlink 就会增加。

  对于 rm 而言，就是减少 i_nlink。这里就出现一个问题，如果一个文件正在被某个进程调用，而用户却执行 rm 操作把文件删除了，会出现什么结果呢？

  当用户执行 rm 操作后，ls 或者其他文件管理命令不再能够找到这个文件，但是进程却依然在继续正常执行，依然能够从文件中正确的读取内容。这是因为，rm 操作只是将 i_nlink 置为 0 了；由于文件被进程引用的缘故，i_count 不为 0，所以系统没有真正删除这个文件。i_nlink 是文件删除的充分条件，而 i_count 才是文件删除的必要条件。

  基于以上只是，大家猜一下，如果在一个进程在打开文件写日志的时候，手动或者另外一个进程将这个日志删除，会发生什么情况？

  是的，数据库并没有停掉。虽然日志文件被删除了，但是有一个进程已经打开了那个文件，所以向那个文件中的写操作仍然会成功，数据仍然会提交。

  下面，告诉大家如何恢复那个删除的文件。

  例如，你删除了tcpdump.log，执行lsof | grep tcpdump.log，你应该能看到这样的输出：

  tcpdump 2864 tcpdump 4w REG 253,0 0 671457 /root/tcpdump.log (deleted)

  然后：

  cp /proc/2864/fd/4 /root/tcpdump.log

  # 40.链接过程发生了什么

  1. **地址与空间分配（Address and Storage Allocation）**
  2. **符号解析（Symbol Resolution）**
  3. **重定位（Relocation）**

  - **第一步 地址与空间分配**
    扫描所有的输入目标文件，获得它们的各个节的长度、属性、位置，并将输入目标文件中的符号表中所有的符号定义和符号引用收集起来，统一放到一个全局的符号表。这一步，链接器能够获得所有输入目标文件的节的长度，并将它们合并，计算出输出文件中各个节合并后的长度与位置，并建立映射关系。

  - **第二步 符号解析与重定位**
    使用前一步中收集到的所有信息，读取输入文件中节的输数据、重定位信息，并且进行符号解析与重定位、调整代码、调整代码中的地址等。事实上，第二步是链接过程的核心，尤其是重定位。

  - 重定位过程也伴随着符号的解析过程。链接的前两步完成之后，链接器就已经确定所有符号的虚拟地址了，那么链接器就可以根据符号的地址对每个需要重定位的指令进行地址修正。

    那么链接器如何知道哪些指令是要被调整的呢？事实上，ELF文件中的 **重定位表（Relocation Table）** 专门用来保存这些与重定位相关的信息。

  # 计算机网络

  #### 1.TCP过程，如何保证可靠性，

  * 可靠性，TCP通过以下方式来提供可靠性：

    * 应用数据被分割成TCP认为最适合发送的数据块。这和UDP完全不同，应用程序产生的数据报长度将保持不变（将数据截断为合理的长度）

    * 当TCP发出一个段后，它启动一个定时器。等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。(超时重发)

    * 当TCP收到发自TCP连接另一端数据，它将发送一个确认。这个确认不是立即发送，通常推迟几分之一秒用来对包的完整性进行校验。

    * TCP将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP将丢弃这个报文段和不确认收到此报文段。 (校验出包有错，丢弃报文段，不给出响应，TCP发送数据端，超时时会重发数据)。

    * 既然TCP报文段作为IP数据报来传输，而IP数据报的到达可能会失序，因此TCP报文段的到达也可能会失序。如果必要，TCP将对收到的数据进行重新排序，将收到的数据以正确的顺序交给应用层。 (对失序数据进行重新排序，然后才交给应用层)。

    * 既然IP数据报会发生重复，TCP的接收端必须丢弃重复的数据。(对于重复数据，能够丢弃重复数据)。

    * TCP还能提供流量控制。

      如果发送者发送数据过快，接收者来不及接收，那么就会有分组丢失。为了避免分组丢失，控制发送者的发送速度，使得接收者来得及接收，这就是流量控制。流量控制根本目的是防止分组丢失，它是构成TCP可靠性的一方面。

      TCP连接的每一方都有固定大小的缓冲空间。TCP的接收端只允许另一端发送接收端缓冲区所能接纳的数据。这将防止较快主机致使较慢主机的缓冲区溢出。(TCP可以进行流量控制，防止较快主机致使较慢主机的缓冲区溢出)TCP使用的流量控制协议是可变大小的滑动窗口协议。
      滑动窗口协议机制建议参见：http://blog.chinaunix.net/uid-26275986-id-4109679.html

      ## 滑动窗口协议：

      - TCP协议的使用
      - 维持发送方/接收方缓冲区 缓冲区是 用来解决网络之间数据不可靠的问题，例如丢包，重复包，出错，乱序

      在TCP协议中，发送方和接受方通过各自维护自己的缓冲区。通过商定包的重传机制等一系列操作，来解决不可靠的问题。

      ## 问题一：如何保证次序？

      > 提出问题：在我们滑动窗口协议之前，我们如何来保证发送方与接收方之间，每个包都能被收到。并且是按次序的呢？

      

      ![问题1](https://user-gold-cdn.xitu.io/2019/3/30/169cd8c84bdacece?w=880&h=411&f=png&s=146266)

      发送方发送一个包1，这时候接收方确认包1。发送包2，确认包2。就这样一直下去，知道把数据完全发送完毕，这样就结束了。那么就解决了丢包，出错，乱序等一些情况！同时也存在一些问题。问题：吞吐量非常的低。我们发完包1，一定要等确认包1.我们才能发送第二个包。

      

      ## 问题二：如何提高吞吐量？

      > 提出问题：那么我们就不能先连发几个包等他一起确认吗？这样的话，我们的速度会不会更快，吞吐量更高些呢？

      

      ![改进方案](https://user-gold-cdn.xitu.io/2019/3/30/169cd8c84bc6e732?w=841&h=345&f=png&s=133907)

      如图，这个就是我们把两个包一起发送，然后一起确认。可以看出我们改进的方案比之前的好很多，所花的时间只是一个来回的时间。接下来，我们还有一个问题：改善了吞吐量的问题

      

      ## 问题三：如何实现最优解？

      > 问题：我们每次需要发多少个包过去呢？发送多少包是最优解呢？

      我们能不能把第一个和第二个包发过去后，收到第一个确认包就把第三个包发过去呢？而不是去等到第二个包的确认包才去发第三个包。这样就很自然的产生了我们"滑动窗口"的实现。

      ![实现](https://user-gold-cdn.xitu.io/2019/3/30/169cd8c84bee5e77?w=895&h=281&f=png&s=92339)

      在图中，我们可看出灰色1号2号3号包已经发送完毕，并且已经收到Ack。这些包就已经是过去式。4、5、6、7号包是黄色的，表示已经发送了。但是并没有收到对方的Ack，所以也不知道接收方有没有收到。8、9、10号包是绿色的。是我们还没有发送的。这些绿色也就是我们接下来马上要发送的包。 可以看出我们的窗口正好是11格。后面的11-16还没有被读进内存。要等4号-10号包有接下来的动作后，我们的包才会继续往下发送。

      

      ### 正常情况

      

      ![正常情况](https://user-gold-cdn.xitu.io/2019/3/30/169cd8c84c05c552?w=861&h=194&f=png&s=45184)

      可以看到4号包对方已经被接收到，所以被涂成了灰色。“窗口”就往右移一格，这里只要保证“窗口”是7格的。 我们就把11号包读进了我们的缓存。进入了“待发送”的状态。8、9号包已经变成了黄色，表示已经发送出去了。接下来的操作就是一样的了，确认包后，窗口往后移继续将未发送的包读进缓存，把“待发送“状态的包变为”已发送“。

      

      ### 丢包情况

      有可能我们包发过去，对方的Ack丢了。也有可能我们的包并没有发送过去。从发送方角度看就是我们没有收到Ack。

      ![丢包](https://user-gold-cdn.xitu.io/2019/3/30/169cd8c84c92d41b?w=859&h=222&f=png&s=59761)

      发生的情况：一直在等Ack。如果一直等不到的话，我们也会把读进缓存的待发送的包也一起发过去。但是，这个时候我们的窗口已经发满了。所以并不能把12号包读进来，而是始终在等待5号包的Ack。

      

      > 如果我们这个Ack始终不来怎么办呢？

      ### 超时重发

      这时候我们有个解决方法：`超时重传` 这里有一点要说明：这个Ack是要按顺序的。必须要等到5的Ack收到，才会把6-11的Ack发送过去。这样就保证了滑动窗口的一个顺序。

      ![超时重发](https://user-gold-cdn.xitu.io/2019/3/30/169cd8c84caf97fd?w=875&h=207&f=png&s=65982)

      这时候可以看出5号包已经接受到Ack，后面的6、7、8号包也已经发送过去已Ack。窗口便继续向后移动。

    * 字节流服务
      两个应用程序通过TCP连接交换8bit字节构成的字节流。TCP不在字节流中插入记录标识符。我们将这称为字节流服务（bytestreamservice）。
      TCP对字节流的内容不作任何解释:: TCP对字节流的内容不作任何解释。TCP不知道传输的数据字节流是二进制数据，还是ASCII字符、EBCDIC字符或者其他类型数据。对字节流的解释由TCP连接双方的应用层解释。

    * 7、拥塞控制

      名词解释

      开始具体介绍这些算法之前，需要先对几个名词进行一下解释

      - rwnd: 接收窗口，由 TCP 的接收端确定
      - cwnd: 拥塞窗口，由 TCP 的发送端视当前网络状况而定

      rwnd 说明了接收端当前可以接收的最大数据量，cwnd 说明了当前网络情况允许发送的最大数据量。因此，实际可以发送的最大数据量就是 `min(rwnd, cwnd)`

      - [MSS (Maximum Segment Size): 最大分段大小](https://zh.wikipedia.org/wiki/最大分段大小)
      - [RTT (Round Trip Time): 网络往返时间](https://zh.wikipedia.org/wiki/來回通訊延遲)
      - RTO (Retransmission TimeOut): 重传超时时间
      - ssthresh: 慢启动门限，如果当前拥塞窗口小于此值则触发慢启动（就是快速增长），其初始值是由系统定义的

      ## 慢启动

      在网络中，如果某个路由器节点没有能够及时的将数据传递出去，那么这些数据将会挤压在路由器的缓存内，等待稍后处理。而如果挤压的数据量超过路由器缓存的承载量的话，多余的数据将会被直接丢弃。

      为了防止大量的数据导致网络阻塞，一个新建立的连接在开始的时候会将 cwnd 初始化为 1，即只会发送一个 MSS 的数据，然后随着报文段被确认而逐渐增长。这也是慢启动这个名称的来源。

      虽然起点很低，但是 cwnd 是指数级增长的，每次报文被确认都会在原有的基础上乘 2。

      1. 初始 -> cwnd = 1
      2. 经过一个 RTT -> cwnd = 1 * 2 = 2
      3. 经过两个 RTT -> cwnd = 2 * 2 = 4
      4. 经过三个 RTT -> cwnd = 4 * 2 = 8 ……

      可见，虽然 cwnd 的初始值很小，但是增长极快，很快就会达到一个很大的数字。

      ## 拥塞避免

      前面提到，慢启动的增长速度极快，因此如果一直使用慢启动的话，网络必定是承受不住的。因此需要一个限制来确保传输不会过快，这个限制就是 ssthresh，而这个限制的过程就是拥塞避免。

      在当前的 cwnd 值小于 ssthresh 时，TCP 执行慢启动，以极快的速度增长。当 cwnd 值达到（即大于等于） ssthresh 时，开始执行拥塞避免。在此阶段，每经过一个 RTT，cwnd 将不再在原有基础上乘 2，而是在原有基础上加 1。从而由指数增长转为线性增长。

      ## 如果出现了丢包

      上面的两个算法确实能让 TCP 较快的开始，同时在某个阈值之后放慢增长速度，从而不至于爆炸增长。但是，线性增长也是在增长，当到达网络的极限之后，就会出现丢包。

      当 TCP 超过 RTO 还没有收到对数据的确认的话，将会调整自己的发送速率，主要做以下几件事：

      - 将此次连接的 ssthresh 设置为当前 cwnd 值的一半
      - 将 cwnd 值设置为 1，重新进入慢启动阶段

    * 

      ## 快重传

      以上的两个算法基本已经可以尽可能的提高网络的利用率了，但还是有可以优化的地方。网络环境瞬息万变，如果每个包的丢失都要等一个 RTO 才能重传的话，会浪费很多时间。

      从我之前的博客[《TCP 的滑动窗口》](https://meik2333.com/posts/tcp-sliding-window/)可以看出，TCP 会同时发送多个包，但对每个包的确认并不是这个包的字节编号，而是已经完整收到的最后一个包的下一个字节。

      也就是说，如果发送了 1、2、3、4、5 五个包，1 到达的时候确认编号将是 2；如果此时 3 早于 2 到达，那么确认号将仍是 2 而不是 4。如果 2 丢失了的话，3、4、5 到达，将会产生三个编号为 2 的确认。

      快重传就利用了这个特性，当发送端连续收到 3 个重复的 ACK 时，将会触发快重传机制。发送端立即重发丢失的包，然后进入快恢复阶段。

      ## 快恢复

      1. 将 ssthresh 设置为当前 cwnd 值的一半
      2. 将 cwnd 的值设置为 ssthresh + 3，加 3 是因为收到了三个重复的 ACK，表示有三个“老”的数据包离开了网络
      3. 之后如果继续收到重复的 ACK 的话，每个重复的 ACK 将导致当前 cwnd + 1
      4. 当收到一个新的 ACK 的时候，快恢复结束。将 cwnd 的值设置为 ssthresh，重新进入拥塞避免阶段

  #### 2.ISO模型和TCP/IP模型，其中的协议

  | OSI七层网络模型         | TCP/IP四层概念模型                   | 对应网络协议                            |
  | ----------------------- | ------------------------------------ | --------------------------------------- |
  | 应用层（Application）   | 应用层                               | HTTP、TFTP, FTP, NFS, WAIS、SMTP        |
  | 表示层（Presentation）  | Telnet, Rlogin, SNMP, Gopher         |                                         |
  | 会话层（Session）       | SMTP, DNS                            |                                         |
  | 传输层（Transport）     | 传输层                               | TCP, UDP                                |
  | 网络层（Network）       | 网络层                               | IP, ICMP, ARP, RARP, AKP, UUCP          |
  | 数据链路层（Data Link） | 数据链路层                           | FDDI, Ethernet, Arpanet, PDN, SLIP, PPP |
  | 物理层（Physical）      | IEEE 802.1A, IEEE 802.2到IEEE 802.11 |                                         |

  ![2018041112053246](Z:\blog\source\_posts\面试\2018041112053246.png)

  其中IEE 802只是针对不同的网络结构有不同以太帧格式

  #### 3.HTTP和HTTPS的区别，具体内容HTTP2.0，HTTP2.0的多路复用，HTTP1.1 Pipeline起了什么作用

  * HTTPS 协议提供了三个关键的指标

    - `加密(Encryption)`， HTTPS 通过对数据加密来使其免受窃听者对数据的监听，这就意味着当用户在浏览网站时，没有人能够监听他和网站之间的信息交换，或者跟踪用户的活动，访问记录等，从而窃取用户信息。

    - `数据一致性(Data integrity)`，数据在传输的过程中不会被窃听者所修改，用户发送的数据会`完整`的传输到服务端，保证用户发的是什么，服务器接收的就是什么。

    - `身份认证(Authentication)`，是指确认对方的真实身份，也就是`证明你是你`（可以比作人脸识别），它可以防止中间人攻击并建立用户信任。

    - `TLS(Transport Layer Security)` 是 `SSL(Secure Socket Layer)` 的后续版本，它们是用于在互联网两台计算机之间用于`身份验证`和`加密`的一种协议。

      > 注意：在互联网中，很多名称都可以进行互换。

      我们都知道一些在线业务（比如在线支付）最重要的一个步骤是创建一个值得信赖的交易环境，能够让客户安心的进行交易，SSL/TLS 就保证了这一点，SSL/TLS 通过将称为 `X.509` 证书的数字文档将网站和公司的实体信息绑定到`加密密钥`来进行工作。每一个`密钥对(key pairs)` 都有一个 `私有密钥(private key)` 和 `公有密钥(public key)`，私有密钥是独有的，一般位于服务器上，用于解密由公共密钥加密过的信息；公有密钥是公有的，与服务器进行交互的每个人都可以持有公有密钥，用公钥加密的信息只能由私有密钥来解密。

      > 什么是 `X.509`：X.509 是`公开密钥`证书的标准格式，这个文档将加密密钥与（个人或组织）进行安全的关联。
      >
      > X.509 主要应用如下
      >
      > - SSL/TLS 和 HTTPS 用于经过身份验证和加密的 Web 浏览
      >
      > - 通过 [S/MIME](https://www.ssl.com/article/sending-secure-email-with-s-mime/) 协议签名和加密的电子邮件
      >
      > - 代码签名：它指的是使用数字证书对软件应用程序进行签名以安全分发和安装的过程。
      >
      > 

  * SSL 即`安全套接字层`，它在 OSI 七层网络模型中处于第五层，SSL 在 1999 年被 `IETF(互联网工程组)`更名为 TLS ，即`传输安全层`，直到现在，TLS 一共出现过三个版本，1.1、1.2 和 1.3 ，目前最广泛使用的是 1.2，所以接下来的探讨都是基于 TLS 1.2 的版本上的。

    TLS 用于两个通信应用程序之间提供保密性和数据完整性。TLS 由**记录协议、握手协议、警告协议、变更密码规范协议、扩展协议**等几个子协议组成，综合使用了**对称加密、非对称加密、身份认证**等许多密码学前沿技术（如果你觉得一项技术很简单，那你只是没有学到位，任何技术都是有美感的，牛逼的人只是欣赏，并不是贬低）。

    说了这么半天，我们还没有看到 TLS 的命名规范呢，下面举一个 TLS 例子来看一下 TLS 的结构（可以参考 https://www.iana.org/assignments/tls-parameters/tls-parameters.xhtml）

    ```http
    ECDHE-ECDSA-AES256-GCM-SHA384
    ```

    这是啥意思呢？我刚开始看也有点懵啊，但其实是有套路的，因为 TLS 的密码套件比较规范，基本格式就是 **密钥交换算法 - 签名算法 - 对称加密算法 - 摘要算法** 组成的一个密码串，有时候还有`分组模式`，我们先来看一下刚刚是什么意思

    使用 ECDHE 进行密钥交换，使用 ECDSA 进行签名和认证，然后使用 AES 作为对称加密算法，密钥的长度是 256 位，使用 GCM 作为分组模式，最后使用 SHA384 作为摘要算法。

    TLS 在根本上使用`对称加密`和 `非对称加密` 两种形式。

    ### 对称加密

    在了解对称加密前，我们先来了解一下`密码学`的东西，在密码学中，有几个概念：**明文、密文、加密、解密**

    - `明文(Plaintext)`，一般认为明文是有意义的字符或者比特集，或者是通过某种公开编码就能获得的消息。明文通常用 m 或 p 表示
    - `密文(Ciphertext)`，对明文进行某种加密后就变成了密文
    - `加密(Encrypt)`，把原始的信息（明文）转换为密文的信息变换过程
    - `解密(Decrypt)`，把已经加密的信息恢复成明文的过程。

    `对称加密(Symmetrical Encryption)`顾名思义就是指**加密和解密时使用的密钥都是同样的密钥**。只要保证了密钥的安全性，那么整个通信过程也就是具有了机密性。

    ![img](https://img2020.cnblogs.com/blog/1515111/202003/1515111-20200314103224183-1197326850.png)

    TLS 里面有比较多的加密算法可供使用，比如 DES、3DES、AES、ChaCha20、TDEA、Blowfish、RC2、RC4、RC5、IDEA、SKIPJACK 等。目前最常用的是 AES-128, AES-192、AES-256 和 ChaCha20。

    `DES` 的全称是 `Data Encryption Standard(数据加密标准)` ，它是用于数字数据加密的对称密钥算法。尽管其 56 位的短密钥长度使它对于现代应用程序来说太不安全了，但它在加密技术的发展中具有很大的影响力。

    `3DES` 是从原始数据加密标准（DES）衍生过来的加密算法，它在 90 年代后变得很重要，但是后面由于更加高级的算法出现，3DES 变得不再重要。

    AES-128, AES-192 和 AES-256 都是属于 AES ，AES 的全称是`Advanced Encryption Standard(高级加密标准)`，它是 DES 算法的替代者，安全强度很高，性能也很好，是应用最广泛的对称加密算法。

    `ChaCha20` 是 Google 设计的另一种加密算法，密钥长度固定为 256 位，纯软件运行性能要超过 AES，曾经在移动客户端上比较流行，但 ARMv8 之后也加入了 AES 硬件优化，所以现在不再具有明显的优势，但仍然算得上是一个不错算法。

    （其他可自行搜索）

    #### 加密分组

    对称加密算法还有一个`分组模式` 的概念，对于 GCM 分组模式，只有和 AES，CAMELLIA 和 ARIA 搭配使用，而 AES 显然是最受欢迎和部署最广泛的选择，**它可以让算法用固定长度的密钥加密任意长度的明文。**

    最早有 ECB、CBC、CFB、OFB 等几种分组模式，但都陆续被发现有安全漏洞，所以现在基本都不怎么用了。最新的分组模式被称为 `AEAD（Authenticated Encryption with Associated Data）`，在加密的同时增加了认证的功能，常用的是 GCM、CCM 和 Poly1305。

    比如 `ECDHE_ECDSA_AES128_GCM_SHA256` ，表示的是具有 128 位密钥， AES256 将表示 256 位密钥。GCM 表示具有 128 位块的分组密码的现代认证的关联数据加密（AEAD）操作模式。

    我们上面谈到了对称加密，对称加密的加密方和解密方都使用同一个`密钥`，也就是说，加密方必须对原始数据进行加密，然后再把密钥交给解密方进行解密，然后才能解密数据，这就会造成什么问题？这就好比《小兵张嘎》去送信（信已经被加密过），但是嘎子还拿着解密的密码，那嘎子要是在途中被鬼子发现了，那这信可就是被完全的暴露了。所以，对称加密存在风险。

    ### 非对称加密

    `非对称加密(Asymmetrical Encryption)` 也被称为`公钥加密`，相对于对称加密来说，非对称加密是一种新的改良加密方式。密钥通过网络传输交换，它能够确保及时密钥被拦截，也不会暴露数据信息。非对称加密中有两个密钥，一个是公钥，一个是私钥，公钥进行加密，私钥进行解密。公开密钥可供任何人使用，私钥只有你自己能够知道。

    ![img](https://img2020.cnblogs.com/blog/1515111/202003/1515111-20200314103234231-1641743561.png)

    使用公钥加密的文本只能使用私钥解密，同时，使用私钥加密的文本也可以使用公钥解密。公钥不需要具有安全性，因为公钥需要在网络间进行传输，非对称加密可以解决`密钥交换`的问题。网站保管私钥，在网上任意分发公钥，你想要登录网站只要用公钥加密就行了，密文只能由私钥持有者才能解密。而黑客因为没有私钥，所以就无法破解密文。

    非对称加密算法的设计要比对称算法难得多（我们不会探讨具体的加密方式），常见的比如 DH、DSA、RSA、ECC 等。

    其中 `RSA` 加密算法是最重要的、最出名的一个了。例如 `DHE_RSA_CAMELLIA128_GCM_SHA256`。它的安全性基于 `整数分解`，使用两个超大素数的乘积作为生成密钥的材料，想要从公钥推算出私钥是非常困难的。

    `ECC（Elliptic Curve Cryptography）`也是非对称加密算法的一种，它基于`椭圆曲线离散对数`的数学难题，使用特定的曲线方程和基点生成公钥和私钥， ECDHE 用于密钥交换，ECDSA 用于数字签名。

    TLS 是使用`对称加密`和`非对称加密` 的混合加密方式来实现机密性。

    ### 混合加密

    RSA 的运算速度非常慢，而 AES 的加密速度比较快，而 TLS 正是使用了这种`混合加密`方式。在通信刚开始的时候使用非对称算法，比如 RSA、ECDHE ，首先解决`密钥交换`的问题。然后用随机数产生对称算法使用的`会话密钥（session key）`，再用`公钥加密`。对方拿到密文后用`私钥解密`，取出会话密钥。这样，双方就实现了对称密钥的安全交换。

    ![img](https://img2020.cnblogs.com/blog/1515111/202003/1515111-20200314103243596-1560928278.png)

    现在我们使用混合加密的方式实现了机密性，是不是就能够安全的传输数据了呢？还不够，在机密性的基础上还要加上`完整性`、`身份认证`的特性，才能实现真正的安全。而实现完整性的主要手段是 `摘要算法(Digest Algorithm)`

    ### 摘要算法

    如何实现完整性呢？在 TLS 中，实现完整性的手段主要是 `摘要算法(Digest Algorithm)`。摘要算法你不清楚的话，MD5 你应该清楚，MD5 的全称是 `Message Digest Algorithm 5`，它是属于`密码哈希算法(cryptographic hash algorithm)`的一种，MD5 可用于从任意长度的字符串创建 128 位字符串值。尽管 MD5 存在不安全因素，但是仍然沿用至今。MD5 最常用于`验证文件`的完整性。但是，它还用于其他安全协议和应用程序中，例如 SSH、SSL 和 IPSec。一些应用程序通过向明文加盐值或多次应用哈希函数来增强 MD5 算法。

    > 什么是加盐？在密码学中，`盐`就是一项随机数据，用作哈希数据，密码或密码的`单向`函数的附加输入。盐用于保护存储中的密码。例如
    >
    > ![img](https://img2020.cnblogs.com/blog/1515111/202003/1515111-20200314103259737-2039259071.png)
    >
    > 什么是单向？就是在说这种算法没有密钥可以进行解密，只能进行单向加密，加密后的数据无法解密，不能逆推出原文。

    我们再回到摘要算法的讨论上来，其实你可以把摘要算法理解成一种特殊的压缩算法，它能够把任意长度的数据`压缩`成一种固定长度的字符串，这就好像是给数据加了一把锁。

    ![img](https://img2020.cnblogs.com/blog/1515111/202003/1515111-20200314103319423-1341911060.png)

    除了常用的 MD5 是加密算法外，`SHA-1(Secure Hash Algorithm 1)` 也是一种常用的加密算法，不过 SHA-1 也是不安全的加密算法，在 TLS 里面被禁止使用。目前 TLS 推荐使用的是 SHA-1 的后继者：`SHA-2`。

    SHA-2 的全称是`Secure Hash Algorithm 2` ，它在 2001 年被推出，它在 SHA-1 的基础上做了重大的修改，SHA-2 系列包含六个哈希函数，其摘要（哈希值）分别为 224、256、384 或 512 位：**SHA-224, SHA-256, SHA-384, SHA-512**。分别能够生成 28 字节、32 字节、48 字节、64 字节的摘要。

    有了 SHA-2 的保护，就能够实现数据的完整性，哪怕你在文件中改变一个标点符号，增加一个空格，生成的文件摘要也会完全不同，不过 SHA-2 是基于明文的加密方式，还是不够安全，那应该用什么呢？

    安全性更高的加密方式是使用 `HMAC`，在理解什么是 HMAC 前，你需要先知道一下什么是 MAC。

    MAC 的全称是`message authentication code`，它通过 MAC 算法从消息和密钥生成，MAC 值允许验证者（也拥有秘密密钥）检测到消息内容的任何更改，从而保护了消息的数据完整性。

    HMAC 是 MAC 更进一步的拓展，它是使用 MAC 值 + Hash 值的组合方式，HMAC 的计算中可以使用任何加密哈希函数，例如 SHA-256 等。

    ![img](https://img2020.cnblogs.com/blog/1515111/202003/1515111-20200314103329536-1028852062.png)

    现在我们又解决了完整性的问题，那么就只剩下一个问题了，那就是`认证`，认证怎么做的呢？我们再向服务器发送数据的过程中，黑客（攻击者）有可能伪装成任何一方来窃取信息。它可以伪装成你，来向服务器发送信息，也可以伪装称为服务器，接受你发送的信息。那么怎么解决这个问题呢？

    ### 认证

    如何确定你自己的唯一性呢？我们在上面的叙述过程中出现过公钥加密，私钥解密的这个概念。提到的私钥只有你一个人所有，能够辨别唯一性，所以我们可以把顺序调换一下，变成私钥加密，公钥解密。使用私钥再加上摘要算法，就能够实现`数字签名`，从而实现认证。

    ![img](https://img2020.cnblogs.com/blog/1515111/202003/1515111-20200314103337266-971088165.png)

    到现在，综合使用对称加密、非对称加密和摘要算法，我们已经实现了**加密、数据认证、认证**，那么是不是就安全了呢？非也，这里还存在一个**数字签名的认证问题**。因为私钥是是自己的，公钥是谁都可以发布，所以必须发布经过认证的公钥，才能解决公钥的信任问题。

    所以引入了 `CA`，CA 的全称是 `Certificate Authority`，证书认证机构，你必须让 CA 颁布具有认证过的公钥，才能解决公钥的信任问题。

    全世界具有认证的 CA 就几家，分别颁布了 DV、OV、EV 三种，区别在于可信程度。DV 是最低的，只是域名级别的可信，EV 是最高的，经过了法律和审计的严格核查，可以证明网站拥有者的身份（在浏览器地址栏会显示出公司的名字，例如 Apple、GitHub 的网站）。不同的信任等级的机构一起形成了层级关系。

    ![img](https://img2020.cnblogs.com/blog/1515111/202003/1515111-20200314103348932-1268001810.png)

    通常情况下，数字证书的申请人将生成由私钥和公钥以及证书`签名请求（CSR）`组成的密钥对。CSR是一个编码的文本文件，其中包含公钥和其他将包含在证书中的信息（例如域名，组织，电子邮件地址等）。密钥对和 CSR生成通常在将要安装证书的服务器上完成，并且 CSR 中包含的信息类型取决于证书的验证级别。与公钥不同，申请人的私钥是安全的，永远不要向 CA（或其他任何人）展示。

    生成 CSR 后，申请人将其发送给 CA，CA 会验证其包含的信息是否正确，如果正确，则使用颁发的私钥对证书进行数字签名，然后将其发送给申请人。

    ![img](https://img2020.cnblogs.com/blog/1515111/202003/1515111-20200314103356543-322299194.png)

  * HTTP和HTTPS的区别

    * HTTPS协议需要到CA申请证书，一般免费证书很少，需要交费。
    * HTTP协议运行在TCP之上，所有传输的内容都是明文，HTTPS运行在SSL/TLS之上，SSL/TLS运行在TCP之上，所有传输的内容都经过加密的。
    * HTTP和HTTPS使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。

  * HTTPS可以有效的防止运营商劫持，解决了防劫持的一个大问题。

  * HTTP1.0 和 HTTP1.1

    * **缓存处理**，在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。
    * **带宽优化及网络连接的使用**，HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。
    * **错误通知的管理**，在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。
    * **Host头处理**，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。
    * **长连接**，HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。
    * HTTP/1.1 Pipeling解决方式为，若干个请求排队串行化单线程处理，后面的请求等待前面请求的返回才能获得执行机会，一旦有某请求超时等，后续请求只能被阻塞，毫无办法，也就是人们常说的线头阻塞；

  * HTTP2.0

    * 多路复用：

      在http1.1中，浏览器客户端在同一时间，针对同一域名下的请求有一定数量的限制，超过限制数目的请求会被阻塞。这也是为何一些站点会有多个静态资源 CDN 域名的原因之一。

      而http2.0中的多路复用优化了这一性能。多路复用允许同时通过单一的http/2 连接发起多重的请求-响应消息。有了新的分帧机制后，http/2 不再依赖多个TCP连接去实现多流并行了。每个数据流都拆分成很多互不依赖的帧，而这些帧可以交错（乱序发送），还可以分优先级，最后再在另一端把它们重新组合起来。

      http 2.0 连接都是持久化的，而且客户端与服务器之间也只需要一个连接（每个域名一个连接）即可。http2连接可以承载数十或数百个流的复用，多路复用意味着来自很多流的数据包能够混合在一起通过同样连接传输。当到达终点时，再根据不同帧首部的流标识符重新连接将不同的数据流进行组装。

      http1.1过程：货轮1从A地到B地去取货物，取到货物后，从B地返回，然后货轮2在A返回并卸下货物后才开始再从A地出发取货返回，如此有序往返。

      http2.0过程：货轮1、2、3、4、5从A地无序全部出发，取货后返回，然后根据货轮号牌卸载对应货物。

      ![preview](https://pic4.zhimg.com/v2-a3b152bfae26b0cfa2aa96ab2c638b4b_r.jpg)

    * 头部压缩

      http1.x的头带有大量信息，而且每次都要重复发送。http/2使用encoder来减少需要传输的header大小，通讯双方各自缓存一份头部字段表，既避免了重复header的传输，又减小了需要传输的大小。

      对于相同的数据，不再通过每次请求和响应发送，通信期间几乎不会改变通用键-值对(用户代理、可接受的媒体类型，等等)只需发送一次。

      事实上,如果请求中不包含首部(例如对同一资源的轮询请求)，那么，首部开销就是零字节，此时所有首部都自动使用之前请求发送的首部。

      如果首部发生了变化，则只需将变化的部分加入到header帧中，改变的部分会加入到头部字段表中，首部表在 http 2.0 的连接存续期内始终存在，由客户端和服务器共同渐进地更新。

      需要注意的是，http 2.0关注的是首部压缩，而我们常用的gzip等是报文内容（body）的压缩，二者不仅不冲突，且能够一起达到更好的压缩效果。

      http/2使用的是专门为首部压缩而设计的HPACK②算法

    HTTP响应分为Header和Body两部分（Body是可选项），我们在`Network`中看到的Header最重要的几行如下：

    ```
    200 OK
    ```

    `200`表示一个成功的响应，后面的`OK`是说明。失败的响应有`404 Not Found`：网页不存在，`500 Internal Server Error`：服务器内部出错，等等。

    ```
    Content-Type: text/html
    ```

    `Content-Type`指示响应的内容，这里是`text/html`表示HTML网页。请注意，浏览器就是依靠`Content-Type`来判断响应的内容是网页还是图片，是视频还是音乐。浏览器并不靠URL来判断响应的内容，所以，即使URL是`http://example.com/abc.jpg`，它也不一定就是图片。

    HTTP响应的Body就是HTML源码。

  

  #### 4.HTTP返回码

  | 状态码 | 状态码英文名称                  | 中文描述                                                     |
  | :----- | :------------------------------ | :----------------------------------------------------------- |
  | 100    | Continue                        | 继续。[客户端](http://www.dreamdu.com/webbuild/client_vs_server/)应继续其请求 |
  | 101    | Switching Protocols             | 切换协议。服务器根据客户端的请求切换协议。只能切换到更高级的协议，例如，切换到HTTP的新版本协议 |
  |        |                                 |                                                              |
  | 200    | OK                              | 请求成功。一般用于GET与POST请求                              |
  | 201    | Created                         | 已创建。成功请求并创建了新的资源                             |
  | 202    | Accepted                        | 已接受。已经接受请求，但未处理完成                           |
  | 203    | Non-Authoritative Information   | 非授权信息。请求成功。但返回的meta信息不在原始的服务器，而是一个副本 |
  | 204    | No Content                      | 无内容。服务器成功处理，但未返回内容。在未更新网页的情况下，可确保浏览器继续显示当前文档 |
  | 205    | Reset Content                   | 重置内容。服务器处理成功，用户终端（例如：浏览器）应重置文档视图。可通过此返回码清除浏览器的表单域 |
  | 206    | Partial Content                 | 部分内容。服务器成功处理了部分GET请求                        |
  |        |                                 |                                                              |
  | 300    | Multiple Choices                | 多种选择。请求的资源可包括多个位置，相应可返回一个资源特征与地址的列表用于用户终端（例如：浏览器）选择 |
  | 301    | Moved Permanently               | 永久移动。请求的资源已被永久的移动到新URI，返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替 |
  | 302    | Found                           | 临时移动。与301类似。但资源只是临时被移动。客户端应继续使用原有URI |
  | 303    | See Other                       | 查看其它地址。与301类似。使用GET和POST请求查看               |
  | 304    | Not Modified                    | 未修改。所请求的资源未修改，服务器返回此状态码时，不会返回任何资源。客户端通常会缓存访问过的资源，通过提供一个头信息指出客户端希望只返回在指定日期之后修改的资源 |
  | 305    | Use Proxy                       | 使用代理。所请求的资源必须通过代理访问                       |
  | 306    | Unused                          | 已经被废弃的HTTP状态码                                       |
  | 307    | Temporary Redirect              | 临时重定向。与302类似。使用GET请求重定向                     |
  |        |                                 |                                                              |
  | 400    | Bad Request                     | 客户端请求的语法错误，服务器无法理解                         |
  | 401    | Unauthorized                    | 请求要求用户的身份认证                                       |
  | 402    | Payment Required                | 保留，将来使用                                               |
  | 403    | Forbidden                       | 服务器理解请求客户端的请求，但是拒绝执行此请求               |
  | 404    | Not Found                       | 服务器无法根据客户端的请求找到资源（网页）。通过此代码，网站设计人员可设置"您所请求的资源无法找到"的个性页面 |
  | 405    | Method Not Allowed              | 客户端请求中的方法被禁止                                     |
  | 406    | Not Acceptable                  | 服务器无法根据客户端请求的内容特性完成请求                   |
  | 407    | Proxy Authentication Required   | 请求要求代理的身份认证，与401类似，但请求者应当使用代理进行授权 |
  | 408    | Request Time-out                | 服务器等待客户端发送的请求时间过长，超时                     |
  | 409    | Conflict                        | 服务器完成客户端的 PUT 请求时可能返回此代码，服务器处理请求时发生了冲突 |
  | 410    | Gone                            | 客户端请求的资源已经不存在。410不同于404，如果资源以前有现在被永久删除了可使用410代码，网站设计人员可通过301代码指定资源的新位置 |
  | 411    | Length Required                 | 服务器无法处理客户端发送的不带Content-Length的请求信息       |
  | 412    | Precondition Failed             | 客户端请求信息的先决条件错误                                 |
  | 413    | Request Entity Too Large        | 由于请求的实体过大，服务器无法处理，因此拒绝请求。为防止客户端的连续请求，服务器可能会关闭连接。如果只是服务器暂时无法处理，则会包含一个Retry-After的响应信息 |
  | 414    | Request-URI Too Large           | 请求的URI过长（URI通常为网址），服务器无法处理               |
  | 415    | Unsupported Media Type          | 服务器无法处理请求附带的媒体格式                             |
  | 416    | Requested range not satisfiable | 客户端请求的范围无效                                         |
  | 417    | Expectation Failed              | 服务器无法满足Expect的请求头信息                             |
  |        |                                 |                                                              |
  | 500    | Internal Server Error           | 服务器内部错误，无法完成请求                                 |
  | 501    | Not Implemented                 | 服务器不支持请求的功能，无法完成请求                         |
  | 502    | Bad Gateway                     | 作为网关或者代理工作的服务器尝试执行请求时，从远程服务器接收到了一个无效的响应 |
  | 503    | Service Unavailable             | 由于超载或系统维护，服务器暂时的无法处理客户端的请求。延时的长度可包含在服务器的Retry-After头信息中 |
  | 504    | Gateway Time-out                | 充当网关或代理的服务器，未及时从远端服务器获取请求           |
  | 505    | HTTP Version not supported      | 服务器不支持请求的HTTP协议的版本，无法完成处理               |

  #### 5.IP地址作用，MAC地址作用

  **一. 整体与局部**

  信息传递时候，需要知道的其实是两个地址：

  - 终点地址（Final destination address）
  - 下一跳的地址（Next hop address）

  IP地址本质上是终点地址，它在跳过路由器（hop）的时候不会改变，而MAC地址则是下一跳的地址，每跳过一次路由器都会改变。

  这就是为什么还要用MAC地址的原因之一，它起到了记录下一跳的信息的作用。

  注：一般来说IP地址经过路由器是不变的，不过NAT（Network address translation）例外，这也是有些人反对NAT而支持IPV6的原因之一。

  **二. 分层实现**
  如果在IP包头（header）中增加了”下一跳IP地址“这个字段，在逻辑上来说，如果IP地址够用，交换机也支持根据IP地址转发（现在的二层交换机不支持这样做），其实MAC地址并不是必要的。

  但用MAC地址和IP地址两个地址，用于分别表示物理地址和逻辑地址是有好处的。这样分层可以使网络层与链路层的协议更灵活地替换，网络层不一定非要用『IP』协议，链路层也不一定非用『以太网』协议。

  这就像OSI七层模型，TCP/IP五层模型其实也不是必要的，用双层模型甚至单层模型实现网络也不是不可以的，只是那样做很蛋疼罢了。

  **三. 早期的『以太网』实现**

  早期的以太网只有集线器（hub），没有交换机（switch），所以发出去的包能被以太网内的所有机器监听到，因此要附带上MAC地址，每个机器只需要接受与自己MAC地址相匹配的包。

  **四.IP协议头部结构**

  IPv4 数据报头部至少为 20 字节，结构如下。

  ![img](https://zinglix.xyz/img/in-post/IP/1.png)

  - 版本：确定 IP 协议的版本（IPv4 或 IPv6），从而能正确解释后面的内容。
  - IHL（头部长度）：由于选项的存在，由此字段确定数据从何处开始。
  - DS（区分服务，DiffServ）：3位优先权字段(现已被忽略) + 4位TOS字段 + 1位保留字段(须为0)。4位TOS字段分别表示最小延时、最大吞吐量、最高可靠性、最小费用，其中最多有一个能置为1。应用程序根据实际需要来设置 TOS值，如ssh和telnet这样的登录程序需要的是最小延时的服务，文件传输ftp需要的是最大吞吐量的服务
  - ECN：拥塞标识符。当路由器感知到拥塞时会设置这两位以降低发送速度。
  - 总长度：IP 数据报的总长度，包括首部和头部，字节为单位。由于位数限制最多 65535 字节，所以数据大小最多为 65515 字节，但是链路层一般不能携带这么大的数据，很少超过 1500 字节。
  - 标识：唯一地标识主机发送的每一个数据报，其初始值是随机的，每发送一个数据报其值就加1。同一个数据报的所有分片都具有相同的标识值
  - 标志: 位1保留，位2表禁止分片(DF)，若设置了此位，IP模块将不对数据报进行分片，在此情况下若IP数据报超过MTU，IP模块将丢弃数据报并返回一个ICMP差错报文；位3标识更多分片(MF)，除了数据报的最后一个分片，其他分片都要把它设置为1
  - 位偏移：分片相对原始IP数据报数据部分的偏移。实际的偏移值为该值左移3位后得到的，所以除了最后一个IP数据报分片外，每个IP分片的数据部分的长度都必须是8的整数倍
  - 生存期（Time-To-Live，TTL）：标识可通过的路由器数量上限。每经过一个路由器将该值减一，为 0 时丢弃，避免在一个网络环路中无限传输。
  - 协议：指示了上层是什么协议，6 为 TCP，17 为 UDP。
  - 头部校验和：用 16 位反码和（Internet 校验和）计算头部，对于数据部分校验由上层协议实现。由于 TTL 每经过一次路由器都会改变，校验和也要因此重新计算。

  #### 6.子网掩码的作用

  在设置电脑IP时，会碰到两个重要的参数，一个是IP地址，一个是子网掩码。IP地址是互联网上每个子网或每个主机在网络上的唯一身份标签，那子网掩码是干什么用的呢？

  随着互联网的发展，越来越多的网络产生，有的网络多则几百台主机，少则区区几台，这样就浪费了大量的IP地址，所以要划分子网。为了有序地区分和管理不同的子网，就需要使用子网掩码。

  子网掩码是一种用来指明一个IP地址所标示的主机处于哪个子网中。子网掩码不能单独存在，它必须结合IP地址一起使用。子网掩码只有一个作用，就是将某个IP地址划分成[网络地址](https://baike.baidu.com/item/网络地址)和[主机地址](https://baike.baidu.com/item/主机地址)两部分。

  #### 7.为什么要三次握手和四次挥手

  先补充一下TCP协议的头部结构

  ![img](https://img-blog.csdn.net/20170324223007286?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveXRodW5kZXI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

  * 4位版本号：IP协议（IPv4）版本号位4
  * 4位头部长度：标识头部有多少个4字节，即最大共15*4个字节
  * 8位服务类型：包含一个4位优先权字段：最小延时，最大吞吐量，最高可靠性和最小费用。
  * 16位总长度：表示整个IP数据报的长度，最大表示65535，但由于MTU限制，一般无法到达这个值。
  * 16位标识：唯一的标识数据报。系统采用加1的式边发送边赋值。
  * 3位标识（保留，DF禁止分片，MF更多分片）：所以这个标志是为分片存在，DF设置时禁止分片所以如果数据报太大则发送失败。MF设置时，如果产生分片，除了最后一个分片，其他此片置1。
  * 13位分片偏移：分片相对原始IP数据报开始处的偏移。
  * 8位生存时间（TTL）：数据报到达目的地之前允许经过的路由跳跳数。跳一下减1，得0丢弃。
  * 8位协议：用来区分上层协议（ICMP为1，TCP为6，UDP为17）。
  * 16位头部校验和：仅以CRC算法检验数据报头部在传输过程中是否损坏。
  * 32位源端口IP地址和目的端口地址很明白。
  * 选项（可变长）：记录路由，告诉途径得所有路由把IP填进来。 时间戳，告诉每个路由器都将数据报被转发的时间传进来。松散路由选择，指定一个路由器IP地址列表，必须按这个表发送，严格路由选择，数据报经过路由表。

  

  

  `TCP`的三次握手建立连接和四次挥手断开连接，相信很多人都听说过，也都看过相关的内容，本篇是为了记录自己对与这两种操作的理解。

  在进入正式内容之前，先来看几个符号的概念：

  - **序列号`seq`：** 用来标记数据段的顺序，`TCP`把连接中发送的所有数据字节都编上一个序号，第一个字节的编号由本地随机产生；给字节编上序号后，就给每一个报文段指派一个序号；序列号`seq`就是这个报文段中的第一个字节的数据编号。
  - **确认号`ack`：** 期待收到对方下一个报文段的第一个数据字节的序号；序列号表示报文段携带数据的第一个字节的编号；而确认号指的是期望接收到下一个字节的编号；因此当前报文段最后一个字节的编号+1即为确认号。
  - **确认`ACK`：** 仅当`ACK=1`时，确认号字段才有效。`ACK=0`时，确认号无效
  - **同步`SYN`：** 连接建立时用于同步序号。当`SYN=1`，`ACK=0`时表示：这是一个连接请求报文段。若同意连接，则在响应报文段中使得`SYN=1`，`ACK=1`。因此，`SYN=1`表示这是一个连接请求，或连接接受报文。`SYN`这个标志位只有在`TCP`建产连接时才会被置1，握手完成后`SYN`标志位被置0。
  - **终止`FIN`：** 用来释放一个连接。`FIN=1`表示：此报文段的发送方的数据已经发送完毕，并要求释放运输连接

  ![img](https://user-gold-cdn.xitu.io/2019/8/15/16c92ce524608eac?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

  **第一次握手🤝：** 客户端向服务器发出连接请求报文，这时报文首部中的同部位`SYN=1`，同时随机生成初始序列号 `seq=x`，此时，客户端进程进入了 `SYN-SENT`状态，等待服务器的确认。

  **第二次握手🤝：** 服务器收到请求报文后，如果同意连接，则发出确认报文。确认报文中应该 `ACK=1`，`SYN=1`，确认号是`ack=x+1`，同时也要为自己随机初始化一个序列号 `seq=y`，此时，服务器进程进入了`SYN-RCVD`状态，询问客户端是否做好准备。

  **第三次握手🤝：**  客户端进程收到确认后，还要向服务器给出确认。确认报文的`ACK=1`，`ack=y+1`，此时，连接建立，客户端进入`ESTABLISHED`状态，服务器端也进入`ESTABLISHED`状态。

  **握手🤝为什么需要三次呢，如果把最后一次的去掉改为两次握手🤝是否可行呢?**

  假如现在客户端想向服务端进行握手，它发送了第一个连接的请求报文，但是由于网络信号差或者服务器负载过多，这个请求没有立即到达服务端，而是在某个网络节点中长时间的滞留了，以至于滞留到客户端连接释放以后的某个时间点才到达服务端，那么这就是一个失效的报文，但是服务端接收到这个失效的请求报文后，就误认为客户端又发了一次连接请求，服务端就会想向客户端发出确认的报文，表示同意建立连接。

  假如不采用三次握手，那么只要服务端发出确认，表示新的建立就连接了。但是现在客户端并没有发出建立连接的请求，其实这个请求是失效的请求，一切都是服务端在自相情愿，因此客户端是不会理睬服务端的确认信息，也不会向服务端发送确认的请求，但是服务器却认为新的连接已经建立起来了，并一直等待客户端发来数据，这样的情况下，服务端的很多资源就没白白浪费掉了。

  采用三次握手的办法就是为了防止上述这种情况的发生，比如就在刚才的情况下，客户端不会向服务端发出确认的请求，服务端会因为收不到确认的报文，就知道客户端并没有要建立连接，那么服务端也就不会去建立连接，这就是三次握手的作用。

  * 四次挥手

  **第一次挥手👋：**  客户端进程发出连接释放FIN报文，并且停止发送数据。释放数据报文首部，`FIN=1`，其序列号为`seq=x`,此时，客户端进入`FIN-WAIT-1`（终止等待1）状态。

  **第二次挥手👋：**  服务端进程收到连接释放`FIN`报文，发出确认`ACK`报文，`ACK=1`，`ack=x+1`，并且带上自己的序列号`seq=y`，此时，服务端就进入了`CLOSE-WAIT`（关闭等待）状态。此时，服务端通知高层的应用进程，客户端向服务端的方向就释放了，这时候处于半关闭状态，即客户端已经没有数据要发送了，但是服务端若发送数据，客户端依然要接受。这个状态还要持续一段时间，也就是整个`CLOSE-WAIT`状态持续的时间。客户端收到服务端的确认请求后，此时，客户端就进入`FIN-WAIT-2`（终止等待2）状态，等待服务器发送连接释放报文，在这之前依然可以接收服务端发送过来的最后的数据。

  **第三次挥手👋：** 服务端将最后的数据发送给客户端完成后，就向客户端发送连接释放`FIN`报文，`FIN=1`，`ack=x+1`，此时的序列号为`seq=z`，此时，服务端就进入了`LAST-ACK`（最后确认）状态，等待客户端的确认。

  **第四次挥手👋：** 客户端接收到服务端的连接释放`FIN`报文后，必须发出确认报文，`ACK=1`，`ack=z+1`，而自己的序列号是`seq=x+1`，此时，客户端就进入了`TIME-WAIT`（时间等待）状态。此时服务端收到客户端发送过来的确认报文，就立即撤销自己的传输控制块`TCB`,进入`CLOSED`状态，注意此时的`TCP`连接还没有释放，必须经过`2MSL`（最长报文段寿命）的时间后，客户端没有收到服务端发来的任何数据，证明服务端已正常关闭，此时客户端会撤销相应传输控制块`TCB`后，进入`CLOSED`状态。至此，`TCP`的连接才真正的断开了。（服务端结束`TCP`连接的时间要比客户端稍微早一些）

  **为什么断开连接需要四次挥手👋呢，像建立连接的时候一样，三次行不行呢？**

  `TCP`协议是一种面向连接的、可靠的、基于字节流的运输层通信协议。`TCP`是全双工 模式，这就意味着，在客户端想要断开连接时，客户端向服务端发送`FIN`报文，只是表示客户端已经没有数据要发送了，但是这个时候客户端还是可以接收来自服务端的数据。

  当服务端接收到`FIN`报文，并返回`ACK`报文，表示服务端已经知道了客户端要断开连接，客户端已经没有数据要发送了，但是这个时候服务端可能依然有数据要传输给客户端。

  当服务端的数据传输完之后，服务端会发送`FIN`报文给客户端，表示服务端也没有数据要传输了，服务端同意关闭连接，之后，客户端收到`FIN`报文，立即发送给客户端一个`ACK`报文，确定关闭连接。在之后，客户端和服务端彼此就愉快的断开了这次的`TCP`连接。

  或许会有疑问，为什么服务端的`ACK`报文和`FIN`报文都是分开发送的，但是在三次握手的时候却是`ACK`报文和`SYN`报文是一起发送的，因为在三次握手的过程中，当服务端收到客户端的`SYN`连接请求报文后，可以直接发送`SYN+ACK`报文。其中`ACK`报文是用来应答的，`SYN`报文是用来同步的。但是在关闭连接时，当服务端接收到`FIN`报文时，很可能并不会立即关闭`SOCKET`，所以只能先回复一个`ACK`报文，告诉客户端，你发的`FIN`报文我收到了，只有等到服务端所有的数据都发送完了，才能发送`FIN`报文，因此`ACK`报文和`FIN`报文不能一起发送。所以断开连接的时候才需要四次挥手来完成。

  **如何快速结束四次挥手**

  reset复位

  #### 8.输入一个URL发生的事情

  https://segmentfault.com/a/1190000006879700

  1. DNS解析（UDP请求 + TCP协议）

     * DNS的规范规定了2种类型的DNS服务器，一个叫主DNS服务器，一个叫辅助DNS服务器。在一个区中主DNS服务器从自己本机的数据文件中读取该区的DNS数据信息，而辅助DNS服务器则从区的主DNS服务器中读取该区的DNS数据信息。当一个辅助DNS服务器启动时，它需要与主DNS服务器通信，并加载数据信息，这就叫做区传送（zone transfer）。 这种情况下，使用TCP协议。

     * 针对第一问，*为什么域名解析用UDP协议?*

       因为UDP快啊！UDP的DNS协议只要一个请求、一个应答就好了。而使用基于TCP的DNS协议要三次握手、发送数据以及应答、四次挥手。但是UDP协议传输内容不能超过512字节。不过客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。

       针对第二问，*为什么区域传送用TCP协议？*

       因为TCP协议可靠性好啊！你要从主DNS上复制内容啊，你用不可靠的UDP？ 因为TCP协议传输的内容大啊，你用最大只能传512字节的UDP协议？万一同步的数据大于512字节，你怎么办？

     * 我们要先把域名的结构讲清楚！ `www.tmall.com`对应的真正的域名为`www.tmall.com.`。末尾的`.`称为根域名，因为每个域名都有根域名，因此我们通常省略。

       根域名的下一级，叫做"顶级域名"（top-level domain，缩写为TLD），比如`.com、.net`；

       再下一级叫做"次级域名"（second-level domain，缩写为SLD），比如`www.tmall.com`里面的`.tmall`，这一级域名是用户可以注册的；

       再下一级是主机名（host），比如`www.tmall.com`里面的`www`，又称为"三级域名"，这是用户在自己的域里面为服务器分配的名称，是用户可以任意分配的。

       那么解析流程就是**分级查询**！

       (1)先在本机的DNS里头查，如果有就直接返回了。本机DNS就是下面这个东东

       (2)本机DNS里头发现没有，就去根服务器里查。根服务器发现这个域名是属于`com`域，因此根域DNS服务器会返回它所管理的`com`域中的DNS 服务器的IP地址，意思是“虽然我不知道你要查的那个域名的地址，但你可以去`com`域问问看”

       (3)本机的DNS接到又会向`com`域的DNS服务器发送查询消息。`com` 域中也没有`www.tmall.com`这个域名的信息，和刚才一样，`com`域服务器会返回它下面的`tmall.com`域的DNS服务器的IP地址。

  2. TCP连接

  3. 发送HTTP请求

  4. 服务器处理请求并返回HTTP报文关闭TCP连接

  5. 浏览器解析渲染页面

  6. 连接结束

  #### 9.怎么用UDP实现可靠传输

  最简单的方式是在应用层模仿传输层TCP的可靠性传输。下面不考虑拥塞处理，可靠UDP的简单设计。

  - 1、添加seq/ack机制，确保数据发送到对端
  - 2、添加发送和接收缓冲区，主要是用户超时重传。
  - 3、添加超时重传机制。

  详细说明：送端发送数据时，生成一个随机seq=x，然后每一片按照数据大小分配seq。数据到达接收端后接收端放入缓存，并发送一个ack=x的包，表示对方已经收到了数据。发送端收到了ack包后，删除缓冲区对应的数据。时间到后，定时任务检查是否需要重传数据。

  目前有如下开源程序利用udp实现了可靠的数据传输。分别为***RUDP、RTP、UDT\***。

  ##### 1、RUDP（Reliable User Datagram Protocol）

  ***RUDP 提供一组数据服务质量增强机制，如拥塞控制的改进、重发机制及淡化服务器算法等\***，从而在包丢失和网络拥塞的情况下， RTP 客户机（实时位置）面前呈现的就是一个高质量的 RTP 流。在不干扰协议的实时特性的同时，可靠 UDP 的拥塞控制机制允许 TCP 方式下的流控制行为。

  ##### 2、RTP（Real Time Protocol）

  ***RTP为数据提供了具有实时特征的端对端传送服务\***，如在组播或单播网络服务下的交互式视频音频或模拟数据。

  应用程序通常在 UDP 上运行 RTP 以便使用其多路结点和校验服务；这两种协议都提供了传输层协议的功能。但是 RTP 可以与其它适合的底层网络或传输协议一起使用。如果底层网络提供组播方式，那么 RTP 可以使用该组播表传输数据到多个目的地。

  RTP 本身并没有提供按时发送机制或其它服务质量（QoS）保证，它依赖于底层服务去实现这一过程。 RTP 并不保证传送或防止无序传送，也不确定底层网络的可靠性。 RTP 实行有序传送， RTP 中的序列号允许接收方重组发送方的包序列，同时序列号也能用于决定适当的包位置，例如：在视频解码中，就不需要顺序解码。

  #### 10.Socket网络编程

  ##### 长连接与短连接

  所谓长连接，指在一个TCP连接上可以连续发送多个数据包，在TCP连接保持期间，如果没有数据包发送，需要双方发检测包以维持此连接，一般需要自己做在线维持。 
  短连接是指通信双方有数据交互时，就建立一个TCP连接，数据发送完成后，则断开此TCP连接，一般银行都使用短连接。 
  比如http的，只是连接、请求、关闭，过程时间较短,服务器若是一段时间内没有收到请求即可关闭连接。 
  其实长连接是相对于通常的短连接而说的，也就是长时间保持客户端与服务端的连接状态。

  https://blog.csdn.net/u012707739/article/details/75918894

  ![TCP socket 函数流程](https://img-blog.csdn.net/20170723162227540?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjcwNzczOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

  #### 11.GET和POST的区别

  GET在浏览器回退时是无害的，而POST会再次提交请求。
  GET产生的URL地址可以被Bookmark，而POST不可以。
  GET请求会被浏览器主动cache，而POST不会，除非手动设置。
  GET请求只能进行url编码，而POST支持多种编码方式。
  GET请求参数会被完整保留在浏览器历史记录里，而POST中的参数不会被保留。
  GET请求在URL中传送的参数是有长度限制的，而POST没有。
  对参数的数据类型，GET只接受ASCII字符，而POST没有限制。
  GET比POST更不安全，因为参数直接暴露在URL上，所以不能用来传递敏感信息。
  GET参数通过URL传递，POST放在Request body中。

  对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200（返回数据）；

  而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok（返回数据）。

  也就是说，GET只需要汽车跑一趟就把货送到了，而POST得跑两趟，第一趟，先去和服务器打个招呼“嗨，我等下要送一批货来，你们打开门迎接我”，然后再回头把货送过去。

  因为POST需要两步，时间上消耗的要多一点，看起来GET比POST更有效。因此Yahoo团队有推荐用GET替换POST来优化网站性能。但这是一个坑！跳入需谨慎。为什么？

  1. GET与POST都有自己的语义，不能随便混用。

  2. 据研究，在网络环境好的情况下，发一次包的时间和发两次包的时间差别基本可以无视。而在网络环境差的情况下，两次包的TCP在验证数据包完整性上，有非常大的优点。

  3. 并不是所有浏览器都会在POST中发送两次包，Firefox就只发送一次。

  #### 12.断点续传的原理

  ### 前端

  前端大文件上传网上的大部分文章已经给出了解决方案，核心是`利用 Blob.prototype.slice` 方法，和数组的 slice 方法相似，调用的 slice 方法可以返回`原文件的某个切片`

  这样我们就可以根据预先设置好的切片最大数量将文件切分为一个个切片，然后借助 http 的可并发性，同时上传多个切片，这样从原本传一个大文件，变成了`同时`传多个小的文件切片，可以大大减少上传时间

  另外由于是并发，传输到服务端的顺序可能会发生变化，所以我们还需要给每个切片记录顺序

  ### 服务端

  服务端需要负责接受这些切片，并在接收到所有切片后`合并`切片

  这里又引伸出两个问题

  1. 何时合并切片，即切片什么时候传输完成
  2. 如何合并切片

  第一个问题需要前端进行配合，前端在每个切片中都携带切片最大数量的信息，当服务端接受到这个数量的切片时自动合并，也可以额外发一个请求主动通知服务端进行切片的合并

  第二个问题，具体如何合并切片呢？这里可以使用 nodejs 的 读写流（readStream/writeStream），将所有切片的流传输到最终文件的流里

  `talk is cheap,show me the code`，接着我们用代码实现上面的思路

  Linux具体的代码实现步骤为：

  1.客户端会去寻找文件已经下载的指针位置

  2.将文件下载的位置传输到header里面，然后通知服务端从该位置传输

  3.服务端收到请求，从文件512K开始传输

  具体如下

  1，使用gethostbyname()获取站点“www.boa.org”的IP地址
  2，以“boa-0.94.13.tar.gz”为例，构建HTTP请求报文首部：

     char *httpreq = "GET /boa-0.94.13.tar.gz HTTP/1.1\r\n"
                     "Range: bytes=%d-%d\r\n"
                     "Host: %s\r\n\r\n"
                      begin, end, host);

  注意：
      A) 以上字符串httpreq就是发送给远程主机www.boa.org的HTTP请求报文，通过TCP发送
      B) Range字段是要求主机发送申请文件的部分内容，begin和end分别是文件的开始和结束
         B.1 如果写成 "Range: bytes=0-\r\n"，代表要求主机发送全文
         B.2 如果写成 "Range: bytes=-1000\r\n"，代表要求主机发送前1000个字节
         B.3 如果写成 "Range: bytes=0-200\r\n"，代表要求主机发送前200个字节
      C) 正常情况下，HTTP服务器会对这个请求报文返回206，并给出实际返回的字节数和范围：
         C.1 "Content-Length: 1000" 这个字段代表本次HTTP发来的数据大小为1000个字节（不含HTTP首部）
         C.2 "Content-Range: bytes 2000-2999/9999" 代表本次传送的数据范围是第2000-2999个字节（共1000个字节），而所请求的文件总大小是9999个字节。

  3，将收到的报文的HTTP首部去掉（HTTP首部是指从开头到\r\n\r\n结尾的部分），剩下的就是下载的文件内容
  4，将文件内容以非缓冲方式保存下来。

  5，如果发生下载时网络断线，或者人为终止了下载进程，那么在下一次下载时先获取当前已下载部分的大小，并作为Range参数告知远端HTTP服务器，要求发送部分文件，实现断点续传，节约网络流量节约时间。

  服务端： 
   1、接收到客户端请求文件下载数据后，将文件大小返回给客户端。
   2、客户端根据文件的偏移，请求下载文件，服务端根据偏移的文件数据返回给客户端。 如：从文件的1000地址位置开始，请求1024字节的文件数据。

  客户端：
  1、请求文件下载，获取到文件大小。【如果是多线程同时下载一个文件，这里要先创建该文件，并且大小与需要下载的文件一致，然后每个线程负责一块往里面填数据】
  2、检查是否是断点续传【检查临时文件扩展名，如后面多加.tmp】，如果不是从0开始请求文件数据，请求到后保存到文件，文件名为临时文件扩展名。
  3、如果是断点续传，获取已保存的临时文件大小，根据大小为起始偏移请求继续下载文件。

  #### 13.unicode编码

  ## ASCII

  最早的字符集叫 American Standard Code for Information Interchange（美国信息交换标准代码），简称 ASCII，由 American National Standard Institute（美国国家标准协会）制定。在ASCII 字符集中，字母 `A` 对应的字符编码是 `65`，转换成二进制是 `0100 0001`，由于二进制表示比较长，通常使用十六进制 `41`。

  ## GB2312、GBK

  ASCII 字符集总共规定了 128 种字符规范，但是并没有涵盖西文字母之外的字符，当需要计算机显示存储中文的时候，就需要一种对中文进行编码的字符集，GB 2312 就是解决中文编码的字符集，由国家标准委员会发布。同时考虑到中文语境中往往也需要使用西文字母，GB 2312 也实现了对 ASCII 的向下兼容，原理是西文字母使用和 ASCII 中相同的代码，但是 GB 2312 只涵盖了 6000 多个汉字，还有很多没有包含在其中，所以又出现了 GBK 和 GB 18030，两种字符集都是在 GB 2312 的基础上进行了扩展。

  ## Unicode

  可以看到，光是简体中文，就先后出现了至少三种字符集，繁体中文方面也有 BIG5 等字符集，几乎每种语言都需要有一个自己的字符集，每个字符集使用了自己的编码规则，往往互不兼容。同一个字符在不同字符集下的字符代码不同，这使得跨语言交流的过程中双方必须要使用相同的字符编码才能不出现乱码的情况。为了解决传统字符编码的局限性，Unicode 诞生了，Unicoide 的全称是 Universal Multiple-Octet Coded Character Set（通用多八位字符集，简称 UCS）。Unicode 在一个字符集中包含了世界上所有文字和符号，统一编码，来终结不同编码产生乱码的问题。

  Unicode也为了每个字符发了一张身份证，这张“身份证”上有一串唯一的数字ID确定了这个字符。

  这串数字在整个计算机的世界具有唯一性，Unicode给这串数字ID起了个名字叫［码点］。

  https://juejin.cn/post/6844904034944434183

  ## 字符编码 UTF-8

  Unicode 统一了所有字符的编码，是一个 Character Set，也就是字符集，字符集只是给所有的字符一个唯一编号，但是却没有规定如何存储，一个编号为 `65` 的字符，只需要一个字节就可以存下，但是编号 `40657` 的字符需要两个字节的空间才可以装下，而更靠后的字符可能会需要三个甚至四个字节的空间。

  这时，用什么规则存储 Unicode 字符就成了关键，我们可以规定，一个字符使用四个字节存储，也就是 32 位，这样就能涵盖现有 Unicode 包含的所有字符，这种编码方式叫做 UTF-32（UTF 是 UCS Transformation Format 的缩写）。UTF-32 的规则虽然简单，但是缺陷也很明显，假设使用 UTF-32 和 ASCII 分别对一个只有西文字母的文档编码，前者需要花费的空间是后者的四倍（ASCII 每个字符只需要一个字节存储）。

  在存储和网络传输中，通常使用更为节省空间的变长编码方式 UTF-8，UTF-8 代表 8 位一组表示 Unicode 字符的格式，使用 1 - 4 个字节来表示字符。

  UTF-8 的编码规则如下（U+ 后面的数字代表 Unicode 字符代码）：

  ```text
  U+ 0000 ~ U+ 007F: 0XXXXXXX
  U+ 0080 ~ U+ 07FF: 110XXXXX 10XXXXXX
  U+ 0800 ~ U+ FFFF: 1110XXXX 10XXXXXX 10XXXXXX
  U+10000 ~ U+1FFFF: 11110XXX 10XXXXXX 10XXXXXX 10XXXXXX
  ```

  可以看到，UTF-8 通过开头的标志位位数实现了变长。对于单字节字符，只占用一个字节，实现了向下兼容 ASCII，并且能和 UTF-32 一样，包含 Unicode 中的所有字符，又能有效减少存储传输过程中占用的空间。

  #### 14.session，cookie，token区别

  * session
    session的中文翻译是“会话”，当用户打开某个web应用时，便与web服务器产生一次session。服务器使用session把用户的信息临时保存在了服务器上，用户离开网站后session会被销毁。这种用户信息存储方式相对cookie来说更安全，可是session有一个缺陷：如果web服务器做了负载均衡，那么下一个操作请求到了另一台服务器的时候session会丢失。

  * cookie
    cookie是保存在本地终端的数据。cookie由服务器生成，发送给浏览器，浏览器把cookie以kv形式保存到某个目录下的文本文件内，下一次请求同一网站时会把该cookie发送给服务器。由于cookie是存在客户端上的，所以浏览器加入了一些限制确保cookie不会被恶意使用，同时不会占据太多磁盘空间，所以每个域的cookie数量是有限的。

  * token
    token的意思是“令牌”，是用户身份的验证方式，最简单的token组成:uid(用户唯一的身份标识)、time(当前时间的时间戳)、sign(签名，由token的前几位+盐以哈希算法压缩成一定长的十六进制字符串，可以防止恶意第三方拼接token请求服务器)。还可以把不变的参数也放进token，避免多次查库

  * cookie和session的关系：

    1.用户第一次请求服务器的时候，服务器根据用户提交的相关信息，创建创建对应的 Session ，请求返回时将此 Session 的唯一标识信息 SessionID 返回给浏览器，浏览器接收到服务器返回的 SessionID 信息后，会将此信息存入到 Cookie 中，同时 Cookie 记录此 SessionID 属于哪个域名。

    2.当用户第二次访问服务器的时候，请求会自动判断此域名下是否存在 Cookie 信息，如果存在自动将 Cookie 信息也发送给服务端，服务端会从 Cookie 中获取 SessionID，再根据 SessionID 查找对应的 Session 信息，如果没有找到说明用户没有登录或者登录失效，如果找到 Session 证明用户已经登录可执行后面操作。

    3.根据以上流程可知，SessionID 是连接 Cookie 和 Session 的一道桥梁，大部分系统也是根据此原理来验证用户登录状态。

  * 如果浏览器禁止cookie怎么办

    既然服务端是根据 Cookie 中的信息判断用户是否登录，那么如果浏览器中禁止了 Cookie，如何保障整个机制的正常运转。

    第一种方案，每次请求中都携带一个 SessionID 的参数，也可以 Post 的方式提交，也可以在请求的地址后面拼接 `xxx?SessionID=123456...`。

    第二种方案，Token 机制。Token 机制多用于 App 客户端和服务器交互的模式，也可以用于 Web 端做用户状态管理。

    Token 的意思是“令牌”，是服务端生成的一串字符串，作为客户端进行请求的一个标识。Token 机制和 Cookie 和 Session 的使用机制比较类似。

    当用户第一次登录后，服务器根据提交的用户信息生成一个 Token，响应时将 Token 返回给客户端，以后客户端只需带上这个 Token 前来请求数据即可，无需再次登录验证。

  * 分布式session

    在互联网公司为了可以支撑更大的流量，后端往往需要多台服务器共同来支撑前端用户请求，那如果用户在 A 服务器登录了，第二次请求跑到服务 B 就会出现登录失效问题。

    分布式 Session 一般会有以下几种解决方案：

    - Nginx ip_hash 策略，服务端使用 Nginx 代理，每个请求按访问 IP 的 hash 分配，这样来自同一 IP 固定访问一个后台服务器，避免了在服务器 A 创建 Session，第二次分发到服务器 B 的现象。
    - Session 复制，任何一个服务器上的 Session 发生改变（增删改），该节点会把这个 Session 的所有内容序列化，然后广播给所有其它节点。
    - 共享 Session，服务端无状态话，将用户的 Session 等信息使用缓存中间件来统一管理，保障分发到每一个服务器的响应结果都一致。

    建议采用第三种方案。

  * cookie 和session的区别

    - 作用范围不同，Cookie 保存在客户端（浏览器），Session 保存在服务器端。
    - 存取方式的不同，Cookie 只能保存 ASCII，Session 可以存任意数据类型，一般情况下我们可以在 Session 中保持一些常用变量信息，比如说 UserId 等。
    - 有效期不同，Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭或者 Session 超时都会失效。
    - 隐私策略不同，Cookie 存储在客户端，比较容易遭到不法获取，早期有人将用户的登录名和密码存储在 Cookie 中导致信息被窃取；Session 存储在服务端，安全性相对 Cookie 要好一些。
    - 存储大小不同， 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie。

  ## 15.TCP粘包问题

  https://draveness.me/whys-the-design-tcp-message-frame/

  ### 粘包的主要原因：

  - 发送方每次写入数据 < 套接字（Socket）缓冲区大小；
  - 接收方读取套接字（Socket）缓冲区数据不够及时。

  ### 半包的主要原因：

  - 发送方每次写入数据 > 套接字（Socket）缓冲区大小；
  - 发送的数据大于协议的 MTU (Maximum Transmission Unit，最大传输单元)，因此必须拆包。

  ### 小知识点：什么是缓冲区？

  缓冲区又称为缓存，它是内存空间的一部分。也就是说，在内存空间中预留了一定的存储空间，这些存储空间用来缓冲输入或输出的数据，这部分预留的空间就叫做缓冲区。

  缓冲区的优势以文件流的写入为例，如果我们不使用缓冲区，那么每次写操作 CPU 都会和低速存储设备也就是磁盘进行交互，那么整个写入文件的速度就会受制于低速的存储设备（磁盘）。但如果使用缓冲区的话，每次写操作会先将数据保存在高速缓冲区内存上，当缓冲区的数据到达某个阈值之后，再将文件一次性写入到磁盘上。因为内存的写入速度远远大于磁盘的写入速度，所以当有了缓冲区之后，文件的写入速度就被大大提升了。

  ### 解决方案1：固定缓冲区大小

  固定缓冲区大小的实现方案，只需要控制服务器端和客户端发送和接收字节的（数组）长度相同即可。

  服务器端实现代码如下：

  ```java
  /**
   * 服务器端，改进版本一（只负责接收消息）
   */
  static class ServSocketV1 {
      private static final int BYTE_LENGTH = 1024;  // 字节数组长度（收消息用）
      public static void main(String[] args) throws IOException {
          ServerSocket serverSocket = new ServerSocket(9091);
          // 获取到连接
          Socket clientSocket = serverSocket.accept();
          try (InputStream inputStream = clientSocket.getInputStream()) {
              while (true) {
                  byte[] bytes = new byte[BYTE_LENGTH];
                  // 读取客户端发送的信息
                  int count = inputStream.read(bytes, 0, BYTE_LENGTH);
                  if (count > 0) {
                      // 接收到消息打印
                      System.out.println("接收到客户端的信息是:" + new String(bytes).trim());
                  }
                  count = 0;
              }
          }
      }
  }
  ```

  客户端实现代码如下：

  ```java
  /**
   * 客户端，改进版一（只负责接收消息）
   */
  static class ClientSocketV1 {
      private static final int BYTE_LENGTH = 1024;  // 字节长度
      public static void main(String[] args) throws IOException {
          Socket socket = new Socket("127.0.0.1", 9091);
          final String message = "Hi,Java."; // 发送消息
          try (OutputStream outputStream = socket.getOutputStream()) {
              // 将数据组装成定长字节数组
              byte[] bytes = new byte[BYTE_LENGTH];
              int idx = 0;
              for (byte b : message.getBytes()) {
                  bytes[idx] = b;
                  idx++;
              }
              // 给服务器端发送 10 次消息
              for (int i = 0; i < 10; i++) {
                  outputStream.write(bytes, 0, BYTE_LENGTH);
              }
          }
      }
  }
  ```

  ### 解决方案二：封装请求协议

  这种解决方案的实现思路是将请求的数据封装为两部分：数据头+数据正文，在数据头中存储数据正文的大小，当读取的数据小于数据头中的大小时，继续读取数据，直到读取的数据长度等于数据头中的长度时才停止。

  因为这种方式可以拿到数据的边界，所以也不会导致粘包和半包的问题，但这种实现方式的编码成本较大也不够优雅，因此不是最佳的实现方案，因此我们这里就略过，直接来看最终的解决方案吧。

  ### 解决方案三：特殊字符结尾，按行读取

  以特殊字符结尾就可以知道流的边界了，因此也可以用来解决粘包和半包的问题，**此实现方案是我们推荐最终解决方案**。

  这种解决方案的核心是，使用 Java 中自带的 `BufferedReader` 和 `BufferedWriter`，也就是带缓冲区的输入字符流和输出字符流，通过写入的时候加上 `\n` 来结尾，读取的时候使用 `readLine` 按行来读取数据，这样就知道流的边界了，从而解决了粘包和半包的问题。

  服务器端实现代码如下：

  ```java
  /**
   * 服务器端，改进版三(只负责收消息)
   */
  static class ServSocketV3 {
      public static void main(String[] args) throws IOException {
          // 创建 Socket 服务器端
          ServerSocket serverSocket = new ServerSocket(9092);
          // 获取客户端连接
          Socket clientSocket = serverSocket.accept();
          // 使用线程池处理更多的客户端
          ThreadPoolExecutor threadPool = new ThreadPoolExecutor(100, 150, 100,
                  TimeUnit.SECONDS, new LinkedBlockingQueue<>(1000));
          threadPool.submit(() -> {
              // 消息处理
              processMessage(clientSocket);
          });
      }
      /**
       * 消息处理
       * @param clientSocket
       */
      private static void processMessage(Socket clientSocket) {
          // 获取客户端发送的消息流对象
          try (BufferedReader bufferedReader = new BufferedReader(
                  new InputStreamReader(clientSocket.getInputStream()))) {
              while (true) {
                  // 按行读取客户端发送的消息
                  String msg = bufferedReader.readLine();
                  if (msg != null) {
                      // 成功接收到客户端的消息并打印
                      System.out.println("接收到客户端的信息:" + msg);
                  }
              }
          } catch (IOException ioException) {
              ioException.printStackTrace();
          }
      }
  }
  ```

  客户端的实现代码如下：

  ```java
  /**
   * 客户端，改进版三(只负责发送消息)
   */
  static class ClientSocketV3 {
      public static void main(String[] args) throws IOException {
          // 启动 Socket 并尝试连接服务器
          Socket socket = new Socket("127.0.0.1", 9092);
          final String message = "Hi,Java."; // 发送消息
          try (BufferedWriter bufferedWriter = new BufferedWriter(
                  new OutputStreamWriter(socket.getOutputStream()))) {
              // 给服务器端发送 10 次消息
              for (int i = 0; i < 10; i++) {
                  // 注意:结尾的 \n 不能省略,它表示按行写入
                  bufferedWriter.write(message + "\n");
                  // 刷新缓冲区(此步骤不能省略)
                  bufferedWriter.flush();
              }
          }
      }
  }
  ```

  ## 16.ARP协议

  * 第一步:首先，每个主机都会有自己的ARP缓存区中建立一个ARP列表，以表示IP地址和MAC地址之间的对应关系
  * 第二步:当源主机要发送数据时，
    首先检测ARP列表中是否对应IP地址的目的主机的MAC地址
    如果有，则直接发送数据
    如果没有，就向本网段的所有主机发送ARP数据包，内容:
    我是IP地址，mac地址，谁是IP地址，mac?
  * 第三步:当本网络的所有主机收到该ARP数据包时，
    首先检查数据包中的IP地址是否是自己的IP地址，
    如果不是，则忽略该数据包
    如果是，则首先从数据包中取出源主机的IP和mac地址写入到ARP列表中，如果以存在，则覆盖
    然后将自己的mac地址写入arp响应包中，告诉源主机自己是它想要找的mac地址
  * 第四步:源主机收到ARP响应包后，将目的主机的IP和mac地址写入arp列表，并利用此信息发送数据
    如果源主机一直没有收到arp响应数据包，表示arp查询失败。

  #### 17.TCP的TIME_WAIT问题：

  1. 当客户端没有待发送的数据时，它会向服务端发送 `FIN` 消息，发送消息后会进入 `FIN_WAIT_1` 状态；

  2. 服务端接收到客户端的 `FIN` 消息后，会进入 `CLOSE_WAIT` 状态并向客户端发送 `ACK` 消息，客户端接收到 `ACK` 消息时会进入 `FIN_WAIT_2` 状态；

  3. 当服务端没有待发送的数据时，服务端会向客户端发送 `FIN` 消息；

  4. 客户端接收到 `FIN` 消息后，会进入 `TIME_WAIT` 状态并向服务端发送 `ACK` 消息，服务端收到后会进入 `CLOSED` 状态；

  5. 客户端等待**两个最大数据段生命周期**（Maximum segment lifetime，MSL）[2](https://draveness.me/whys-the-design-tcp-time-wait/#fn:2)的时间后也会进入 `CLOSED` 状态；

     ![tcp-closing-connection](https://img.draveness.me/2020-03-10-15838517142219-tcp-closing-connection.png)

  从上述过程中，我们会发现 `TIME_WAIT` 仅在主动断开连接的一方出现，被动断开连接的一方会直接进入 `CLOSED` 状态，进入 `TIME_WAIT` 的客户端需要等待 2 MSL 才可以真正关闭连接。TCP 协议需要 `TIME_WAIT` 状态的原因和客户端需要等待两个 MSL 不能直接进入 `CLOSED` 状态的原因是一样的：

  - 防止延迟的数据段被其他使用相同源地址、源端口、目的地址以及目的端口的 TCP 连接收到；
  - 保证 TCP 连接的远程被正确关闭，即等待被动关闭连接的一方收到 `FIN` 对应的 `ACK` 消息；

  #### 18.MTU是什么：

  最大传送单元（Maximum Transfer Unit，MTU）。意思是IP数据报的最大长度（不包括帧头帧尾）。MTU的单位是字节。

  #### 19.TIMEWAIT

  * MSL是什么意思？

  MSL,  the maximum segment lifetime （最长报文段寿命） 。 segment可以在网络中存活的最长时间，超过这个时间会被丢弃。这个时间限制是有界限的，TCP Segment通过IP datagrams进行传输，IP datagrams使用ttl属性来定义这个时间。RFC793中规定MSL为2分钟，但这完全是从工程上来考虑，对于现在的网络，MSL=2分钟可能太长了一些。因此TCP允许不同的实现可根据具体情况使用更小的MSL值。TTL与MSL是有关系的但不是简单的相等关系，MSL要大于TTL。 

  * 为什么需要等待2MSL？

  在TIME_WAIT （也被叫做２MSL等待状态）状态下，客户端向服务器发出ACK以后需要等待2个MSL的时间。假如ACK在传输中丢失，超时后服务端会重新发送FIN, 客户端收到以后会重新发ACK。假如在2MSL时间中客户端都没有收到服务端重发的FIN,那么客户端认为服务端已经收到了客户端发送的ACK， 此时客户端才可以放心的断开连接。

  * 为什么等待时间是2MSL?

  客户端发送的ACK segment存活期1MSL,服务端重发FIN segment存活期1MSL，加一起2MSL。2MSL是一个临界值，利用尽量大的等待时间来确保TCP连接断开的可靠性。

  #### 20.HTTP3.0

  HTTP3.0又称为HTTP Over QUIC，其弃用TCP协议，改为使用基于UDP协议的QUIC协议来实现。

  ![img](https://s2.51cto.com/oss/202009/12/f2ea7ee7d8a29615f8b80440eb0a20ac.png)

  QUIC协议必须要实现HTTP2.0在TCP协议上的重要功能，同时解决遗留问题，我们来看看QUIC是如何实现的。

  **3.1 队头阻塞问题**

  队头阻塞 Head-of-line blocking(缩写为HOL blocking)是计算机网络中是一种性能受限的现象，通俗来说就是：一个数据包影响了一堆数据包，它不来大家都走不了。

  队头阻塞问题可能存在于HTTP层和TCP层，在HTTP1.x时两个层次都存在该问题。

  HTTP2.0协议的多路复用机制解决了HTTP层的队头阻塞问题，但是在TCP层仍然存在队头阻塞问题。

  TCP协议在收到数据包之后，这部分数据可能是乱序到达的，但是TCP必须将所有数据收集排序整合后给上层使用，如果其中某个包丢失了，就必须等待重传，从而出现某个丢包数据阻塞整个连接的数据使用。

  QUIC协议是基于UDP协议实现的，在一条链接上可以有多个流，流与流之间是互不影响的，当一个流出现丢包影响范围非常小，从而解决队头阻塞问题。

  **3.2 0RTT 建链**

  衡量网络建链的常用指标是RTT Round-Trip Time，也就是数据包一来一回的时间消耗。

  [![img](https://s3.51cto.com/oss/202009/12/639bd2fc429a71a6d623052a581d292b.png)](https://s3.51cto.com/oss/202009/12/639bd2fc429a71a6d623052a581d292b.png)

  RTT包括三部分：往返传播时延、网络设备内排队时延、应用程序数据处理时延。

  [![img](https://s2.51cto.com/oss/202009/12/680cd20b6219f1606724d6641c56a660.png)](https://s2.51cto.com/oss/202009/12/680cd20b6219f1606724d6641c56a660.png)

  一般来说HTTPS协议要建立完整链接包括:TCP握手和TLS握手，总计需要至少2-3个RTT，普通的HTTP协议也需要至少1个RTT才可以完成握手。

  然而，QUIC协议可以实现在第一个包就可以包含有效的应用数据，从而实现0RTT，但这也是有条件的。

  简单来说，基于TCP协议和TLS协议的HTTP2.0在真正发送数据包之前需要花费一些时间来完成握手和加密协商，完成之后才可以真正传输业务数据。

  但是QUIC则第一个数据包就可以发业务数据，从而在连接延时有很大优势，可以节约数百毫秒的时间。

  [![img](https://s3.51cto.com/oss/202009/12/f4b3d8d9b7406711a2cc52d36299b40b.png-wh_600x-s_4143360623.png)](https://s3.51cto.com/oss/202009/12/f4b3d8d9b7406711a2cc52d36299b40b.png-wh_600x-s_4143360623.png)

  QUIC的0RTT也是需要条件的，对于第一次交互的客户端和服务端0RTT也是做不到的，毕竟双方完全陌生。

  因此，QUIC协议可以分为首次连接和非首次连接，两种情况进行讨论。

  **3.3 首次连接和非首次连接**

  使用QUIC协议的客户端和服务端要使用1RTT进行密钥交换，使用的交换算法是DH(Diffie-Hellman)迪菲-赫尔曼算法。

  DH算法开辟了密钥交换的新思路，在之前的文章中提到的RSA算法也是基于这种思想实现的，但是DH算法和RSA的密钥交换不完全一样，感兴趣的读者可以看看DH算法的数学原理。

  DH算法开辟了密钥交换的新思路，在之前的文章中提到的RSA算法也是基于这种思想实现的，但是DH算法和RSA的密钥交换不完全一样，感兴趣的读者可以看看DH算法的数学原理。

  **3.3.1 首次连接**

  简单来说一下，首次连接时客户端和服务端的密钥协商和数据传输过程，其中涉及了DH算法的基本过程：

  [![img](https://s5.51cto.com/oss/202009/12/a381ee3c0c58eb5906010f1967f9b79c.png-wh_600x-s_1149123713.png)](https://s5.51cto.com/oss/202009/12/a381ee3c0c58eb5906010f1967f9b79c.png-wh_600x-s_1149123713.png)

  **3.3.2 非首次连接**

  前面提到客户端和服务端首次连接时服务端传递了config包，里面包含了服务端公钥和两个随机数，客户端会将config存储下来，后续再连接时可以直接使用，从而跳过这个1RTT，实现0RTT的业务数据交互。

  客户端保存config是有时间期限的，在config失效之后仍然需要进行首次连接时的密钥交换。

  **3.4 前向安全问题**

  前向安全是密码学领域的专业术语，看下百度上的解释：

  前向安全或前向保密Forward Secrecy是密码学中通讯协议的安全属性，指的是长期使用的主密钥泄漏不会导致过去的会话密钥泄漏。

  前向安全能够保护过去进行的通讯不受密码或密钥在未来暴露的威胁，如果系统具有前向安全性，就可以保证在主密钥泄露时历史通讯的安全，即使系统遭到主动攻击也是如此。

  通俗来说，前向安全指的是密钥泄漏也不会让之前加密的数据被泄漏，影响的只有当前，对之前的数据无影响。

  前面提到QUIC协议首次连接时先后生成了两个加密密钥，由于config被客户端存储了，如果期间服务端私钥泄漏，那么可以根据K = mod p计算出密钥K。

  如果一直使用这个密钥进行加解密，那么就可以用K解密所有历史消息，因此后续又生成了新密钥，使用其进行加解密，当时完成交互时则销毁，从而实现了前向安全。

  [![img](https://s5.51cto.com/oss/202009/12/1e4f5bfbaa79a6ac5e78af3a7df0f4b3.png-wh_600x-s_3126673257.png)](https://s5.51cto.com/oss/202009/12/1e4f5bfbaa79a6ac5e78af3a7df0f4b3.png-wh_600x-s_3126673257.png)

  **3.5 前向纠错**

  前向纠错是通信领域的术语，看下百科的解释：

  前向纠错也叫前向纠错码Forward Error Correction 简称FEC 是增加数据通讯可信度的方法，在单向通讯信道中，一旦错误被发现，其接收器将无权再请求传输。

  FEC 是利用数据进行传输冗余信息的方法，当传输中出现错误，将允许接收器再建数据。

  听这段描述就是做校验的，看看QUIC协议是如何实现的：

  QUIC每发送一组数据就对这组数据进行异或运算，并将结果作为一个FEC包发送出去，接收方收到这一组数据后根据数据包和FEC包即可进行校验和纠错。

  **3.6 连接迁移**

  网络切换几乎无时无刻不在发生。

  TCP协议使用五元组来表示一条唯一的连接，当我们从4G环境切换到wifi环境时，手机的IP地址就会发生变化，这时必须创建新的TCP连接才能继续传输数据。

  QUIC协议基于UDP实现摒弃了五元组的概念，使用64位的随机数作为连接的ID，并使用该ID表示连接。

  基于QUIC协议之下，我们在日常wifi和4G切换时，或者不同基站之间切换都不会重连，从而提高业务层的体验。

  [![img](https://s3.51cto.com/oss/202009/12/7ae1354afe6af5c993aaffbe3d8e5907.png-wh_600x-s_3347289128.png)](https://s3.51cto.com/oss/202009/12/7ae1354afe6af5c993aaffbe3d8e5907.png-wh_600x-s_3347289128.png)

  **4. QUIC的应用和前景**

  通过前面的一些介绍我们看出来QUIC协议虽然是基于UDP来实现的，但是它将TCP的重要功能都进行了实现和优化，否则使用者是不会买账的。

  QUIC协议的核心思想是将TCP协议在内核实现的诸如可靠传输、流量控制、拥塞控制等功能转移到用户态来实现，同时在加密传输方向的尝试也推动了TLS1.3的发展。

  但是TCP协议的势力过于强大，很多网络设备甚至对于UDP数据包做了很多不友好的策略，进行拦截从而导致成功连接率下降。

  主导者谷歌在自家产品做了很多尝试，国内腾讯公司也做了很多关于QUIC协议的尝试。

  其中腾讯云对QUIC协议表现了很大的兴趣，并做了一些优化然后在一些重点产品中对连接迁移、QUIC成功率、弱网环境耗时等进行了实验，给出了来自生产环境的诸多宝贵数据。

  简单看一组腾讯云在移动互联网场景下的不同丢包率下的请求耗时分布：

  ![img](https://s5.51cto.com/oss/202009/12/3172e5b485e5d04448333dc317ada456.png-wh_600x-s_1079457352.png)

  ## 21.HTTP和RPC的区别

  RPC（即Remote Procedure Call，远程过程调用）和HTTP（HyperText Transfer Protocol，超文本传输协议）他们最本质的区别，就是RPC主要工作在TCP协议之上，而HTTP服务主要是工作在HTTP协议之上，我们都知道HTTP协议是在传输层协议TCP之上的，所以效率来看的话，RPC当然是要更胜一筹。

  ## 22.tcp快速打开技术

  通过cookie来记录从而减少握手次数

  ## 23.HTTP请求都会收到什么东西

  请求报文由请求行（request line）、请求头（header）、请求体四个部分组成,如下图所示：

  #### 1. 请求行包含请求方法、URL、协议版本

  - 请求方法包含 8 种：GET、POST、PUT、DELETE、PATCH、HEAD、OPTIONS、TRACE。
  - URL 即请求地址，由 <协议>：//<主机>：<端口>/<路径>?<参数> 组成
  - 协议版本即 http 版本号

  ```
  POST /chapter17/user.html HTTP/1.1
  ```

  以上代码中“POST”代表请求方法，“/chapter17/user.html”表示 URL，“HTTP/1.1”代表协议和协议的版本。现在比较流行的是 Http1.1 版本

  #### 2. 请求头包含请求的附加信息，由关键字/值对组成，每行一对，关键字和值用英文冒号“:”分隔。

  请求头部通知服务器有关于客户端请求的信息。它包含许多有关的客户端环境和请求正文的有用信息。其中比如：**Host，表示主机名，虚拟主机；Connection,HTTP/1.1 增加的，使用 keepalive，即持久连接，一个连接可以发多个请求；User-Agent，请求发出者，兼容性以及定制化需求。**

  #### 3. 请求体，可以承载多个请求参数的数据，包含回车符、换行符和请求数据，并不是所有请求都具有请求数据。

  ```
  name=tom&password=1234&realName=tomson
  ```

  上面代码，承载着 name、password、realName 三个请求参数。

  ## 24.网络中的一些攻击原理：

  ### CSRF攻击：

  CSRF（Cross-site request forgery）跨站请求伪造：攻击者诱导受害者进入第三方网站，在第三方网站中，向被攻击网站发送跨站请求。利用受害者在被攻击网站已经获取的注册凭证，绕过后台的用户验证，达到冒充用户对被攻击的网站执行某项操作的目的。

  一个典型的CSRF攻击有着如下的流程：

  - 受害者登录a.com，并保留了登录凭证（Cookie）。
  - 攻击者引诱受害者访问了b.com。
  - b.com 向 a.com 发送了一个请求：a.com/act=xx。浏览器会默认携带a.com的Cookie。
  - a.com接收到请求后，对请求进行验证，并确认是受害者的凭证，误以为是受害者自己发送的请求。
  - a.com以受害者的名义执行了act=xx。
  - 攻击完成，攻击者在受害者不知情的情况下，冒充受害者，让a.com执行了自己定义的操作。

  ### 几种常见的攻击类型

  **GET类型的CSRF**

  GET类型的CSRF利用非常简单，只需要一个HTTP请求，一般会这样利用：

  ```html
   ![](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/ff0cdbee.example/withdraw?amount=10000&for=hacker)
   
  ```

  在受害者访问含有这个img的页面后，浏览器会自动向`http://bank.example/withdraw?account=xiaoming&amount=10000&for=hacker`发出一次HTTP请求。bank.example就会收到包含受害者登录信息的一次跨域请求。

  **POST类型的CSRF**

  这种类型的CSRF利用起来通常使用的是一个自动提交的表单，如：

  ```html
   <form action="http://bank.example/withdraw" method="POST">
      <input type="hidden" name="account" value="xiaoming">
      <input type="hidden" name="amount" value="10000">
      <input type="hidden" name="for" value="hacker">
  </form>
  <script> document.forms[0].submit(); </script> 
  ```

  访问该页面后，表单会自动提交，相当于模拟用户完成了一次POST操作。

  POST类型的攻击通常比GET要求更加严格一点，但仍并不复杂。任何个人网站、博客，被黑客上传页面的网站都有可能是发起攻击的来源，后端接口不能将安全寄托在仅允许POST上面。

  **链接类型的CSRF**

  链接类型的CSRF并不常见，比起其他两种用户打开页面就中招的情况，这种需要用户点击链接才会触发。这种类型通常是在论坛中发布的图片中嵌入恶意链接，或者以广告的形式诱导用户中招，攻击者通常会以比较夸张的词语诱骗用户点击，例如：

  ```html
    <a href="http://test.com/csrf/withdraw.php?amount=1000&for=hacker" taget="_blank" target="_blank" rel="noopener">
    重磅消息！！
    </a><a>
  ```

  由于之前用户登录了信任的网站A，并且保存登录状态，只要用户主动访问上面的这个PHP页面，则表示攻击成功。

  ### CSRF的特点

  - 攻击一般发起在第三方网站，而不是被攻击的网站。被攻击的网站无法防止攻击发生。
  - 攻击利用受害者在被攻击网站的登录凭证，冒充受害者提交操作；而不是直接窃取数据。
  - 整个过程攻击者并不能获取到受害者的登录凭证，仅仅是“冒用”。
  - 跨站请求可以用各种方式：图片URL、超链接、CORS、Form提交等等。部分请求方式可以直接嵌入在第三方论坛、文章中，难以进行追踪。

  CSRF通常是跨域的，因为外域通常更容易被攻击者掌控。但是如果本域下有容易被利用的功能，比如可以发图和链接的论坛和评论区，攻击可以直接在本域下进行，而且这种攻击更加危险。

  ## 防护策略

  CSRF通常从第三方网站发起，被攻击的网站无法防止攻击发生，只能通过增强自己网站针对CSRF的防护能力来提升安全性。

  上文中讲了CSRF的两个特点：

  - CSRF（通常）发生在第三方域名。
  - CSRF攻击者不能获取到Cookie等信息，只是使用。

  针对这两点，我们可以专门制定防护策略，如下：

  - 阻止不明外域的访问

    - 同源检测

    既然CSRF大多来自第三方网站，那么我们就直接禁止外域（或者不受信任的域名）对我们发起请求。

    - Samesite Cookie

  - 提交时要求附加本域才能获取的信息

    - CSRF Token

    当用户从客户端得到了Token，再次提交给服务器的时候，服务器需要判断Token的有效性，验证过程是先解密Token，对比加密字符串以及时间戳，如果加密字符串一致且时间未过期，那么这个Token就是有效的。

    这种方法要比之前检查Referer或者Origin要安全一些，Token可以在产生并放于Session之中，然后在每次请求时把Token从Session中拿出，与请求中的Token进行比对，但这种方法的比较麻烦的在于如何把Token以参数的形式加入请求。

    - 双重Cookie验证

  ### XSS攻击

  Cross-Site Scripting（跨站脚本攻击）简称 XSS，是一种代码注入攻击。攻击者通过在目标网站上注入恶意脚本，使之在用户的浏览器上运行。利用这些恶意脚本，攻击者可获取用户的敏感信息如 Cookie、SessionID 等，进而危害数据安全。

  为了和 CSS 区分，这里把攻击的第一个字母改成了 X，于是叫做 XSS。

  XSS 的本质是：恶意代码未经过滤，与网站正常的代码混在一起；浏览器无法分辨哪些脚本是可信的，导致恶意脚本被执行。

  而由于直接在用户的终端执行，恶意代码能够直接获取用户的信息，或者利用这些信息冒充用户向网站发起攻击者定义的请求。

  在部分情况下，由于输入的限制，注入的恶意脚本比较短。但可以通过引入外部的脚本，并由浏览器执行，来完成比较复杂的攻击策略。

  这里有一个问题：用户是通过哪种方法“注入”恶意脚本的呢？

  不仅仅是业务上的“用户的 UGC 内容”可以进行注入，包括 URL 上的参数等都可以是攻击的来源。在处理输入时，以下内容都不可信：

  - 来自用户的 UGC 信息
  - 来自第三方的链接
  - URL 参数
  - POST 参数
  - Referer （可能来自不可信的来源）
  - Cookie （可能来自其他子域注入）

  分类：

  #### 存储型 XSS

  存储型 XSS 的攻击步骤：

  1. 攻击者将恶意代码提交到目标网站的数据库中。
  2. 用户打开目标网站时，网站服务端将恶意代码从数据库取出，拼接在 HTML 中返回给浏览器。
  3. 用户浏览器接收到响应后解析执行，混在其中的恶意代码也被执行。
  4. 恶意代码窃取用户数据并发送到攻击者的网站，或者冒充用户的行为，调用目标网站接口执行攻击者指定的操作。

  这种攻击常见于带有用户保存数据的网站功能，如论坛发帖、商品评论、用户私信等。

  #### 反射型 XSS

  反射型 XSS 的攻击步骤：

  1. 攻击者构造出特殊的 URL，其中包含恶意代码。
  2. 用户打开带有恶意代码的 URL 时，网站服务端将恶意代码从 URL 中取出，拼接在 HTML 中返回给浏览器。
  3. 用户浏览器接收到响应后解析执行，混在其中的恶意代码也被执行。
  4. 恶意代码窃取用户数据并发送到攻击者的网站，或者冒充用户的行为，调用目标网站接口执行攻击者指定的操作。

  反射型 XSS 跟存储型 XSS 的区别是：存储型 XSS 的恶意代码存在数据库里，反射型 XSS 的恶意代码存在 URL 里。

  反射型 XSS 漏洞常见于通过 URL 传递参数的功能，如网站搜索、跳转等。

  由于需要用户主动打开恶意的 URL 才能生效，攻击者往往会结合多种手段诱导用户点击。

  POST 的内容也可以触发反射型 XSS，只不过其触发条件比较苛刻（需要构造表单提交页面，并引导用户点击），所以非常少见。

  #### DOM 型 XSS

  DOM 型 XSS 的攻击步骤：

  1. 攻击者构造出特殊的 URL，其中包含恶意代码。
  2. 用户打开带有恶意代码的 URL。
  3. 用户浏览器接收到响应后解析执行，前端 JavaScript 取出 URL 中的恶意代码并执行。
  4. 恶意代码窃取用户数据并发送到攻击者的网站，或者冒充用户的行为，调用目标网站接口执行攻击者指定的操作。

  DOM 型 XSS 跟前两种 XSS 的区别：DOM 型 XSS 攻击中，取出和执行恶意代码由浏览器端完成，属于前端 JavaScript 自身的安全漏洞，而其他两种 XSS 都属于服务端的安全漏洞。

  预防：

  很难通过技术手段完全避免 XSS，但我们可以总结以下原则减少漏洞的产生：

  - **利用模板引擎** 开启模板引擎自带的 HTML 转义功能。例如： 在 ejs 中，尽量使用 `<%= data %>` 而不是 `<%- data %>`； 在 doT.js 中，尽量使用 `{{! data }` 而不是 `{{= data }`； 在 FreeMarker 中，确保引擎版本高于 2.3.24，并且选择正确的 `freemarker.core.OutputFormat`。

  - **避免内联事件** 尽量不要使用 `onLoad="onload('{{data}}')"`、`onClick="go('{{action}}')"` 这种拼接内联事件的写法。在 JavaScript 中通过 `.addEventlistener()` 事件绑定会更安全。<!--%---><!--%=--></a><!--参数--><!--路径--><!--端口--><!--主机--><!--协议--></bits></iostream><!--=leftalloc<=nobjs*n，就分配--></int></int><!--只能重载为友元函数。--><!--和--></typename></sys></unistd.h></t>

- **避免拼接 HTML** 前端采用拼接 HTML 的方法比较危险，如果框架允许，使用 `createElement`、`setAttribute` 之类的方法实现。或者采用比较成熟的渲染框架，如 Vue/React 等。

- **时刻保持警惕** 在插入位置为 DOM 属性、链接等位置时，要打起精神，严加防范。

- **增加攻击难度，降低攻击后果** 通过 CSP、输入长度配置、接口安全措施等方法，增加攻击的难度，降低攻击的后果。

- **主动检测和发现** 可使用 XSS 攻击字符串和自动扫描工具寻找潜在的 XSS 漏洞。

- #### 纯前端渲染

  纯前端渲染的过程：

  1. 浏览器先加载一个静态 HTML，此 HTML 中不包含任何跟业务相关的数据。
  2. 然后浏览器执行 HTML 中的 JavaScript。
  3. JavaScript 通过 Ajax 加载业务数据，调用 DOM API 更新到页面上。

  在纯前端渲染中，我们会明确的告诉浏览器：下面要设置的内容是文本（`.innerText`），还是属性（`.setAttribute`），还是样式（`.style`）等等。浏览器不会被轻易的被欺骗，执行预期外的代码了。

  但纯前端渲染还需注意避免 DOM 型 XSS 漏洞（例如 `onload` 事件和 `href` 中的 `javascript:xxx` 等，请参考下文”预防 DOM 型 XSS 攻击“部分）。

### SYNLood攻击

SYN Flood攻击如何工作？

通过利用TCP连接的握手过程，SYN Flood攻击工作。在正常情况下，TCP连接显示三个不同的进程以进行连接。

1.首先，客户端向服务器发送SYN数据包，以便启动连接。

2.服务器响应该初始包与SYN / ACK包，以确认通信。

3.最后，客户端返回ACK数据包以确认从服务器接收到的数据包。完成这个数据包发送和接收序列后，TCP连接打开并能发送和接收数据。



![img](https://pic3.zhimg.com/80/v2-e44dd1bd4306d59f9fbc313fb811ca26_1440w.jpg)

**img**



为了创建拒绝服务，攻击者利用这样的漏洞，即在接收到初始SYN数据包之后，服务器将用一个或多个SYN / ACK数据包进行响应，并等待握手中的最后一步。这是它的工作原理：

攻击者向目标服务器发送大量SYN数据包，通常会使用欺骗性的IP地址。

然后，服务器响应每个连接请求，并留下开放端口准备好接收响应。

当服务器等待从未到达的最终ACK数据包时，攻击者继续发送更多的SYN数据包。每个新的SYN数据包的到达导致服务器暂时维持新的开放端口连接一段时间，一旦所有可用端口被使用，服务器就无法正常工作。



![img](https://pic4.zhimg.com/80/v2-e930712ecec724de9fc84b7065c08a63_1440w.jpg)

**img**



在网络中，当服务器断开连接但连接另一端的机器没有连接时，连接被认为是半开的。在这种类型的DDoS攻击中，目标服务器不断离开打开的连接，等待每个连接超时，然后端口再次可用。结果是这种攻击可以被认为是“半开攻击”。

防御：

- 扩展积压工作队列

目标设备安装的每个操作系统都允许具有一定数量的半开连接。若要响应大量 SYN 数据包，一种方法是增加操作系统允许的最大半开连接数目。为成功扩展最大积压工作，系统必须额外预留内存资源以处理各类新请求。如果系统没有足够的内存，无法应对增加的积压工作队列规模，将对系统性能产生负面影响，但仍然好过拒绝服务。

- 回收最先创建的 TCP 半开连接

另一种缓解策略是在填充积压工作后覆盖最先创建的半开连接。这项策略要求完全建立合法连接的时间低于恶意 SYN 数据包填充积压工作的时间。当攻击量增加或积压工作规模小于实际需求时，这项特定的防御措施将不奏效。

### ACK Flood

指攻击者通过使用TCP ACK数据包使服务器过载。像其他DDoS攻击一样，ACK Flood攻击的目的是通过使用垃圾数据来减慢攻击目标的速度或使其崩溃，从而导致拒绝向其他用户提供服务。目标服务器被迫处理接收到的每个ACK数据包，消耗太多计算能力，以至于无法为合法用户提供服务。

### DNS劫持：

DNS劫持就是通过劫持了DNS服务器，通过某些手段取得某域名的解析记录控制权，进而修改此域名的解析结果，导致对该域名的访问由原IP地址转入到修改后的指定IP，其结果就是对特定的网址不能访问或访问的是假网址，从而实现窃取资料或者破坏原有正常服务的目的。DNS劫持通过篡改DNS服务器上的数据返回给用户一个错误的查询结果来实现的。

DNS劫持症状：在某些地区的用户在成功连接宽带后，首次打开任何页面都指向ISP提供的“电信互联星空”、“网通黄页广告”等内容页面。还有就是曾经出现过用户访问Google域名的时候出现了百度的网站。这些都属于DNS劫持。

**解决方法**

对于DNS劫持，可以采用使用国外免费公用的DNS服务器解决。例如OpenDNS（208.67.222.222）或GoogleDNS（8.8.8.8）。

### DNS污染：

DNS污染是一种让一般用户由于得到虚假目标主机IP而不能与其通信的方法，是一种DNS缓存投毒攻击（DNS cache poisoning）。

其工作方式是：由于通常的DNS查询没有任何认证机制，而且DNS查询通常基于的UDP是无连接不可靠的协议，因此DNS的查询非常容易被篡改，通过对UDP端口53上的DNS查询进行入侵检测，一经发现与关键词相匹配的请求则立即伪装成目标域名的解析服务器（NS，Name Server）给查询者返回虚假结果。

DNS污染发生在用户请求的第一步上，直接从协议上对用户的DNS请求进行干扰。
DNS污染症状：目前一些被禁止访问的网站很多就是通过DNS污染来实现的，例如YouTube、Facebook等网站。

**解决办法**：

对于DNS污染，可以说，个人用户很难单单靠设置解决，通常可以使用VPN或者域名远程解析的方法解决，但这大多需要购买付费的VPN或SSH等，也可以通过修改Hosts的方法，手动设置域名正确的IP地址。

# 25.ICMP协议

ICMP是（Internet Control Message Protocol）Internet控制[报文](http://baike.baidu.com/view/175122.htm)协议。它是[TCP\/IP协议族](http://baike.baidu.com/view/2221037.htm)的一个子协议，用于在IP[主机](http://baike.baidu.com/view/23880.htm)、[路由](http://baike.baidu.com/view/18655.htm)器之间传递控制消息。控制消息是指[网络通](http://baike.baidu.com/view/8079702.htm)不通、[主机](http://baike.baidu.com/view/23880.htm)是否可达、[路由](http://baike.baidu.com/view/18655.htm)是否可用等网络本身的消息。这些控制消息虽然并不传输用户数据，但是对于用户数据的传递起着重要的作用。

## Ping

ping命令主要是用于检测网络的连通性。

Ping命令发送一个ICMP请求报文给目的IP，然后目的IP回复一个ICMP报文。

原理：网络上的机器都有唯一确定的IP地址，我们给目标IP地址发送一个数据包，对方就要返回一个同样大小的数据包，根据返回的数据包我们可以确定目标主机的存在，可以初步判断目标主机的操作系统等。

因为ping命令是使用ICMP协议，所以没有端口号，但是有两个域：类型和代码。

可能返回的消息列表：

11 超时

12 参数问题

3 终点不可达

4源点抑制，通知送信方抑制发送数据包

## traceroute：

通过发送探测报文来获取链路地址信息。第一个探测报文TTL为1，到达第一个路由器时，TTL减1为0所以丢掉这个探测包，同时向源主机发回ICMP时间超过报文，这时源主机就获得了第一个路由器的IP地址；接着源主机发送第二个探测报文，TTL增1为2，到达第一个路由器TTL减1为1并转发探测包到第二个路由器，这时TTL减1为0，丢掉这个探测包并向源主机发回ICMP时间超过报文，源主机就获得了第二个路由器的IP地址；以此类推，直到探测报文到达traceroute的目的地，这时源主机就获得了到目的地的每一跳路由的IP地址。

# 26.TCP建立连接服务器崩溃断电情况会发生怎样情况

## 1.服务器崩溃：

服务器会发送RST包让客户端关闭连接
发送RST包的情况：
1)服务器没有打开对应的端口
2)服务器想主动关闭连接非优雅关闭非优雅关闭
3)在一个已关闭的socket中收到数据一般是半打开连接，一方关闭了另一方却不知道一般是半打开连接，一方关闭了另一方却不知道

## 2.服务器断电、网线被拔：

包不能被接收，需要客户端开启keep-alive，长期未收到响应就主动关闭连接。
一般心跳包的机制是：客户端主动发送，服务器定时接收。若客户端没有收到响应就判定服务器断连，若服务器没有到收到心跳包就判定客户端断连。

# 27.HTTP的响应时间：

| 1. DNS 解析网站域名               | 1-RTT |      |
| :-------------------------------- | :---- | :--- |
| 2. 访问 HTTP 网页 TCP 握手        | 1-RTT |      |
| 3. HTTPS 重定向 302               | 1-RTT |      |
| 4. 访问 HTTPS 网页 TCP 握手       | 1-RTT |      |
| 5. TLS 握手第一阶段 Say Hello     | 1-RTT |      |
| 6. 【证书校验】CA 站点的 DNS 解析 | 1-RTT |      |
| 7. 【证书校验】CA 站点的 TCP 握手 | 1-RTT |      |
| 8. 【证书校验】请求 OCSP 验证     | 1-RTT |      |
| 9. TLS 握手第二阶段 加密          | 1-RTT |      |
| 10. 第一个 HTTPS 请求             | 1-RTT |      |

总共10个RTT

## HTTP重定向：

URL 重定向，也称为 URL 转发，是一种当实际资源，如单个页面、表单或者整个 Web 应用被迁移到新的 URL 下的时候，保持（原有）链接可用的技术。HTTP 协议提供了一种特殊形式的响应—— HTTP 重定向（HTTP redirects）来执行此类操作。

重定向可实现许多目标：

- 站点维护或停机期间的临时重定向。
- 永久重定向将在更改站点的URL，上传文件时的进度页等之后保留现有的链接/书签。
- 上传文件时的表示进度的页面。

## TLS握手过程

其实TLS提供了多种密钥交换算法，这里以RSA为例，分步讲解以下TLS的握手过程。

##### 客户端初始请求

客户端向服务器发出请求，会带上以下信息：

1. 一个客户端生成的随机数
2. 支持的加密方式，即图中的Cipher Suite
3. 支持SSL/TLS协议的版本号
4. Session ID，如果是之前断开的会话，会带上Session ID用来恢复会话。
5. 签名使用的哈希算法

##### 服务器回应

服务器响应客户端的请求，如果上一步带上了Session ID，则直接恢复会话，否则带上以下信息给客户端：

1. 选择SSL/TLS协议版本号
2. 选择加密方式
3. 一个服务器生成的随机数
4. 服务器证书

客户端会在这里校验服务器证书的合法性，包括检测证书有效时间，以及证书中域名与当前会话域名是否匹配，并沿着信任链查找顶层证书颁发者是否是操作系统信任的CA机构，这里验证信任链的时候，需要用上一级证书的公钥对证书里的签名进行解密，还原对应的摘要值，再使用证书信息计算证书的摘要值，最后通过对比两个摘要值是否相等，如果不相等则认为该证书不可信，如果相等则认为该级证书链正确，以此类推对整个证书链进行校验。

##### 客户端回应

客户端校验服务器证书的合法性，生成premaster secret，向服务器发送以下信息：

1. 用服务器证书取出的公钥加密后的premaster secret，这里的premaster secret其实是另一个随机数
2. 加密约定改变通知，通知服务器，以后的通信都适用协商好的加密方法和密钥进行加密
3. 客户端握手结束通知。这个报文也是验证消息，是前面发送的所有内容的哈希值，用来供服务器校验。

##### 服务器最后确认

这是握手过程的最后一步，服务器会把以下信息发送给客户端：

1. 加密约定改变通知，通知客户端，以后的通信都适用协商好的加密方法和密钥进行加密
2. 服务器握手结束通知，该报文也作为校验消息，供客户端验证。

到这个时候，客户端和服务器同时拥有了3个随机数，使用这三个随机数生成的密钥，将被用于后续通信的对称加密。也就是说，只有握手过程才有非对称加密，非对称加密是比较慢的，因此只用于建立安全的信道，Payload的传输都是对称加密的，对称加密的速度和资源占用比非对称加密都要好，因此对性能的影响是很小的。

# 28.10M带宽下下载速度应该是多少呢

一台10M独享带宽的服务器，理论上理想的下载速度是10M/8=1.25M/S ,即每秒下载速度最大是1.25MB,实际使用中，如果你的下载速度接近或者等于这个数值，就是正常的，另外，下载速度受下载源文件所在服务器的影响，也就是说不是你随便下载一个文件都能有这个速度，从不同网站下载文件，由于对方服务器是否繁忙，是否有所限制等原因．也将影响你的下载速度．所以可以寻找稳定的下载网站进行文件的下载测试．

# 数据库

#### 1.数据库索引

在关系数据库中，如果有上万甚至上亿条记录，在查找记录的时候，想要获得非常快的速度，就需要使用索引。

索引是关系数据库中对某一列或多个列的值进行预排序的数据结构。通过使用索引，可以让数据库系统不必扫描整个表，而是直接定位到符合条件的记录，这样就大大加快了查询速度。

为什么要用B树做索引：

优点一： B+树只有叶节点存放数据，其余节点用来索引，而B-树是每个索引节点都会有Data域。

优点二： B+树所有的Data域在叶子节点，并且所有叶子节点之间都有一个链指针。 这样遍历叶子节点就能获得全部数据，这样就能进行区间访问啦。在数据库中基于范围的查询是非常频繁的，而B树不支持这样的遍历操作。

#### 2.数据库事务

事务（Transaction）是并发控制的基本单位。所谓的事务，它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。例如，银行转账工作：从一个账号扣款并使另一个账号增款，这两个操作要么都执行，要么都不执行。所以，应该把它们看成一个事务。事务是数据库维护数据一致性的单位，在每个事务结束时，都能保 持数据一致性。

#### 3.数据库的三大范式

1.第一范式：属性具有原子性，不可再分解，例如：日期这个属性，如果认为日期可以分解成年月日，含有该属性的表就不是第一范式2.
2.第二范式：在第一范式的基础上，非主属性完全依赖于候选键（若干主属性组成），不存在部分依赖
3.第三范式：在满足第二范式的基础上，消除传递依赖，就是说每个非主属性都不传递依赖于候选码。例如：（学生，课程，老师，老师职称），（学生，课程）->老师->老师职称，老师和老师职称这里存在传递依赖。
BC范式：在满足第三范式的基础上主属性不依赖于主属性

4.第四范式：

必须满足第三范式

表中不能包含一个实体的两个或多个互相独立的多值因子。

显然，第四范式也是一个比第三范式严格的范式。

第四范式的意思是：当一个表中的非主属性互相独立时（3NF），这些非主属性不应该有多值。若有多值就违反了第四范式。

5.第五范式

必须满足第四范式，表必须可以分解为较小的表，除非那些表在逻辑上拥有与原始表相同的主键。

#### 4.数据库ACID特性

事务：要做的事情，对数据库进行读写操作。事务是并发控制的基本单位，一个程序包含多个事务
AAtomicityAtomicityCConsistencyConsistencyIIsolationIsolationDDurabilityDurability==>原子性，一致性，隔离性，持久性

原子性：事务作为一个整体被执行，对数据的操作要么全都执行要么全不执行
一致性：事务应确保从一个一致状态转变到另一个一致状态，一致状态的含义是数据库中的数据应满足完整性约束
隔离性：多个事务并发执行时，一个事务的执行不影响其他事务的执行
持久性：一个事务一旦提交，他对数据库的修改应该永久保存在数据库中
举例：
A向B转钱，分为六个过程：
1.读取A账户的钱；
2.A账户的钱递减；
3.写回A账户；
4.读取B账户的钱；
5.B账户的钱递增；
6.写回B账户。

原子性：以上转钱的中间过程要么全做，要么全部不做，中间如果某个步骤出错，则回滚（单库事务回滚使用ROLLBACK）到初始的状态，通过undo log来实现（在操作任何数据之前，首先将数据备份到undo log中，进行数据修改，如果出现了错误，执行了ROLLBACK语句，系统可以利用undo log中的备份将数据恢复到事务开始之前的状态）
一致性：A账户和B账户，转账之前和转账之后钱的总数是不变的，通过undo log来实现然后
隔离性：在进行并发操作时，A向B转账，C也向B转账，转账完成之后，B账户的钱是两次转账的总和；在A向B转账的过程中，只要事务还没有提交，两个账户里面的钱不会发生变化
持久性：转账成功，两个账号的钱会写入数据库中做持久化保存，通过redo log重做日志重做日志来实现（redo log记录的是新数据的备份，在事务提交之前只要将redo log持久化即可，不需要将数据持久化，当系统崩溃时，系统可以根据redo log的内容，将数据恢复到最新的状态）
原子性和一致性的关联：
原子性的破坏可能导致数据库的不一致，数据的不一致并不都和原子性有关。
例如：A账户向B账户转100元，但是B账户只增加了50元，此过程符合事务的原子性，但是数据的一致性被破坏了。

#### 5.Mysql的四种隔离状态

该隔离级别的事务会读到其它未提交事务的数据，此现象也称之为 **脏读** 。

一个事务可以读取另一个已提交的事务，多次读取会造成不一样的结果，此现象称为**不可重复读问题**

幻读和不可重复读类似，但不可重复读重点在于update和delete，而幻读的重点在于insert。

通过对数据库加锁实现并发控制，锁分为：排它锁（X锁，写锁）和共享锁（S锁，读锁）

隔离级别：四种隔离级别（读未提交、读已提交、可重复读、串行化）

- 未提交读ReadUncommittedReadUncommitted：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据
- 提交读ReadCommittedReadCommitted：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 不重复读不重复读
- 可重复读RepeatedReadRepeatedRead：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读
- 串行读SerializableSerializable：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞

进行读操作时，需要申请并获得S锁（共享锁，其他事务可以继续加锁，但不能加排它锁）；进行写操作时，需要申请并获得X锁（排它锁，其他事务不能再获得任何锁）
《一级封锁协议》（写操作时，加X锁，只有当X锁释放后，才能允许其他事物对该事务操作），可防止丢失修改；
《二级封锁协议》（在一级封锁协议的基础上，在读数据时加S锁，读完后释放），可防止不丢失修改，不读脏数据；
《三级封锁协议》（在一级封锁协议的基础上，读数据时加S锁，直到事务结束时释放）可防止不丢失修改，不读脏数据，不可重复读。
mysql的默认隔离级别是：可重复读。myISAM和InnoDB都支持表级锁，但Innodb默认是行级锁。
oracle支持读已提交和串行化两种隔离级别，默认情况是读已提交



![在这里插入图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**在这里插入图片描述**



#### 6.mysql的MVCC机制（实现了可重复读）



![img](https://img-blog.csdnimg.cn/20190515143938256.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nhbnl1ZXNhbjAwMDA=,size_16,color_FFFFFF,t_70)

**img**



#### 7.mysql优化方法

https://zhuanlan.zhihu.com/p/59818056

1. 表的设计要合理化，符合三范式（3FN）
2. 添加适当的索引
3. 分表技术（水平分割、垂直分割）
4. 存储过程（数据库三层结构）
5. mysql配置的优化（最大并法术。缓存大小myini）
6. mysql硬件升级
7. 定时清除不需要的数据。定时警醒碎片整理

# 8.myisAM和Innodb区别

- myISAM 和 innodb底层实现都是B+树，innodb是mysql默认的存储引擎
  区别：
  1.innodb支持事务并且提供了外键约束，MYISAM不支持事务并且没有外键约束
  2.innodb是聚簇索引，使用B+树的索引结构，数据文件和（主键）索引文件绑在一起，因此要求表必须有主键，通过主键索引的查询效率很高，但是通过辅助索引需要查询两次，先检索辅助索引查询到主键，第二次根据主键查询到数据；主键索引的叶子节点存储的就是数据文件，辅助索引的叶子节点存储的是主键的值。因此主键不应该过大，主键太大，其他索引也都会很大；
  myisam是非聚簇索引，使用B+树的索引结构，数据文件和索引文件分离，索引保存的是指向数据文件的指针，主键索引和辅助索引保存的都是数据文件的指针
  总结：
  myISAM是非聚簇索引；data域存放的是数据的地址，数据文件和索引文件在内存中是分开的，索引文件保存数据文件的地址；主索引key唯一，辅索引key可以重复
  inndb是聚簇索引；data域存放的是完整的数据记录，只有数据文件；主索引key是表的主键，要求表必须有主键，根据主键创建索引，而myisam没有此要求

### 聚簇索引的优点

1.当你需要取出一定范围内的数据时，用聚簇索引也比用非聚簇索引好。

2.当通过聚簇索引查找目标数据时理论上比非聚簇索引要快，因为非聚簇索引定位到对应主键时还要多一次目标记录寻址,即多一次I/O。

3.使用覆盖索引扫描的查询可以直接使用页节点中的主键值。

### 聚簇索引的缺点

1.**插入速度严重依赖于插入顺序**，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于InnoDB表，我们一般都会定义一个自增的ID列为主键。

2.**更新主键的代价很高，因为将会导致被更新的行移动**。因此，对于InnoDB表，我们一般定义主键为不可更新。

3.**二级索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据。**

二级索引的叶节点存储的是主键值，而不是行指针（非聚簇索引存储的是指针或者说是地址），这是为了减少当出现行移动或数据页分裂时二级索引的维护工作，但会让二级索引占用更多的空间。

4.**采用聚簇索引插入新值比采用非聚簇索引插入新值的速度要慢很多**，因为插入要保证主键不能重复，判断主键不能重复，采用的方式在不同的索引下面会有很大的性能差距，聚簇索引遍历所有的叶子节点，非聚簇索引也判断所有的叶子节点，但是聚簇索引的叶子节点除了带有主键还有记录值，记录的大小往往比主键要大的多。这样就会导致聚簇索引在判定新记录携带的主键是否重复时进行昂贵的I/O代价。

#### 9.redis定时机制

[https://www.zhangjava.com/%E4%BD%BF%E7%94%A8Redis%E5%AE%8C%E6%88%90%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/](https://www.zhangjava.com/使用Redis完成定时任务/)

如何使用Redis完成定时任务

#### 10.redis是单线程为什么如此高效

https://juejin.cn/post/6844903814500220936

1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O11；

2、数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的；

3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；

4、使用多路I/O复用模型，非阻塞IO；

5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；

#### 11.redis数据类型

https://www.cnblogs.com/lizhenghn/p/5322887.html

#### 12.redis和memcached区别

https://www.cnblogs.com/aspirant/p/8883871.html

**redis 支持复杂的数据结构**

redis 相比 memcached 来说，拥有更多的数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作， redis 会是不错的选择。

**redis 原生支持集群模式**

在 redis3.x 版本中，便能支持 cluster 模式，而 memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。

**性能对比**

由于 redis 只使用单核，而 memcached 可以使用多核，所以平均每一个核上 redis 在存储小数据时比memcached 性能更高。而在 100k 以上的数据中，memcached 性能要高于 redis。虽然 redis 最近也在存储大数据的性能上进行优化，但是比起 memcached，还是稍有逊色。

redis 的线程模型

redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。

#### 13.redis哨兵模式

https://juejin.cn/post/6844903663362637832

#### 14.mysql出现的问题 幻读，脏读

https://juejin.cn/post/6844903665367547918

#### 15.mysql的锁

https://learnku.com/articles/39212?order_by=vote_count&

#### 16.秒杀系统的设计

https://gongfukangee.github.io/2019/06/09/SecondsKill/

https://blog.51cto.com/15009384/2563456

https://jishuin.proginn.com/p/763bfbd2c789

http://www.soolco.com/post/50235_1_1.html

## 秒杀场景的特点

秒杀场景是电商网站定期举办的活动，这个活动有明确的开始和结束时间，而且参与互动的商品是事先定义好了，参与秒杀商品的个数也是有限制的。同时会提供一个秒杀的入口，让用户通过这个入口进行抢购。

总结一下秒杀场景的特点：

- 定时开始，秒杀时大量用户会在同一时间，抢购同一商品，网站瞬时流量激增。
- 库存有限，秒杀下单数量远远大于库存数量，只有少部分用户能够秒杀成功。
- 操作可靠，秒杀业务流程比较简单，一般就是下订单减库存。库存就是用户争夺的“资源”，实际被消费的“资源”不能超过计划要售出的“资源”，也就是不能被“超卖”。

## 系统隔离的设计思路

在分析秒杀的特点后，我们发现秒杀活动是有计划的，并且在短时间内会爆发大量的请求。为了不影响现有的业务系统的正常运行，我们需要把它和现有的系统做隔离。

即使秒杀活动出现问题也不会影响现有的系统。隔离的设计思路可以从三个维度来思考。

- 业务隔离
- 技术隔离
- 数据库隔离

## 业务隔离

既然秒杀是一场活动，那它一定和常规的业务不同，我们可以把它当成一个单独的项目来看。在活动开始之前，最好设计一个“热场”。

“热场”的形式多种多样，例如：分享活动领优惠券，领秒杀名额等等。“热场”的形式不重要，重要的是通过它获取一些准备信息。

例如：有可能参与的用户数，他们的地域分布，他们感兴趣的商品。为后面的技术架构提供数据支持。

## 技术隔离



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



技术隔离架构图

前面有了准备工作，那么从技术上需要有以下几个方面的考虑：

- 客户端，前端秒杀页面使用专门的页面，这些页面包括静态的 HTML 和动态的 JS，他们都需要在 CDN 上缓存。
- 接入层，加入过滤器专门处理秒杀请求，即使我们扩展再多的应用，使用再多的应用服务器，部署再多的负载均衡器，都会遇到支撑不住海量请求的时候。
- 所以，在这一层我们要考虑的是如何做好限流，当超过系统承受范围的时候，需要果断阻止请求的涌入。
- 应用层，瞬时的海量请求好比请求的“高峰”，我们架构系统的目的就是“削峰”。
- 需要使用服务集群和水平扩展，让“高峰”请求分流到不同的服务器进行处理。同时，还会利用缓存和队列技术减轻应用处理的压力，通过异步请求的方式做到最终一致性。
- 由于是多线程操作，而且商品的额度有限，为了解决超卖的问题，需要考虑进程锁的问题。

## 数据库隔离

秒杀活动持续时间短，瞬时数据量大。为了不影响现有数据库的正常业务，可以建立新的库或者表来处理。

在秒杀结束以后，需要把这部分数据同步到主业务系统中，或者查询表中。如果数据量特别巨大，到千万级别甚至上亿，建议使用分表或者分库。

## 客户端设计

上面提到的三个隔离维度中，我们对技术维度是最为关心的。如果说浏览器/客户端是用户接触“秒杀系统”的入口，那么在这一层提供缓存数据就是非常必要的。

在设计之初，我们会为秒杀的商品生成专门的商品页面和订单页面。这些页面以静态的 HTML 为主，包括的动态信息尽量少。

从业务的角度来说，这些商品的信息早就被用户熟识了，在秒杀的时候，他们关心的是如何快速下单。

既然商品的详情页面和订单页面都是静态生成的，那么就需要定义一个 URL，当要开始秒杀之前，开放这个 URL 给用户访问。

为了防止“程序员或者内部人员”作弊，这里的地址可以通过时间戳和 Hash 算法来生成，也就是说这个地址只有系统知道，到了快秒杀之前才由系统发放出去。

有人说浏览器/客户端如果存放的都是静态页面，那么“控制开始下单”的按钮，以及发送“下单请求”的按钮，也是静态的吗？

答案是否定的，其实静态页面是方便客户端好缓存，下单的动作以及下单时间的控制还是在服务器端。

只不过是通过 JS 文件的方式发送给客户端，在快要秒杀之前，会把这部分 JS 下载到客户端。

因为，其业务逻辑很少，基本只包括时间，用户信息，商品信息等等。所以，其对网络的要求不高。

同时，在网络设计上，我们也会将 JS 和 HTML 同时缓存在 CDN 上面，让用户从离自己最近的 CDN 服务器上获取这些信息。

为了避免秒杀程序参与秒杀，在客户端也会设计一些问答或者滑块的功能，减少此类机器人对服务器的压力。



![img](https://static001.infoq.cn/resource/image/c6/ee/c688448939f17b3c1375d80f048e10ee.png)

**img**



秒杀系统前端设计简图

## 代理层设计

说完了秒杀系统的前端设计，请求自然地来到了代理层。由于用户的请求量大，我们需要用负载均衡加上服务器集群，来面对如此空前的压力。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



代理层三大功能简图

在这一层是可以做缓存，过滤和限流的：

- 缓存，以 Nginx 为例，它可以缓存用户的信息。假设用户信息的修改没有那么频繁，即使有类似的修改也可以通过更新服务来刷新。总比从服务器上获取效率要高得多。
- 过滤，既然缓存了用户信息，这里就可以过滤掉一些不满足条件的用户。注意，这里的用户信息的过滤和缓存只是一个例子。
- 主要想表达的意思是，可以将一些变化不频繁的数据，提到代理层来缓存，提高响应的效率。
- 同时，还可以根据风控系统返回的信息，过滤一些疑似机器人或者恶意请求。例如：从固定 IP 过来的，频率过高的请求。最重要的就是在这一层，可以识别来自秒杀系统的请求。
- 如果是带有秒杀系统的参数，就要把请求路由到秒杀系统的服务器集群。这样才能和正常的业务系统分割开来。
- 限流，每个服务器集群能够承受的压力都是有限的。代理层可以根据服务器集群能够承受的最大压力，设置流量的阀值。
- 阀值的设置可以是动态调整的。例如：集群服务器中有 10 个服务器，其中一台由于压力过大挂掉了。
- 此时就需要调整代理层的流量阀值，将能够处理的请求流量减少，保护后端的应用服务器。
- 当服务器恢复以后，又可以将阀值调回原位。可以通过 Nginx+Lua 合作完成，Lua 从服务注册中心读取服务健康状态，动态调整流量。

## 应用层设计

“秒杀系统”秒杀的是什么？无非是商品。对于系统来说就是商品的库存，购买的商品一旦超过了库存就不能再卖了。

**防止超卖**

超过了库存还可以卖给用户，这就是“超卖”，也是系统设计需要避免的。为了承受大流量的访问，我们用了水平扩展的服务，但是对于他们消费的资源“库存”来说，却只有一个。

为了提高效率，会将这个库存信息放到缓存中。以流行的 Redis 为例，用它存放库存信息，由多个线程来访问就会出现资源争夺的情况。也就是分布式程序争夺唯一资源，为了解决这个问题我们需要实现分布式锁。

假设这里有多个应用响应用户的订单请求，他们同时会去访问 Redis 中存放的库存信息，每接受用户一次请求，都会从 Redis 的库存中减去 1 个商品库存量。

当任何一个进程访问 Redis 中的库存资源时，其他进程是不能访问的，所以这里需要考虑锁的情况（乐观，悲观）。



![img](https://static001.infoq.cn/resource/image/4a/83/4ad1b5f0cbb9cd50b416d8268b082c83.png)

**img**



Redis 缓存承载库存变量

如果锁长期没有释放，需要考虑锁的过期时间，需要设置两个超时时间：

- 资源本身的超时时间，一旦资源被使用一段时间还没有被释放，Redis 会自动释放掉该资源给其他服务使用。
- 服务获取资源的超时时间，一旦一个服务获取资源一段时间后，不管该服务是否处理完这个资源，都需要释放该资源给其他服务使用。

**订单处理流程**

这里的“扣减服务”完成了最简单的扣减库存工作，并没有和其他项目服务打交道，更没有访问数据库。



![img](https://static001.infoq.cn/resource/image/7b/47/7b2d9d0c63bfc24f00dd0eb44a686a47.jpg)

**img**



订单流程示意图

后面的流程相对比较复杂，我们先看图，根据图示来讲解：

- 首先，扣减服务作为下单流程的入口，会先对商品的库存做扣减。同样它会检查商品是否还有库存？
- 由于订单对应的操作步骤比较多，为了让流量变得平滑，这里使用队列存放每个订单请求，等待订单处理服务完成具体业务。
- 订单处理服务实现多线程，或者水平扩展的服务阵列，它们不断监听队列中的消息。一旦发现有新订单请求，就取出订单进行后续处理。
- 注意，这里可以加入类似 ZooKeeper 这样的服务调度来帮助，协调服务调度和任务分配。
- 订单处理服务，处理完订单以后会把结果写到数据库。写数据库是 IO 操作，耗时长。
- 所以，在写数据库的同时，会把结果先写入缓存中，这样用户是可以第一时间查询自己是否下单成功了。
- 结果写入数据库，这个操作有可能成功也有可能失败。
- 为了保证数据的最终一致性，我们用订单结果同步的服务不断的对比，缓存和数据库中的订单结果信息。
- 一旦发现不一致，会去做重试操作。如果重试依旧不成功，会重写信息到缓存，让用户知道失败原因。
- 用户下单以后，焦虑地刷新页面查看下单的结果，实际上是读到的缓存上的下单结果信息。
- 虽然，这个信息和最终结果有偏差，但是在秒杀的场景，要求高性能是前提，结果的一致性，可以后期补偿。

## 数据库设计

讲完了秒杀的处理流程，来谈谈数据库设计要注意的点。

## 数据估算

前面说了秒杀场景需要注意隔离，这里的隔离包括“业务隔离”。就是说我们在秒杀之前，需要通过业务的手段，例如：热场活动，问卷调查，历史数据分析。通过他们去估算这次秒杀可能需要存储的数据量。

这里有两部分的数据需要考虑：

- 业务数据
- 日志数据

前者不言而喻是给业务系统用的。后者，是用来分析和后续处理问题订单用的，秒杀完毕以后还可以用来复盘。

**分表分库**

对于这些数据的存放，需要分情况讨论，例如，MySQL 单表推荐的存储量是 500W 条记录（经验数字）。

如果估算的时候超过了这个数据，建议做分表。如果服务的连接数较多，建议进行分库的操作。

**数据隔离**

由于大量的数据操作是插入，有少部分的修改操作。如果使用关系型数据来存储，建议用专门的表来存放，不建议使用业务系统正在使用的表。

这个开头提到了，数据隔离是必须的，一旦秒杀系统挂了，不会影响到正常业务系统，这个风险意识要有。表的设计除了 ID 以外，最好不要设置其他主键，保证能够快速地插入。

**数据合并**

由于是用的专用表存储，在秒杀活动完毕以后，需要将其和现有的数据做合并。其实，交易已经完成，合并的目的也就是查询。

这个合并需要根据具体情况来分析，如果对于那些“只读”的数据，对于做了读写分离的公司，可以导入到专门负责读的数据库或者 NoSQL 数据库中。

## 压力测试

构建了秒杀系统，一定会面临上线，那么在上线之前压力测试是必不可少的。

我们做压力测试的目的是检验系统崩溃的边缘在哪里？系统的极限在哪里？

这样才能合理地设置流量的上限，为了保证系统的稳定性，多余的流量需要被抛弃。

**压力测试的方法**

合理的测试方法可以帮助我们对系统有深入的了解，这里介绍两种压力测试的方法：

- 正压力测试
- 负压力测试

正压力测试。每次秒杀活动都会计划，使用多少服务器资源，承受多少的请求量。

可以在这个请求量上面不断加压，直到系统接近崩溃或者真正崩溃。简单的说就是做加法。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



正压力测试示意图

负压力测试。在系统正常运行的情况下，逐步减少支撑系统的资源（服务器），看什么时候系统无法支撑正常的业务请求。

例如：在系统正常运行的情况下，逐步减少服务器或者微服务的数量，观察业务请求的情况。说白了就是做减法。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



负压力测试示意图

**压力测试的步骤**



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



测试步骤

有了测试方法的加持，我们来看看需要遵循哪些测试步骤。下面的操作偏套路化，大家在其他系统的压力测试也可以这么做，给大家做个参考。

- 第一，确定测试目标。与性能测试不同的是，压力测试的目标是，什么时候系统会接近崩溃。比如：需要支撑 500W 访问量。
- 第二，确定关键功能。压力测试其实是有重点的，根据 2/8 原则，系统中 20% 的功能被使用的是最多的，我们可以针对这些核心功能进行压力测试。例如：下单，库存扣减。关注核心服务
- 第三，确定负载。这个和关键服务的思路一致，不是每个服务都有高负载的，我们的测试其实是要关注那些负载量大的服务，或者是一段时间内系统中某些服务的负载有波动。这些都是测试目标。
- 第四，选择环境，建议搭建和生产环境一模一样的环境进行测试。
- 第五，确定监视点，实际上就是对关注的参数进行监视，例如 CPU 负载，内存使用率，系统吞吐量等等。
- 第六，产生负载，这里需要从生产环境去获取一些真实的数据作为负载数据源，这部分数据源根据目标系统的承受要求由脚本驱动，对系统进行冲击。建议使用往期秒杀系统的数据，或者实际生产系统的数据进行测试。
- 第七，执行测试，这里主要是根据目标系统，关键组件，用负载进行测试，返回监视点的数据。建议团队可以对测试定一个计划，模拟不同的网络环境，硬件条件进行有规律的测试。
- 第八，分析数据，针对测试的目的，对关键服务的压力测试数据进行分析得知该服务的承受上限在哪里。

一段时间内有负载波动或者大负载的服务进行数据分析，得出服务改造的方向。

## 17.乐观锁和悲观锁

### 悲观锁和乐观锁

- 悲观锁

正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。

在悲观锁的情况下，为了保证事务的隔离性，就需要一致性锁定读。读取数据时给加锁，其它事务无法修改这些数据。修改删除数据时也要加锁，其它事务无法读取这些数据。

- 乐观锁

相对悲观锁而言，乐观锁机制采取了更加宽松的加锁机制。悲观锁大多数情况下依靠数据库的锁机制实现，以保证操作最大程度的独占性。但随之而来的就是数据库性能的大量开销，特别是对长事务而言，这样的开销往往无法承受。

而乐观锁机制在一定程度上解决了这个问题。乐观锁，大多是基于数据版本（ Version ）记录机制实现。何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库表增加一个 “version” 字段来实现。读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。

要说明的是，MVCC的实现没有固定的规范，每个数据库都会有不同的实现方式，这里讨论的是InnoDB的MVCC。

#### 18.Redis的作用

##### 1、最新列表

例如新闻列表页面最新的新闻列表，如果总数量很大的情况下，尽量不要使用select a from A limit 10，尝试redis的 LPUSH命令构建List，一个个顺序都塞进去就可以啦。不过万一内存清掉了咋办？也简单，查询不到存储key的话，用mysql查询并且初始化一个List到redis中就好了。

##### 2、排行榜应用

实现这个功能主要用到的redis数据类型是redis的有序集合zset。zset 是set 类型的一个扩展，比原有的类型多了一个顺序属性，此属性在每次插入数据时会自动调整顺序值,保证value值按照一定顺序连续排列。

我们假设是一个游戏经验值排行榜，那主要的实现思路是：

- 1、在一个新的玩家参与到游戏中时，在redis中的zset中新增一条记录（记录内容看具体的需求）score为0
- 2、当玩家的经验值发生变化时，修改该玩家的score值
- 3、使用redis的ZREVRANGE方法获取排行榜

##### 3、计数器应用

Redis的命令都是原子性的，你可以轻松地利用INCR、DECR命令进行原子性操作，来构建计数系统。由于单线程，可以避免并发问题，保证不会出错，而且100%毫秒级性能。

比如在一个 web 应用程序中，如果想知道用户在一年中每天的点击量，那么只要将用户 ID 以及相关的日期信息作为键，并在每次用户点击页面时，执行一次自增操作即可。

##### 4、数据排重

Redis set是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口。

实现方案：

set 的内部实现是一个 value永远为null的HashMap，实际就是通过计算hash的方式来快速排重的，这也是set能提供判断一个成员是否在集合内的原因。

##### 5、实时的反垃圾系统

反垃圾系统通常都是基于关键词的，使用Redis储存关系词，能够利用Redis的高性能，为监控系统提供稳定及精确的实时监控功能，典型的案例如，邮件系统、评论系统等。

##### 6、可以发布、订阅的实时消息系统

Redis中Pub/Sub系统可以构建实时的消息系统，比如，很多使用Pub/Sub构建的实时聊天应用。

设计思路：

1. 服务端发送消息（含标题，内容），标题按照一定规则存入redis，同时标题（以最少的信息量）推送到客户端，客户点击标题时，获取相应的内容阅读.
2. 如果未读取，可以提示多少条未读，redis能够很快记数
3. 根据一定时间清理缓存

技术实现：

- 需要redis数据库，客户端websocket，服务器端websocket

##### 7、队列应用

队列在现在程序中应用十分广泛，比如日志推送、任务处理等等。以往通常使用http sqs实现队列，其实，使用redis的list类型，也可以实现队列。

## 19.explain每个字段的含义说明：

| id             | 含义                       |
| :------------- | :------------------------- |
| select_type    | 查询类型                   |
| id             | 查询序号                   |
| table          | 表名                       |
| partitions     | 匹配的分区                 |
| type           | join类型                   |
| prossible_keys | 可能会选择的索引           |
| key            | 实际选择的索引             |
| key_len        | 索引的长度                 |
| ref            | 与索引作比较的列           |
| rows           | 要检索的行数估算值估算值   |
| filtered       | 查询条件过滤的行数的百分比 |
| Extra          | 额外信息                   |

### type

连接类型，有如下几种取值，**性能从好到坏排序** 如下：

- system：该表只有一行（相当于系统表），system是const类型的特例
- const：针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可
- eq_ref：当使用了索引的全部组成部分，并且索引是PRIMARY KEY或UNIQUE NOT NULL 才会使用该类型，性能仅次于system及const。
- ref：当满足索引的最左前缀规则，或者索引不是主键也不是唯一索引时才会发生。如果使用的索引只会匹配到少量的行，性能也是不错的。
- fulltext：全文索引
- ref_or_null：该类型类似于ref，但是MySQL会额外搜索哪些行包含了NULL。这种类型常见于解析子查询
- index_merge：此类型表示使用了索引合并优化，表示一个查询里面用到了多个索引
- unique_subquery：该类型和eq_ref类似，但是使用了IN查询，且子查询是主键或者唯一索引
- index_subquery：和unique_subquery类似，只是子查询使用的是非唯一索引
- range：范围扫描，表示检索了指定范围的行，主要用于有限制的索引扫描。比较常见的范围扫描是带有BETWEEN子句或WHERE子句里有>、>=、<、<=、IS NULL、<=>、BETWEEN、LIKE、IN等操作符。
- index：全索引扫描，和ALL类似，只不过index是全盘扫描了索引的数据。当查询仅使用索引中的一部分列时，可使用此类型。有两种场景会触发：
  - 如果索引是查询的覆盖索引，并且索引查询的数据就可以满足查询中所需的所有数据，则只扫描索引树。此时，explain的Extra 列的结果是Using index。index通常比ALL快，因为索引的大小通常小于表数据。
  - 按索引的顺序来查找数据行，执行了全表扫描。此时，explain的Extra列的结果不会出现Uses index。
- ALL：全表扫描，性能最差。

## 20.索引类型：

##### B+TREE索引

在MyISAM中以前缀压缩技术使得索引占用更小并通过数据的物理位置引用被索引的行，而在InnoDB中则按原格式存储并根据主键引用被索引的行。

##### HASH索引：

是Memory引擎的默认索引类型，也是Memory引擎速度快的原因之一。

InnoDB有一个特殊的功能叫做自适应哈希索引，当它发现某些索引值被使用的非常频繁时，它会在内存中基于B+树索引之上再创建一个hash索引，加快数据的查找速度。

##### 聚簇索引：

当表有聚簇索引时，它的数据行实际上存放在索引的叶子页中，也就是 B+Tree的叶子节点上。因为数据行不能存在两个地方，所以一个表只能有一个聚簇索引。

InnoDB中聚簇索引就是表，索引和数据聚簇在了一起。

MyISAM中聚簇索引只是索引，数据行存储数据，索引叶子节点上只有指向数据的指针。

##### 非聚簇索引：

非聚簇索引也是B-Tree数据结构，它的叶子节点包含了引用行的主键列。这意味着通过非聚簇索引查找行，存储引擎需要找到非聚簇索引的叶子节点获得对应的主键，然后根据这个主键值去聚簇索引中查找到对应的行。这里做了至少两次的 BTree 查找。

##### 覆盖索引：

每一个索引在 InnoDB 里面对应一棵B+树，那么此时就存着两棵B+树。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



可以发现区别在与叶子节点中，主键索引存储了整行数据，而非主键索引中存储的值为主键id, 在我们执行如下sql后

```
SELECT age FROM student WHERE name = '小李'；
复制代码
```

流程为：

1. 在name索引树上找到名称为小李的节点 id为03
2. 从id索引树上找到id为03的节点 获取所有数据
3. 从数据中获取字段命为age的值返回 12

**在流程中从非主键索引树搜索回到主键索引树搜索的过程称为：回表**，在本次查询中因为查询结果只存在主键索引树中，我们必须回表才能查询到结果，那么如何优化这个过程呢？引入正文覆盖索引

一个索引包含了所有需要查询的字段的**值**，就称为覆盖索引。

覆盖索引可以直接获取列的数据，而不用在读取数据行，所以效率比较高。

#### 21.事务的特性在MySQL中是如何实现的

##### 原子性的实现

###### MySQL数据库事务的原子性是通过undo log实现的。

事务的所有修改操作增、删、改增、删、改的相反操作都会写入undo log,比如事务执行了一条insert语句，那么undo log就会记录一条相应的delete语句。所以undo log是一个逻辑文件，记录的是相应的SQL语句一旦由于故障，导致事务无法成功提交，系统则会执行undo log中相应的撤销操作，达到事务回滚的目的。

undo log也可以实现MVCC多个版本并发控制多个版本并发控制，下文的隔离性的实现将会详细介绍

##### 持久性的实现

###### MySQl数据库事务的持久性是通过redo log实现的。

事务的所有修改操作增、删、改增、删、改，数据库都会生成一条redo日志记录到redo log.区别于undo log记录SQL语句、redo log记录的是事务对数据库的哪个数据页做了什么修改，属于物理日志。

redo日志应用场景：数据库系统直接崩溃，需要进行恢复，一般数据库都会使用按时间点备份的策略，首先将数据库恢复到最近备份的时间点状态，之后读取该时间点之后的redo log记录，重新执行相应记录，达到最终恢复的目的。

##### 隔离性的实现：通过隔离级别

##### 一致性的实现：实现AID即可实现

#### 22.存储引擎的对比

##### 1.InnoDB：

支持事务处理，支持外键，支持崩溃修复能力和并发控制。如果需要对事务的完整性要求比较高（比如银行），要求实现并发控制（比如售票），那选择InnoDB有很大的优势。如果需要频繁的更新、删除操作的数据库，也可以选择InnoDB，因为支持事务的提交（commit）和回滚（rollback）。

##### 2.MyISAM：

插入数据快，空间和内存使用比较低。如果表主要是用于插入新记录和读出记录，那么选择MyISAM能实现处理高效率。如果应用的完整性、并发性要求比 较低，也可以使用。如果数据表主要用来插入和查询记录，则MyISAM引擎能提供较高的处理效率

##### 3.MEMORY：

所有的数据都在内存中，数据的处理速度快，但是安全性不高。如果需要很快的读写速度，对数据的安全性要求较低，可以选择MEMOEY。它对表的大小有要求，不能建立太大的表。所以，这类数据库只使用在相对较小的数据库表。如果只是临时存放数据，数据量不大，并且不需要较高的数据安全性，可以选择将数据保存在内存中的Memory引擎，MySQL中使用该引擎作为临时表，存放查询的中间结果

##### 5.Archive

如果只有INSERT和SELECT操作，可以选择Archive，Archive支持高并发的插入操作，但是本身不是事务安全的。Archive非常适合存储归档数据，如记录日志信息可以使用Archiv

#### 23.数据库连接池

第一、连接池的建立。一般在系统初始化时，连接池会根据系统配置建立，并在池中创建了几个连接对象，以便使用时能从连接池中获取。连接池中的连接不能随意创建和关闭，这样避免了连接随意建立和关闭造成的系统开销。Java中提供了很多容器类可以方便的构建连接池，例如Vector、Stack等。

第二、连接池的管理。连接池管理策略是连接池机制的核心，连接池内连接的分配和释放对系统的性能有很大的影响。其管理策略是：
当客户请求数据库连接时，首先查看连接池中是否有空闲连接，如果存在空闲连接，则将连接分配给客户使用；如果没有空闲连接，则查看当前所开的连接数是否已经达到最大连接数，如果没达到就重新创建一个连接给请求的客户；如果达到就按设定的最大等待时间进行等待，如果超出最大等待时间，则抛出异常给客户。
当客户释放数据库连接时，先判断该连接的引用次数是否超过了规定值，如果超过就从连接池中删除该连接，否则保留为其他客户服务。
该策略保证了数据库连接的有效复用，避免频繁的建立、释放连接所带来的系统资源开销。

第三、连接池的关闭。当应用程序退出时，关闭连接池中所有的连接，释放连接池相关的资源，该过程正好与创建相反。

## 连接池主要参数

**使用连接池时，要配置一下参数**

1. 最小连接数：是连接池一直保持的数据库连接,所以如果应用程序对数据库连接的使用量不大,将会有大量的数据库连接资源被浪费.
2. 最大连接数：是连接池能申请的最大连接数,如果数据库连接请求超过次数,后面的数据库连接请求将被加入到等待队列中,这会影响以后的数据库操作
3. 最大空闲时间
4. 获取连接超时时间
5. 超时重试连接次数

## 连接池的配置与维护

连接池中到底应该放置多少连接，才能使系统的性能最佳？

系统可采取设置最小连接数（minConnection）和最大连接数（maxConnection）等参数来控制连接池中的连接。比方说，最小连接数是系统启动时连接池所创建的连接数。如果创建过多，则系统启动就慢，但创建后系统的响应速度会很快；如果创建过少，则系统启动的很快，响应起来却慢。这样，可以在开发时，设置较小的最小连接数，开发起来会快，而在系统实际使用时设置较大的，因为这样对访问客户来说速度会快些。

最大连接数是连接池中允许连接的最大数目，具体设置多少，要看系统的访问量，可通过软件需求上得到。 如何确保连接池中的最小连接数呢？有动态和静态两种策略。动态即每隔一定时间就对连接池进行检测，如果发现连接数量大于最小连接数，则补充相应数量的新连接,以保证连接池的正常运转。静态是发现空闲连接不够时再去检查。

## 24.MySQL联合索引

联合索引又叫复合索引，是MySQL的InnoDB引擎中的一个索引方式，如果一个系统频繁地使用相同的几个字段查询结果，就可以考虑建立这几个字段的联合索引来提高查询效率。

### 最左前缀原则

在使用联合索引时要注意有个最左前缀原则，最左前缀原则就是要考虑查询的字段的顺序，只有遵守这个原则才能最大地提高查询的效率，下面我们举个例子说明最左前缀原则。

建立 (`a`,`b`,`c`)的联合索引

所谓最左原则指的就是如果你的 SQL 语句中用到了联合索引中的最左边的索引，那么这条 SQL 语句就可以利用这个联合索引去进行匹配，值得注意的是，当遇到范围查询>、<、between、like>、<、between、like就会停止匹配。

假设，我们对a,ba,b字段建立一个索引，也就是说，你where后条件为

```
a = 1
a = 1 and b = 2
```

是可以匹配索引的。但是要注意的是~你执行

```
b= 2 and a =1
```

也是能匹配到索引的，因为Mysql有优化器会自动调整a,b的顺序与索引顺序一致。
相反的，你执行

```
b = 2
```

就匹配不到索引了。
而你对a,b,c,da,b,c,d建立索引,where后条件为

```
a = 1 and b = 2 and c > 3 and d = 4 
```

那么，a,b,c三个字段能用到索引，而d就匹配不到。因为遇到了范围查询！

#### **最左匹配的原理？**

假设，我们对a,ba,b字段建立索引，那么入下图所示

![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



如图所示他们是按照a来进行排序，在a相等的情况下，才按b来排序。

因此，我们可以看到a是有序的1，1，2，2，3，3。而b是一种全局无序，局部相对有序状态!
*什么意思呢？*
从全局来看，b的值为1，2，1，4，1，2，是无序的，因此直接执行`b = 2`这种查询条件没有办法利用索引。

从局部来看，当a的值确定的时候，b是有序的。例如a = 1时，b值为1，2是有序的状态。当a=2时候，b的值为1,4也是有序状态。
因此，你执行`a = 1 and b = 2`是a,b字段能用到索引的。而你执行`a > 1 and b = 2`时，a字段能用到索引，b字段用不到索引。因为a的值此时是一个范围，不是固定的，在这个范围内b值不是有序的，因此b字段用不上索引。

综上所示，最左匹配原则，在遇到范围查询的时候，就会停止匹配。

### 联合索引提高查询效率的原理

MySQL会为InnoDB的每个表建立聚簇索引，如果表有索引会建立二级索引。聚簇索引以主键建立索引，如果没有主键以表中的唯一键建立，唯一键也没会以隐式的创建一个自增的列来建立。聚簇索引和二级索引都是一个b+树，b+树的特点是数据按一定顺序存在叶子节点且每页数据相连。一般情况下使用索引查询时，先查询二级索引的b+树，查到数据并拿数据中保存的主键回查聚簇索引查到所有数据。下面我们举个例子来重现这个过程。

nnoDB建立的聚簇索引和二级索引如下图

###### 聚簇索引



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



###### 二级索引



![img](https://oscimg.oschina.net/oscnet/869bc4c4356f87c47db870d6c8d0d0897eb.jpg)

**img**



假如我们想要查找名字为zhaoliu，年龄为30的人的信息。即name=’zhaoliu’,age=30

- （1）先查二级索引，先用二分法查找发现在wangwu名字的右边
- （2）读取右边的这页的数据到内存，二分法查到数据2个name为zhaoliu人。
- （3）继续二分法比较age查到数据id=31
- （4）id=31回查聚簇索引先用二分法查找发现在31右边
- （5）读取31左边这页数据到内存，二分法查到数据并返回数据

如果你仅仅查找id，name和age数据那么这样就用到了覆盖索引，这样就不用回查聚簇索引，在第（3）步直接返回数据即可。

### 题目：

如果sql为

```
SELECT * FROM table WHERE a = 1 and b = 2 and c = 3; 
```

如何建立索引?
如果此题回答为对a,b,ca,b,c建立索引，那都可以回去等通知了。
此题正确答法是，a,b,ca,b,c或者c,b,ac,b,a或者b,a,cb,a,c都可以，重点要的是将**区分度高的字段放在前面，区分度低的字段放后面**。像性别、状态这种字段区分度就很低，我们一般放后面。

例如假设区分度由大到小为b,a,c。那么我们就对b,a,cb,a,c建立索引。在执行sql的时候，优化器会 帮我们调整where后a,b,c的顺序，让我们用上索引。

#### 题型二

如果sql为

```
SELECT * FROM table WHERE a > 1 and b = 2; 
```

如何建立索引?
如果此题回答为对a,ba,b建立索引，那都可以回去等通知了。
此题正确答法是，对b,ab,a建立索引。如果你建立的是a,ba,b索引，那么只有a字段能用得上索引，毕竟最左匹配原则遇到范围查询就停止匹配。
如果对b,ab,a建立索引那么两个字段都能用上，优化器会帮我们调整where后a,b的顺序，让我们用上索引。

#### 题型三

如果sql为

```
SELECT * FROM `table` WHERE a > 1 and b = 2 and c > 3; 
```

如何建立索引?
此题回答也是不一定，b,ab,a或者b,cb,c都可以，要结合具体情况具体分析。

拓展一下

```
SELECT * FROM `table` WHERE a = 1 and b = 2 and c > 3; 
```

怎么建索引？嗯，大家一定都懂了！

#### 题型四

```
SELECT * FROM `table` WHERE a = 1 ORDER BY b;
```

如何建立索引？
这还需要想？一看就是对a,ba,b建索引，当a = 1的时候，b相对有序，可以避免再次排序！
那么

```
SELECT * FROM `table` WHERE a > 1 ORDER BY b; 
```

如何建立索引？
对aa建立索引，因为a的值是一个范围，这个范围内b值是无序的，没有必要对a,ba,b建立索引。

拓展一下

```
SELECT * FROM `table` WHERE a = 1 AND b = 2 AND c > 3 ORDER BY c;
```

怎么建索引?

#### 题型五

```
SELECT * FROM `table` WHERE a IN (1,2,3) and b > 1; 
```

如何建立索引？
还是对a，ba，b建立索引，因为IN在这里可以视为等值引用，不会中止索引匹配，所以还是a,ba,b!

拓展一下

```
SELECT * FROM `table` WHERE a = 1 AND b IN (1,2,3) AND c > 3 ORDER BY c;
```

如何建立索引？此时c排序是用不到索引的。

## 25.SQL语句如何被解析的

SQL解析与优化是属于编译器范畴，和C等其他语言的解析没有本质的区别。其中分为，词法分析、语法和语义分析、优化、执行代码生成。对应到MySQL的部分，如下图



![图1 SQL解析原理](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**图1 SQL解析原理**



### 词法分析

SQL解析由词法分析和语法/语义分析两个部分组成。词法分析主要是把输入转化成一个个Token。其中Token中包含Keyword（也称symbol）和非Keyword。例如，SQL语句 select username from userinfo，在分析之后，会得到4个Token，其中有2个Keyword，分别为select和from

### 语法分析

语法分析就是生成语法树的过程。这是整个解析过程中最精华，最复杂的部分，不过这部分MySQL使用了Bison来完成。即使如此，如何设计合适的数据结构以及相关算法，去存储和遍历所有的信息，也是值得在这里研究的。



![图2 语法树](https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018a/a74c9e9c.png)

**图2 语法树**



## 26.Redis高并发锁机制：

加锁

在Redis中加锁非常简便，直接使用SET命令即可。示例及关键选项说明如下：

```
SET resource_1 random_value NX EX 5
```

| 参数/选项    | 说明                                                         |
| :----------- | :----------------------------------------------------------- |
| resource_1   | 分布式锁的key，只要这个key存在，相应的资源就处于加锁状态，无法被其它客户端访问。 |
| random_value | 一个随机字符串，不同客户端设置的值不能相同。                 |
| EX           | 设置过期时间，单位为秒。您也可以使用PX选项设置单位为毫秒的过期时间。 |
| NX           | 如果需要设置的key在Redis中已存在，则取消设置                 |

## 27.Redis持久化方式

[Redis](http://redis.io/)有两种持久化的方式：快照（`RDB`文件）和追加式文件（`AOF`文件）：

- RDB持久化方式会在一个特定的间隔保存那个时间点的一个数据快照。
- AOF持久化方式则会记录每一个服务器收到的写操作。在服务启动时，这些记录的操作会逐条执行从而重建出原来的数据。写操作命令记录的格式跟Redis协议一致，以追加的方式进行保存。
- Redis的持久化是可以禁用的，就是说你可以让数据的生命周期只存在于服务器的运行时间里。
- 两种方式的持久化是可以同时存在的，但是当Redis重启时，AOF文件会被优先用于重建数据。

### RDB：生成快照：

#### SAVE

[SAVE](http://redis.io/commands/save)命令会使用同步的方式生成RDB快照文件，这意味着在这个过程中会阻塞所有其他客户端的请求。因此不建议在生产环境使用这个命令，除非因为某种原因需要去阻止Redis使用子进程进行后台生成快照（例如调用`fork(2)`出错）。

#### BGSAVE

[BGSAVE](http://redis.io/commands/bgsave)命令使用后台的方式保存RDB文件，调用此命令后，会立刻返回`OK`返回码。Redis会产生一个子进程进行处理并立刻恢复对客户端的服务。在客户端我们可以使用[LASTSAVE](http://redis.io/commands/lastsave)命令查看操作是否成功。

## AOF

快照并不是很可靠。如果你的电脑突然宕机了，或者电源断了，又或者不小心杀掉了进程，那么最新的数据就会丢失。而AOF文件则提供了一种更为可靠的持久化方式。每当Redis接受到会修改数据集的命令时，就会把命令追加到AOF文件里，当你重启Redis时，AOF里的命令会被重新执行一次，重建数据。

#### 如何解决数据不一致问题：

使用子进程也有一个问题需要解决： 因为子进程在进行 AOF 重写期间， 主进程还需要继续处理命令， 而新的命令可能对现有的数据进行修改， 这会让当前数据库的数据和重写后的 AOF 文件中的数据不一致。

为了解决这个问题， Redis 增加了一个 AOF 重写缓存， 这个缓存在 fork 出子进程之后开始启用， Redis 主进程在接到新的写命令之后， 除了会将这个写命令的协议内容追加到现有的 AOF 文件之外， 还会追加到这个缓存中：



![digraph p {      node [style = filled];      edge [style = "bold, dashed"];      //       client [label = "客户端", fillcolor = "#95BBE3"];      server [label = "服务器", fillcolor = "#A8E270"];      client -> server [label = "命令请求"];      current_aof [label = "现有 AOF 文件", shape = box, fillcolor = "#FADCAD"];      aof_rewrite_buf [label = "AOF 重写缓存", shape = box, fillcolor = "#FADCAD"];      server -> current_aof [label = "命令协议内容"];      server -> aof_rewrite_buf [label = "命令协议内容"]; }](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**digraph p { node [style = filled]; edge [style = "bold, dashed"]; // client [label = "客户端", fillcolor = "#95BBE3"]; server [label = "服务器", fillcolor = "#A8E270"]; client -> server [label = "命令请求"]; current_aof [label = "现有 AOF 文件", shape = box, fillcolor = "#FADCAD"]; aof_rewrite_buf [label = "AOF 重写缓存", shape = box, fillcolor = "#FADCAD"]; server -> current_aof [label = "命令协议内容"]; server -> aof_rewrite_buf [label = "命令协议内容"]; }**



换言之， 当子进程在执行 AOF 重写时， 主进程需要执行以下三个工作：

1. 处理命令请求。
2. 将写命令追加到现有的 AOF 文件中。
3. 将写命令追加到 AOF 重写缓存中。

这样一来可以保证：

1. 现有的 AOF 功能会继续执行，即使在 AOF 重写期间发生停机，也不会有任何数据丢失。
2. 所有对数据库进行修改的命令都会被记录到 AOF 重写缓存中。

当子进程完成 AOF 重写之后， 它会向父进程发送一个完成信号， 父进程在接到完成信号之后， 会调用一个信号处理函数， 并完成以下工作：

1. 将 AOF 重写缓存中的内容全部写入到新 AOF 文件中。
2. 对新的 AOF 文件进行改名，覆盖原有的 AOF 文件。

当步骤 1 执行完毕之后， 现有 AOF 文件、新 AOF 文件和数据库三者的状态就完全一致了。

当步骤 2 执行完毕之后， 程序就完成了新旧两个 AOF 文件的交替。

## 28.Redis集群和哨兵模式

### 集群

- 主从复制模式

**Redis 提供了复制（replication）功能，可以实现当一台数据库中的数据更新后，自动将更新的数据同步到其他数据库上**。

在复制的概念中，数据库分为两类，一类是主数据库（master），另一类是从数据库(slave）。主数据库可以进行读写操作，当写操作导致数据变化时会自动将数据同步给从数据库。而从数据库一般是只读的，并接受主数据库同步过来的数据。一个主数据库可以拥有多个从数据库，而一个从数据库只能拥有一个主数据库。

- Sentinel（哨兵）模式

哨兵模式是一种特殊的模式，首先 Redis 提供了哨兵的命令，**哨兵是一个独立的进程，作为进程，它会独立运行。其原理是哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个 Redis 实例**。

- Cluster 模式

Redis 的哨兵模式基本已经可以实现高可用，读写分离 ，但是在这种模式下每台 Redis 服务器都存储相同的数据，很浪费内存，所以在 redis3.0上加入了 Cluster 集群模式，实现了 Redis 的分布式存储，**也就是说每台 Redis 节点上存储不同的内容**。

### 哨兵

哨兵模式是redis`高可用`的实现方式之一
使用一个或者多个哨兵SentinelSentinel实例组成的系统，对redis节点进行监控，在主节点出现故障的情况下，能将从节点中的一个升级为主节点，进行故障转义，保证系统的可用性。

### 心跳检测：

心跳检测发生在命令传播阶段，从节点默认每秒发送一次心跳检测命令 **REPLCONF ACK <偏移量>** 获取主节点心跳。
发送该命令主要为了：检测主从服务器的网络连接状态、辅助实现min-slave选项、检测命令是否丢失。

## 29.Redis缓存作用

**高性能**

假设这么个场景，你有个操作，一个请求过来，吭哧吭哧你各种乱七八糟操作 mysql，半天查出来一个结果，耗时 600ms。但是这个结果可能接下来几个小时都不会变了，或者变了也可以不用立即反馈给用户。那么此时咋办？

缓存啊，折腾 600ms 查出来的结果，扔缓存里，一个 key 对应一个 value，下次再有人查，别走 mysql折腾 600ms 了，直接从缓存里，通过一个 key 查出来一个 value，2ms 搞定。性能提升 300 倍。

就是说对于一些需要复杂操作耗时查出来的结果，且确定后面不怎么变化，但是有很多读请求，那么直接将查询出来的结果放在缓存中，后面直接读缓存就好。

**高并发**

所以要是你有个系统，高峰期一秒钟过来的请求有 1 万，那一个 mysql 单机绝对会死掉。你这个时候就只能上缓存，把很多数据放缓存，别放 mysql。缓存功能简单，说白了就是 key-value 式操作，单机支撑的并发量轻松一秒几万十几万，支撑高并发 so easy。单机承载并发量是 mysql 单机的几十倍。

缓存是走内存的，内存天然就支撑高并发。

## 30.redis过期策略：

redis 过期策略是：定期删除+惰性删除。

所谓定期删除，指的是 redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。

假设 redis 里放了 10w 个 key，都设置了过期时间，你每隔几百毫秒，就检查 10w 个 key，那 redis 基本上就死了，cpu 负载会很高的，消耗在你的检查过期 key 上了。注意，这里可不是每隔 100ms 就遍历所有的设置过期时间的 key，那样就是一场性能上的灾难。实际上 redis 是每隔 100ms 随机抽取一些key 来检查和删除的。

但是问题是，定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是说，在你获取某个 key 的时候，redis 会检查一下 ，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。

获取 key 的时候，如果此时 key 已经过期，就删除，不会返回任何东西。

答案是：走内存淘汰机制。

**内存淘汰机制**

redis 内存淘汰机制有以下几个：

- noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了。
- allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）。
- allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的 key 给干掉啊。
- volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（这个一般不太合适）。
- volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。
- volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。

## 31.如何保证 redis 的高并发和高可用？redis 的主从复制原理能介绍一下么？

#### Redis 主从架构:

单机的 redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。

因此架构做成主从master−slavemaster−slave架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



#### redis 主从复制的核心原理

当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。

如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制。

此时 master 会启动一个后台线程，开始生成一份 RDB 快照文件，同时还会将从客户端 client 新收到的所有写命令缓存在内存中。RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中，接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



------

#### 主从复制的断点续传

从 redis2.8 开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份。

master node 会在内存中维护一个 backlog，master 和 slave 都会保存一个 replica offset 还有一个master run id，offset 就是保存在 backlog 中的。如果 master 和 slave 网络连接断掉了，slave 会让 master 从上次 replica offset 开始继续复制，如果没有找到对应的 offset，那么就会执行一次 resynchronization。

如果根据 host+ip 定位 master node，是不靠谱的，如果 master node 重启或者数据出现了变化，那么 slave node 应该根据不同的 run id 区分。

------

#### 无磁盘化复制

master 在内存中直接创建 RDB，然后发送给 slave，不会在自己本地落地磁盘了。只需要在配置文件中开启 repl-diskless-sync yes 即可。

repl-diskless-sync yes

\# 等待 5s 后再开始复制，因为要等更多 slave 重新连接过来

repl-diskless-sync-delay 5

------

#### 过期 key 处理

slave 不会过期 key，只会等待 master 过期 key。如果 master 过期了一个 key，或者通过 LRU 淘汰了一个 key，那么会模拟一条 del 命令发送给 slave。

------

#### 复制的完整流程

slave node 启动时，会在自己本地保存 master node 的信息，包括 master node 的 host 和 ip，但是复制流程没开始。

slave node 内部有个定时任务，每秒检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络连接。然后 slave node 发送 ping 命令给 master node。如果 master 设置了requirepass，那么 slave node 必须发送 masterauth 的口令过去进行认证。master node 第一次执行全量复制，将所有数据发给 slave node。而在后续，master node 持续将写命令，异步复制给 slave node。

## 32.在集群模式下， redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了 解一致性 hash 算法吗？

**redis cluster 介绍**

- 自动将数据进行分片，每个 master 上放一部分数据
- 提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的

在 redis cluster 架构下，每个 redis 要放开两个端口号，比如一个是 6379，另外一个就是 加 1w 的端口号，比如 16379。

16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议，gossip 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。

**节点间的内部通信机制**

**基本通信原理**

集群元数据的维护有两种方式：集中式、Gossip 协议。redis cluster 节点间采用 gossip 协议进行通信。

集中式是将集群元数据（节点信息、故障等等）几种存储在某个节点上。集中式元数据集中存储的一个典型代表，就是大数据领域的 storm。它是分布式的大数据实时计算引擎，是集中式的元数据存储的结构，底层基于 zookeeper（分布式协调的中间件）对所有元数据进行存储维护。



![img](https://pic3.zhimg.com/80/v2-c06a22979dc0a89968da1498603bc94e_720w.jpg)

**img**



redis 维护集群元数据采用另一个方式， gossip 协议，所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



集中式的好处在于，元数据的读取和更新，时效性非常好，一旦元数据出现了变更，就立即更新到集中式的存储中，其它节点读取的时候就可以感知到；不好在于，所有的元数据的更新压力全部集中在一个地方，可能会导致元数据的存储有压力。

gossip 好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力；不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后。

10000 端口：每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如7001，那么用于节点间通信的就是 17001 端口。每个节点每隔一段时间都会往另外几个节点发送 ping 消息，同时其它几个节点接收到 ping 之后返回 pong。

交换的信息：信息包括故障信息，节点的增加和删除，hash slot 信息等等。

**分布式寻址算法**

- hash 算法（大量缓存重建）
- 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡）
- redis cluster 的 hash slot 算法

**hash 算法**

来了一个 key，首先计算 hash 值，然后对节点数取模。然后打在不同的 master 节点上。一旦某一个master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致大部分的请求过来，全部无法拿到有效的缓存，导致大量的流量涌入数据库。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



**一致性 hash 算法**

一致性 hash 算法将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。这样就能确定每个节点在其哈希环上的位置。

来了一个 key，首先计算 hash 值，并确定此数据在环上的位置，从此位置沿环顺时针**“**行走**”**，遇到的第一个 master 节点就是 key 所在位置。

在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。

燃鹅，一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。为了解决这种热点

问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。

## 33.redis 的并发竞争问题是什么？如何解决这个问题？了解redis 事务的 CAS 方案吗？

### 面试官心理分析

这个也是线上非常常见的一个问题，就是多客户端同时并发写一个 key，可能本来应该先到的数据后到了，导致数据版本错了；或者是多客户端同时获取一个 key，修改值之后再写回去，只要顺序错了，数据就错了。

而且 redis 自己就有天然解决这个问题的 CAS 类的乐观锁方案。

### 面试题剖析

某个时刻，多个系统实例都去更新某个 key。可以基于 zookeeper 实现分布式锁。每个系统通过zookeeper 获取分布式锁，确保同一时间，只能有一个系统实例在操作某个 key，别人都不允许读和写。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



你要写入缓存的数据，都是从 mysql 里查出来的，都得写入 mysql 中，写入 mysql 中的时候必须保存一个时间戳，从 mysql 查出来的时候，时间戳也查出来。

每次要写之前，先判断一下当前这个 value 的时间戳是否比缓存里的 value 的时间戳要新。如果是的话，那么可以写，否则，就不能用旧的数据覆盖新的数据。

## 34.生产环境中的 redis 是怎么部署的？

### 面试官心理分析

看看你了解不了解你们公司的 redis 生产集群的部署架构，如果你不了解，那么确实你就很失职了，你的redis 是主从架构？集群架构？用了哪种集群方案？有没有做高可用保证？有没有开启持久化机制确保可以进行数据恢复？线上 redis 给几个 G 的内存？设置了哪些参数？压测后你们 redis 集群承载多少QPS？

兄弟，这些你必须是门儿清的，否则你确实是没好好思考过。

### 面试题剖析

redis cluster，10 台机器，5 台机器部署了 redis 主实例，另外 5 台机器部署了 redis 的从实例， 每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰 qps 可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求/s。

机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 redis 进程的是 10g 内存，一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。

5 台机器对外提供读写，一共有 50g 内存。

因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，redis 从实例会自动变成主实例继续提供读写服务。

你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。

其实大型的公司，会有基础架构的 team 负责缓存集群的运维。

## 35.如何保证缓存与数据库的双写一致性？

### 面试官心理分析

你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？

### 面试题剖析

一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统不是严格要求“缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：读请求和写请求串行化，串到一个内存队列里去。

串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。

**Cache Aside Pattern**

最经典的缓存+数据库读写的模式，就是 Cache Aside Pattern。 - 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 - 更新的时候，先更新数据库，然后再删除缓存。

## 36.慢查询优化：

当感觉操作数据库查询语句速度变慢，不符合生产效率要求时，可按照以下步骤进行查看
1、 慢查询的开启与捕获，查看可能是哪些SQL语句造成的查询速度慢
2、 explain+SQL语句
3、 show profile分析SQL语句在服务器内执行细节和生命周期情况
4、 通过以上三个步骤大致确定问题SQL之后，可联系运维人员或者DBA进行数据库服务器参数的调整优化

慢查询日志是mysql的一个日志记录，可以用来记录mysql语句执行时间超过指定的long_query_time的SQL语句,long_query_time的默认值是10s

# 37.MySQL的binlog

## 1.概念

- Mysql 5.0以后，支持通过binary log二进制日志二进制日志以支持主从复制。复制允许将来自一个MySQL数据库服务器（master) 的数据复制到一个或多个其他MySQL数据库服务器（slave)，以实现灾难恢复、水平扩展、统计分析、远程数据分发等功能。

  **二进制日志中存储的内容称之为事件，每一个数据库更新操作Insert、Update、Delete，不包括SelectInsert、Update、Delete，不包括Select等都对应一个事件。**

  **注意：本文不是讲解mysql主从复制，而是讲解binlog的应用场景，binlog中包含哪些类型的event，这些event的作用是什么。你可以理解为，是对主从复制中关于binlog解析的细节进行深度剖析。而讲解主从复制主要是为了理解binlog的工作流程。**

  下面以mysql主从复制为例，讲解一个从库是如何从主库拉取binlog，并回放其中的event的完整流程。mysql主从复制的流程如下图所示：

  

  ![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

  **img**

  

  主要分为3个步骤：

  - **第一步：**master在每次准备提交事务完成数据更新前，将改变记录到二进制日志binarylogbinarylog中（这些记录叫做二进制日志事件，binary log event，简称event)
  - **第二步：**slave启动一个I/O线程来读取主库上binary log中的事件，并记录到slave自己的中继日志relaylogrelaylog中。
  - **第三步：**slave还会起动一个SQL线程，该线程从relay log中读取事件并在备库执行，从而实现备库数据的更新。

## 2 binlog的应用场景

binlog本身就像一个螺丝刀，它能发挥什么样的作用，完全取决你怎么使用。就像你可以使用螺丝刀来修电器，也可以用其来固定家具。

### 2.1 读写分离

最典型的场景就是通过Mysql主从之间通过binlog复制来实现横向扩展，来实现读写分离。如下图所示：



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



在这种场景下：

- 有一个主库Master，所有的更新操作都在master上进行
- 同时会有多个Slave，每个Slave都连接到Master上，获取binlog在本地回放，实现数据复制。
- 在应用层面，需要对执行的sql进行判断。所有的更新操作都通过MasterInsert、Update、Delete等Insert、Update、Delete等，而查询操作Select等Select等都在Slave上进行。由于存在多个slave，所以我们可以在slave之间做负载均衡。通常业务都会借助一些数据库中间件，如tddl、sharding-jdbc等来完成读写分离功能。

因为工作性质的原因，笔者见过最多的一个业务，一个master，后面挂了20多个slave。

### 2.2 数据恢复

一些同学可能有误删除数据库记录的经历，或者因为误操作导致数据库存在大量脏数据的情况。例如笔者，曾经因为误操作污染了业务方几十万数据记录。

如何将脏数据恢复成原来的样子？如果恢复已经被删除的记录？

这些都可以通过反解binlog来完成，笔者也是通过这个手段，来恢复业务方的记录。

### 2.3 数据最终一致性

在实际开发中，我们经常会遇到一些需求，在数据库操作成功后，需要进行一些其他操作，如：发送一条消息到MQ中、更新缓存或者更新搜索引擎中的索引等。

**如何保证数据库操作与这些行为的一致性，就成为一个难题**。以数据库与redis缓存的一致性为例：操作数据库成功了，可能会更新redis失败；反之亦然。很难保证二者的完全一致。

**遇到这种看似无解的问题，最好的办法是换一种思路去解决它：**不要同时去更新数据库和其他组件，只是简单的更新数据库即可。

如果数据库操作成功，必然会产生binlog。之后，我们通过一个组件，来模拟的mysql的slave，拉取并解析binlog中的信息。**通过解析binlog的信息，去异步的更新缓存、索引或者发送MQ消息，保证数据库与其他组件中数据的最终一致。**

# 38.为什么InnoDB要用自增主键：

1.InnoDB使用聚集索引，数据记录本身被存于主索引的叶子节点上，这就要求同一个叶子节点内的各条数据记录按主键顺序存放，因此每当一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子，则开辟一个新的页（节点）如果表使用自增主键，那么每次插入新的记录时，记录就会顺序添加到当前索引节点后续位置，当一页写满，就会自动开辟一个新的页。这样就就会形成一个紧凑的索引结构，近似顺序填满，由于每次插入时也不需要移动所有数据，因此效率很高，也不会增加很多额外的开销维护索引。

2.如果使用非自增主键，由于每次插入主键的值近乎于随机，因此每次新纪录都要被插到现有索引页的中间某个位置，此时MySQL不得不为了将新纪录插到合适位置而移动数据，甚至目标页面可能已经被写到磁盘而从缓存中清除，这增加了很多额外开销，同时频繁的移动，分页造成了大量的碎片，得到不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建并优化填充页面。

3.由于MySQL从磁盘读取数据时一块一块来读取的，同时，根据局部性原理，MySQL引擎会选择预读一部分和你当前读数据所在内存相邻的数据块，这个时候这些相邻数据块的数据已经存在于内存中。由于数据库大部分是查询操作，这个时候，如果主键是自增的话，数据存储都是紧凑地存储在一起的，那么对于局部性原理利用和避免过多地I/O操作都有着巨大的促进作用

# 39.MySQL的锁：

## 锁的认识

### 1.1 锁的解释

计算机协调多个进程或线程并发访问某一资源的机制。

### 1.2 锁的重要性

在数据库中，除传统计算资源（CPU、RAM、I\O等）的争抢，数据也是一种供多用户共享的资源。
如何保证数据并发访问的一致性，有效性，是所有数据库必须要解决的问题。
锁冲突也是影响数据库并发访问性能的一个重要因素，因此锁对数据库尤其重要。

### 1.3 锁的缺点

加锁是消耗资源的，锁的各种操作，包括获得锁、检测锁是否已解除、释放锁等 ，都会增加系统的开销。

## 锁的类型

### 行

- 行级锁
- 行级锁是[Mysql](https://cloud.tencent.com/product/cdb?from=10680)中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。
- 特点
- 开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。

### 表

- 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。
- 特点
- 开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。

### 页

- 页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。BDB支持页级锁
- 特点
- 开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般

## MySQL常用存储引擎的锁机制

##### MyISAM和MEMORY采用表级锁table−levellockingtable−levellocking

##### BDB采用页面锁page−levellockingpage−levellocking或表级锁，默认为页面锁

##### InnoDB支持行级锁row−levellockingrow−levellocking和表级锁,默认为行级锁

- InnoDB行锁是通过给索引上的索引项加锁来实现的，InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！

在实际应用中，要特别注意InnoDB行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。

行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁。行级锁的缺点是：由于需要请求大量的锁资源，所以速度慢，内存消耗大。

##### 实例说明

- MySQL InnoDB引擎默认的修改数据语句：update,delete,insert都会自动给涉及到的数据加上排他锁。

select语句默认不会加任何锁类型，如果加排他锁可以使用select …for update语句，加共享锁可以使用select … lock in share mode语句。

所以加过排他锁的数据行在其他事务种是不能修改数据的，也不能通过for update和lock in share mode锁的方式查询数据，但可以直接通过select …from…查询数据，因为普通查询没有任何锁机制。

## 行级锁与死锁

MyISAM中是不会产生死锁的，因为MyISAM总是一次性获得所需的全部锁，要么全部满足，要么全部等待。而在InnoDB中，锁是逐步获得的，就造成了死锁的可能。

在MySQL中，行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条sql语句操作了主键索引，MySQL就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL会先锁定该非主键索引，再锁定相关的主键索引。在UPDATE、DELETE操作时，MySQL不仅锁定WHERE条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的next-key locking。

当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引。另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。

发生死锁后，InnoDB一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。

## 共享锁与排它锁

##### 共享锁（Share Lock）

- 共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。

如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获准共享锁的事务只能读数据，不能修改数据。

用法 SELECT … LOCK IN SHARE MODE;

在查询语句后面增加LOCK IN SHARE MODE，Mysql会对查询结果中的每行都加共享锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。

##### 排它锁（eXclusive Lock）

- 排他锁又称写锁，如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。

用法 SELECT … FOR UPDATE;

在查询语句后面增加FOR UPDATE，Mysql会对查询结果中的每行都加排他锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请排他锁，否则会被阻塞。

## 乐观锁（Optimistic Lock）

##### 是什么

- 假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。

相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。

数据版本,为数据增加的一个版本标识。当读取数据时，将版本标识的值一同读出，数据每更新一次，同时对版本标识进行更新。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的版本标识进行比对，如果数据库表当前版本号与第一次取出来的版本标识值相等，则予以更新，否则认为是过期数据。

实现数据版本有两种方式，第一种是使用版本号，第二种是使用时间戳。

##### 使用版本号实现乐观锁

- 使用版本号时，可以在数据初始化时指定一个版本号，每次对数据的更新操作都对版本号执行+1操作。并判断当前版本号是不是该数据的最新的版本号。

```javascript
1.查询出商品信息
select (status,status,version) from t_goods where id=#{id}
2.根据商品信息生成订单
3.修改商品status为2
update t_goods
set status=2,version=version+1
where id=#{id} and version=#{version};
```

##### 优点与不足

- 乐观并发控制相信事务之间的数据竞争dataracedatarace的概率是比较小的，因此尽可能做下去，直到提交的时候才去锁定，所以不会产生任何锁和死锁。但如果直接简单这么做，还是有可能会遇到不可预期的结果，例如两个事务都读取了数据库的某一行，经过修改以后写回数据库，这时就遇到了问题

## 悲观锁（Pessimistic Lock）

##### 是什么

- 在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制 （也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）

##### 悲观锁的流程

- 在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。
- 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。具体响应方式由开发者根据实际需要决定。
- 如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。
- 其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。

##### MySQL InnoDB中使用悲观锁

- 要使用悲观锁，我们必须关闭mysql数据库的自动提交属性，因为MySQL默认使用autocommit模式，也就是说，当你执行一个更新操作后，MySQL会立刻将结果进行提交。set autocommit=0;

```javascript
//0.开始事务
begin;
//1.查询出商品信息
select status from t_goods where id=1 for update;
//2.根据商品信息生成订单
insert into t_orders (id,goods_id) values (null,1);
//3.修改商品status为2
update t_goods set status=2;
//4.提交事务
commit;
```

上面的查询语句中，我们使用了select…for update的方式，这样就通过开启排他锁的方式实现了悲观锁。此时在t_goods表中，id为1的 那条数据就被我们锁定了，其它的事务必须等本次事务提交之后才能执行。这样我们可以保证当前的数据不会被其它事务修改。

https://mp.weixin.qq.com/s?__biz=MzI5MzE4MzYxMw==&mid=2247487457&idx=1&sn=fd6d57706f1d2013b523c98b11a3a816&source=41#wechat_redirect

# 40.Innodb的存储空间：

MySQL 的存储结构分为 5 级：表空间、段、簇、页、行。



![图片](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**图片**



**表空间 Table Space**

上节课讲磁盘结构的时候讲过了，表空间可以看做是 InnoDB 存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。

分为：系统表空间、独占表空间、通用表空间、临时表空间、Undo 表空间。

**段 Segment**

表空间是由各个段组成的，常见的段有数据段、索引段、回滚段等，段是一个逻辑的概念。一个 ibd 文件（独立表空间文件）里面会由很多个段组成。

创建一个索引会创建两个段，一个是索引段：leaf node segment，一个是数据段：non-leaf node segment。

索引段管理非叶子节点的数据。数据段管理叶子节点的数据。

也就是说，一个表的段数，就是索引的个数乘以 2。

**簇 Extent**

一个段（Segment）又由很多的簇（也可以叫区）组成，每个区的大小是 1MB（64个连续的页）。

每一个段至少会有一个簇，一个段所管理的空间大小是无限的，可以一直扩展下去，但是扩展的最小单位就是簇。

**页 Page**

为了高效管理物理空间，对簇进一步细分，就得到了页。簇是由连续的页（Page）组成的空间，一个簇中有 64 个连续的页。（1MB／16KB=64）。这些页面在物理上和逻辑上都是连续的。

跟大多数数据库一样，InnoDB 也有页的概念（也可以称为块），每个页默认 16KB。

页是 InnoDB 存储引擎磁盘管理的最小单位，通过 innodb_page_size 设置。

一个表空间最多拥有 2^32 个页，默认情况下一个页的大小为 16KB，也就是说一个表空间最多存储 64TB 的数据。

注意，文件系统中，也有页的概念。

操作系统和内存打交道，最小的单位是页 Page。文件系统的内存页通常是 4K。



![图片](https://mmbiz.qpic.cn/mmbiz_png/4UksEIc42Kf39gjuw9f9WqR4ibBiaQ5SIetoKYpopWCwIjtg5F5Vba97EPRib799ndic3wzgyGrZA6YdOQd7lmfNlg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**图片**



```
SHOW VARIABLES LIKE 'innodb_page_size';
```

假设一行数据大小是 1K，那么一个数据页可以放 16 行这样的数据。

举例：一个页放 3 行数据。



![图片](https://mmbiz.qpic.cn/mmbiz_png/4UksEIc42Kf39gjuw9f9WqR4ibBiaQ5SIeM1Eia5FrcMq9P8EWwxHJSJSBqWVTxlVbt7wAzuQxdEWUNj4mn2gGE5Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**图片**



往表中插入数据时，如果一个页面已经写完，产生一个新的叶页面。如果一个簇的所有的页面都被用完，会从当前页面所在段新分配一个簇。

如果数据不是连续的，往已经写满的页中插入数据，会导致叶页面分裂：



![图片](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**图片**





![图片](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**图片**



# 41.索引为什么用B+树而不是用B树和红黑树

我们来看一下 InnoDB 里面的 B+树的存储结构：



![图片](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**图片**



MySQL 中的 B+Tree 有几个特点：

**1、它的关键字的数量是跟路数相等的；**

**2、B+Tree 的根节点和枝节点中都不会存储数据，只有叶子节点才存储数据。搜索到关键字不会直接返回，会到最后一层的叶子节点。比如我们搜索 id=28，虽然在第一层直接命中了，但是全部的数据在叶子节点上面，所以我还要继续往下搜索，一直到叶子节点。**

举个例子：假设一条记录是 1K，一个叶子节点（一页）可以存储 16 条记录。非叶子节点可以存储多少个指针？

假设索引字段是 bigint 类型，长度为 8 字节。指针大小在 InnoDB 源码中设置为6 字节，这样一共 14 字节。非叶子节点（一页）可以存储 16384/14=1170 个这样的单元（键值+指针），代表有 1170 个指针。

树 深 度 为 2 的 时 候 ， 有 1170^2 个 叶 子 节 点 ， 可 以 存 储 的 数 据 为1170*1170*16=21902400。



![图片](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**图片**



在查找数据时一次页的查找代表一次 IO，也就是说，一张 2000 万左右的表，查询数据最多需要访问 3 次磁盘。

所以在 InnoDB 中 B+ 树深度一般为 1-3 层，它就能满足千万级的数据存储。

**3、B+Tree 的每个叶子节点增加了一个指向相邻叶子节点的指针，它的最后一个数据会指向下一个叶子节点的第一个数据，形成了一个有序链表的结构。**

**4、它是根据左闭右开的区间 [ )来检索数据。**

**我们来看一下 B+Tree 的数据搜寻过程：**

1）比如我们要查找 28，在根节点就找到了键值，但是因为它不是页子节点，所以会继续往下搜寻，28 是[28,66)的左闭右开的区间的临界值，所以会走中间的子节点，然后继续搜索，它又是[28,34)的左闭右开的区间的临界值，所以会走左边的子节点，最后在叶子节点上找到了需要的数据。

2）第二个，如果是范围查询，比如要查询从 22 到 60 的数据，当找到 22 之后，只需要顺着节点和指针顺序遍历就可以一次性访问到所有的数据节点，这样就极大地提高了区间查询效率（不需要返回上层父节点重复遍历查找）。

**总结一下，InnoDB 中的 B+Tree 的特点：**

1)它是 B Tree 的变种，B Tree 能解决的问题，它都能解决。B Tree 解决的两大问题是什么？（每个节点存储更多关键字；路数更多）

2)扫库、扫表能力更强（如果我们要对表进行全表扫描，只需要遍历叶子节点就可以了，不需要遍历整棵 B+Tree 拿到所有的数据）

\3) B+Tree 的磁盘读写能力相对于 B Tree 来说更强（根节点和枝节点不保存数据区，所以一个节点可以保存更多的关键字，一次磁盘加载的关键字更多）

4)排序能力更强（因为叶子节点上有下一个数据区的指针，数据形成了链表）

5)效率更加稳定（B+Tree 永远是在叶子节点拿到数据，所以 IO 次数是稳定的）

# 42.Redis跳表结构：

## 深入跳表skiplist

在介绍跳表前，我们先来回顾一下基本链表，并思考为何一步一步演化成跳表的数据结构。



![图片.png](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**图片.png**



在这样一个链表中，如果我们要查找某个数据，那么需要从头开始逐个进行比较，直到找到包含数据的那个节点，或者找到第一个比给定数据大的节点为止（没找到）。也就是说，时间复杂度为Onn。同样，当我们要插入新数据的时候，也要经历同样的查找过程，从而确定插入位置。

假如我们每隔一个节点增加一个指针，让指针指向下下个节点，如下图：



![图片.png](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**图片.png**



现在当我们想查找数据的时候，可以先沿着这个新链表进行查找。当碰到比待查数据大的节点时，再回到原来的链表中进行查找。比如，我们想查找23，查找的路径是沿着下图中标红的指针所指向的方向进行的：



![图片.png](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**图片.png**



- 23首先和7比较，再和19比较，比它们都大，继续向后比较。
- 但23和26比较的时候，比26要小，因此回到下面的链表（原链表），与22比较。
- 23比22要大，沿下面的指针继续向后和26比较。23比26小，说明待查数据23在原链表中不存在，而且它的插入位置应该在22和26之间。

在这个查找过程中，由于新增加的指针，时间复杂度不再是ONN了，也即我们不再需要与链表中每个节点逐个进行比较了。需要比较的节点数大概只有原来的一半。

利用同样的方式，我们可以在上层新产生的链表上，继续扩展指针，从而产生第三层链表。如下图：



![图片.png](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**图片.png**



在这个新的三层链表结构上，如果我们还是查找23，那么沿着最上层链表首先要比较的是19，发现23比19大，接下来我们就知道只需要到19的后面去继续查找，从而一下子跳过了19前面的所有节点。可以想象，当链表足够长的时候，这种多层链表的查找方式能让我们跳过很多下层节点，大大加快查找的速度。

skiplist正是受这种多层链表的想法的启发而设计出来的。实际上，按照上面生成链表的方式，上面每一层链表的节点个数，是下面一层的节点个数的一半，这样查找过程就非常类似于一个二分查找，使得查找的时间复杂度可以降低到Olognlogn。但是，这种方法在插入数据的时候有很大的问题。新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格的2:1的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点（也包括新插入的节点）重新进行调整，这会让时间复杂度重新蜕化成Onn。删除数据也有同样的问题。

执行插入操作时计算随机数的过程，是一个很关键的过程，它对skiplist的统计特性有着很重要的影响。这并不是一个普通的服从均匀分布的随机数，而是服从一定规则的：

- 首先，每个节点肯定都有第1层指针（每个节点都在第1层链表里）。
- 如果一个节点有第i层i>=1i>=1指针（即节点已经在第1层到第i层链表中），那么它有第i+1i+1层指针的概率为p。
- 节点最大的层数不允许超过一个最大值，记为MaxLevel（Redis里是32）。

比如，一个节点随机出的层数是3，那么就把它链入到第1层到第3层这三层链表中。为了表达清楚，下图展示了如何通过一步步的插入操作从而形成一个skiplist的过程：



![图片.png](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**图片.png**



从上面skiplist的创建和插入过程可以看出，每一个节点的层数（level）是随机出来的，而且新插入一个节点不会影响其它节点的层数。因此，插入操作只需要修改插入节点前后的指针，而不需要对很多节点都进行调整。这就降低了插入操作的复杂度。实际上，这是skiplist的一个很重要的特性，这让它在插入性能上明显优于平衡树的方案。这在后面我们还会提到。

skiplist，指的就是除了最下面第1层链表之外，它会产生若干层稀疏的链表，这些链表里面的指针故意跳过了一些节点（而且越高层的链表跳过的节点越多）。这就使得我们在查找数据的时候能够先在高层的链表中进行查找，然后逐层降低，最终降到第1层链表来精确地确定数据位置。在这个过程中，我们跳过了一些节点，从而也就加快了查找速度。

刚刚创建的这个skiplist总共包含4层链表，现在假设我们在它里面依然查找23，下图给出了查找路径：



![图片.png](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**图片.png**



需要注意的是，前面演示的各个节点的插入过程，实际上在插入之前也要先经历一个类似的查找过程，在确定插入位置后，再完成插入操作。

实际应用中的skiplist每个节点应该包含member和score两部分。前面的描述中我们没有具体区分member和score，但实际上列表中是按照score进行排序的，查找过程也是根据score在比较。

## 为什么采用跳表，而不使用哈希表或平衡树实现呢

1. skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，在哈希表上只能做单个key的查找，不适宜做范围查找。所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点。
2. 在做范围查找的时候，平衡树比skiplist操作要复杂。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。
3. 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。
4. 从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/1−p1−p，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。

# 43.MySQL主键选取方式：

https://segmentfault.com/a/1190000009530839

# 数据结构

### 1.哈夫曼编码

https://zhuanlan.zhihu.com/p/75048255

### 2.map为什么要用红黑树

https://www.zhihu.com/question/20545708

### 3.B+树

##### BST（二叉查找树）

二叉查找树又称二叉搜索树，二叉排序树，特点如下:

1. 左子树上所有结点值均小于根结点
2. 右子树上所有结点值均大于根结点
3. 结点的左右子树本身又是一颗二叉查找树
4. 二叉查找树中序遍历得到结果是递增排序的结点序列。

BST 的操作代价分析：
11 查找代价：
任何一个数据的查找过程都需要从根结点出发，沿某一个路径朝叶子结点前进。因此查找中数据比较次数与树的形态密切相关。
当树中每个结点左右子树高度大致相同时，树高为logN。则平均查找长度与logN成正比，查找的平均时间复杂度在OlogNlogN数量级上。
当先后插入的关键字有序时，BST退化成单支树结构。此时树高n。平均查找长度为n+1n+1/2，查找的平均时间复杂度在ONN数量级上。

22 插入代价：
新结点插入到树的叶子上，完全不需要改变树中原有结点的组织结构。插入一个结点的代价与查找一个不存在的数据的代价完全相同。

33 删除代价：
当删除一个结点P，首先需要定位到这个结点P，这个过程需要一个查找的代价。然后稍微改变一下树的形态。如果被删除结点的左、右子树只有一个存在，则改变形态的代价仅为O11。如果被删除结点的左、右子树均存在，只需要将当P的左孩子的右孩子的右孩子的…的右叶子结点与P互换，在改变一些左右子树即可。因此删除操作的时间复杂度最大不会超过OlogNlogN。但是如果是单只结构最坏同样是ONN。

BST效率总结 :
查找最好时间复杂度OlogNlogN，最坏时间复杂度ONN。
插入删除操作算法简单，时间复杂度与查找类似。

##### BBST（AVL）平衡二叉查找树

在计算机科学中，[**AVL树**](https://zh.wikipedia.org/wiki/AVL树)是最早被发明的自平衡二叉查找树。在AVL树中，任一节点对应的两棵子树的最大高度差为1，因此它也被称为**高度平衡树**。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



在对二叉搜索树进行插入或删除操作后，可能会造成树的不平衡，所以需要对不平衡的节点进行旋转操作来保持平衡。

[**树旋转**](https://zh.wikipedia.org/wiki/树旋转)（英语：Tree rotation）是对二叉树的一种操作，不影响元素的顺序（二叉搜索树，但会改变树的结构。

一般有两种旋转方式：

- 左旋转，逆时针旋转
- 右旋转，顺时针旋转

如图所示：



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**





![img](https://xjay.net/201906/insertion-and-deletion-of-avl-tree/Tree_rotation_animation_250x250.gif)

**img**



##### 需要进行旋转的情况

当执行完二叉搜索树的节点插入或删除后，有四种情况会出现树不平衡，需要进行旋转操作：

- 左左情况
- 左右情况
- 右右情况
- 右左情况

下面使用图片说明各种情况：

###### 左左情况



![left-left](https://xjay.net/201906/insertion-and-deletion-of-avl-tree/left-left.jpg)

**left-left**



节点 `z` 的左右子树高度差大于 1，处于不平衡状态。需要右旋转。

###### 左右情况



![left-right](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**left-right**



首先将节点 `y` 进行左旋转，变成左左情况，再进行右旋转。

###### 右右情况



![right-right](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**right-right**



节点 `z` 的左右子树高度差大于 1，处于不平衡状态。需要左旋转。

###### 右左情况



![right-left](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**right-left**



先将节点 `y` 进行右旋转，变成右右情况，再进行左旋转。

###### 复杂度：

- 左右旋转操作：O11，因为只操作了节点指针。
- 更新节点高度和获取平衡因子：O11。
- 插入节点：Ohh ，h为树的高度。因为 AVL 树是平衡二叉树，高度为 h=lognh所以复杂度为 Olognlogn。
- 删除节点：同插入节点为 Ohh。

#### B树：

B 树就是常说的“B 减树（B- 树）”，又名平衡多路（即不止两个子树）查找树，它和平衡二叉树的不同有这么几点：

1. 平衡二叉树节点最多有两个子树，而 B 树每个节点可以有多个子树，**M 阶 B 树表示该树每个节点最多有 M 个子树**
2. 平衡二叉树每个节点只有一个数据和两个指向孩子的指针，而 B 树每个**中间节点**有 k-1 个关键字（可以理解为数据）和 k 个子树（ k 介于阶数 M 和 M/2 之间，M/2 向上取整）
3. B 树的所有叶子节点都在同一层，并且叶子节点只有关键字，指向孩子的指针为 null

和平衡二叉树相同的点在于：B 树的节点数据大小也是按照左小右大，子树与节点的大小比较决定了子树指针所处位置。

看着概念可能有点难理解，来看看图对比下平衡二叉树和 B 树。

##### 对比平衡二叉树和 B 树

首先是节点， 平衡二叉树的节点如下图所示，每个节点有一个数据和最多两个子树：



![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**



B 树中的每个节点由两部分组成：

1. 关键字（可以理解为数据）
2. 指向孩子节点的指针

B 树的节点如下图所示，每个节点可以有不只一个数据，同时拥有**数据数加一**个子树，同时每个节点左子树的数据比当前节点都小、右子树的数据都比当前节点的数据大：



![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**



> 上图是为了方便读者理解 B 树每个节点的内容，实际绘制图形还是以圆表示每个节点。

了解了节点的差异后，来看看 B 树的定义，**一棵 B 树必须满足以下条件**：

1. 若根结点不是终端结点，则至少有2棵子树
2. 除根节点以外的所有非叶结点至少有 M/2 棵子树，至多有 M 个子树（关键字数为子树减一）
3. 所有的叶子结点都位于同一层

用一张图对比平衡二叉树和 B 树：



![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**



可以看到，**B 树的每个节点可以表示的信息更多，因此整个树更加“矮胖”，这在从磁盘中查找数据（先读取到内存、后查找）的过程中，可以减少磁盘 IO 的次数，从而提升查找速度。**

##### B 树中如何查找数据

因为 B 树的子树大小排序规则，因此在 B 树中查找数据时，一般需要这样：

1. 从根节点开始，如果查找的数据比根节点小，就去左子树找，否则去右子树
2. 和子树的多个关键字进行比较，找到它所处的范围，然后去范围对应的子树中继续查找
3. 以此循环，直到找到或者到叶子节点还没找到为止

##### B 树如何保证平衡

我们知道，平衡的树之所以能够加快查找速度，是因为在添加、删除的时候做了某些操作以保证平衡。

平衡二叉树的平衡条件是：**左右子树的高度差不大于 1**；而 B 树的平衡条件则有三点：

1. 叶子节点都在同一层
2. 每个节点的关键字数为子树个数减一（子树个数 k 介于树的阶 M 和它的二分之一
3. 子树的关键字保证左小右大的顺序

也就是说，一棵 3 阶的 B 树（即节点最多有三个子树），每个节点的关键字数最少为 1，最多为 2，如果要添加数据的子树的关键字数已经是最多，就需要**拆分节点，调整树的结构。**

网上找到一张很不错的动图，我们来根据它分析下 B 树添加元素时如何保证平衡。

这个图用以表示往 4 阶 B 树中依次插入下面这组数据的过程：

6 10 4 14 5 11 15 3 2 12 1 7 8 8 6 3 6 21 5 15 15 6 32 23 45 65 7 8 6 5 4



![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**



**建议放大图查看**。

由于我比较懒，我们来根据前几步分析下 B 树的添加流程：

1. 首先明确：**4 阶 B 树表示每个节点最多有 4 个子树、3 个关键字，最少有 2 个子树、一个关键字**
2. 添加 6，第一个节点，没什么好说的
3. 添加 10，根节点最多能放三个关键字，按顺序添到根节点中
4. 添加 4，还能放到根节点中
5. 添加 14，这时超出了关键字最大限制，需要把 14 添加为子树，同时为了保证“所有叶子节点在同一层”，就需要拆几个关键字作为子树：



![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**

拆为：

![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**



这个拆的过程比较复杂，首先要确定根节点保留几个关键字，由于**“非叶子节点的根节点至少有 2 棵子树”**的限制，那就至少需要两个关键字分出去，又因为**“子树数是关键字数+1”**，如果根节点有两个关键字，就得有三个子树，无法满足，所以只好把除 6 以外的三个关键字都拆为子树。

谁和谁在一个子树上呢，根据**“左子树比关键字小、右子树比关键字大”**的规律，4 在左子树，10 和 14 在右子树。

继续添加 ：

1. 添加 5，放到 4 所在的子树上
2. 添加 11，放在 10 和 14 所在的右子树上
3. 添加 15，按大小应该放到 10、11 和 14 所在的子树上，但因为超过了关键字数限制，又得拆分



![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**



因为**“根节点必须都在同一层”**，因此我们不能给现有的左右子树添加子树，只能添加给 6 了；但是如果 6 有三个子树，就必须得有 2 个关键字，提升谁做关键字好呢，这得看谁做 6 中间的子树，因为右子树的所有关键字都得比父节点的关键字大，所以这个提升的关键字只能比未来右子树中的关键字都小，那就只有 10 和 11 可以考虑了。

提升 10 吧，没有比它小的做子树，那就只能提升 11 了：



![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**



再添加元素也是类似的逻辑：

1. 首先考虑要插入的子树是否已经超出了关键字数的限制
2. 超出的话，如果要插入的位置是叶子节点，就只能拆一个关键字添加到要插入位置的父节点
3. 如果非叶子节点，就得从其他子树拆子树给新插入的元素做孩子

删除也是一样的，要考虑删除孩子后，父节点是否还满足子树 k 介于 M/2 和 M 的条件，不满足就得从别的节点拆子树甚至修改相关子树结构来保持平衡。

总之添加、删除的过程很复杂，要考虑的条件很多，具体实现就不细追究了，这里我们有个基本认识即可。

正是这个复杂的保持平衡操作，使得平衡后的 B 树能够发挥出磁盘中快速查找的作用。

#### B + 树

了解了 B 树后再来了解下它的变形版：B+ 树，它比 B 树的查询性能更高。

一棵 B+ 树需要满足以下条件：

1. 节点的子树数和关键字数相同（B 树是关键字数比子树数少一）
2. 节点的关键字表示的是子树中的最大数，在子树中同样含有这个数据
3. 叶子节点包含了全部数据，同时符合左小右大的顺序



![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**



简单概括下 B+ 树的三个特点：

1. 关键字数和子树相同
2. 非叶子节点仅用作索引，它的关键字和子节点有重复元素
3. 叶子节点用指针连在一起

首先第一点不用特别介绍了，在 B 树中，节点的关键字用于在查询时确定查询区间，因此关键字数比子树数少一；而在 B+ 树中，节点的关键字代表子树的最大值，因此关键字数等于子树数。

第二点，除叶子节点外的所有节点的关键字，都在它的下一级子树中同样存在，最后所有数据都存储在叶子节点中。

> 根节点的最大关键字其实就表示整个 B+ 树的最大元素。

第三点，叶子节点包含了全部的数据，并且按顺序排列，B+ 树使用一个链表将它们排列起来，这样在查询时效率更快。

由于 B+ 树的中间节点不含有实际数据，只有子树的最大数据和子树指针，因此磁盘页中可以容纳更多节点元素，也就是说同样数据情况下，B+ 树会 B 树更加“矮胖”，因此查询效率更快。

B+ 树的查找必会查到叶子节点，更加稳定。

有时候需要查询某个范围内的数据，由于 B+ 树的叶子节点是一个有序链表，只需在叶子节点上遍历即可，不用像 B 树那样挨个中序遍历比较大小。

B+ 树的三个优点：

1. 层级更低，IO 次数更少
2. 每次都需要查询到叶子节点，查询性能稳定
3. 叶子节点形成有序链表，范围查询方便

添加过程就不深入研究了，后面用到再看吧，这里先贴一个 B+ 树动态添加元素图：



![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**



\#

#### 红黑树：

平衡树在插入和删除的时候，会通过旋转操作将高度保持在logN。其中两款具有代表性的平衡树分别为AVL树和红黑树。AVL树由于实现比较复杂，而且插入和删除性能差，在实际环境下的应用不如红黑树。

红黑树（Red-Black Tree，以下简称RBTree）的实际应用非常广泛，比如Linux内核中的完全公平调度器、高精度计时器、ext3文件系统等等，各种语言的函数库如Java的TreeMap和TreeSet，C++ STL的map、multimap、multiset等。

RBTree也是函数式语言中最常用的持久数据结构之一，在计算几何中也有重要作用。值得一提的是，Java 8中HashMap的实现也因为用RBTree取代链表，性能有所提升。

##### RBTree的定义

RBTree的定义如下:

1. 任何一个节点都有颜色，黑色或者红色。
2. 根节点是黑色的。
3. 父子节点之间不能出现两个连续的红节点。
4. 任何一个节点向下遍历到其子孙的叶子节点，所经过的黑节点个数必须相等。
5. 空节点被认为是黑色的。

具体插入删除操作十分复杂：参考美团团队技术文档：

https://tech.meituan.com/2016/12/02/redblack-tree.html

插入修复操作分为以下的三种情况，而且新插入的节点的父节点都是红色的：

1. 叔叔节点也为红色。
2. 叔叔节点为空，且祖父节点、父节点和新节点处于一条斜线上。
3. 叔叔节点为空，且祖父节点、父节点和新节点不处于一条斜线上。

###### 插入操作-Case 1

case 1的操作是将父节点和叔叔节点与祖父节点的颜色互换，这样就符合了RBTRee的定义。即维持了高度的平衡，修复后颜色也符合RBTree定义的第三条和第四条。下图中，操作完成后A节点变成了新的节点。如果A节点的父节点不是黑色的话，则继续做修复操作。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



###### 插入操作-Case 2

case 2的操作是将B节点进行右旋操作，并且和父节点A互换颜色。通过该修复操作RBTRee的高度和颜色都符合红黑树的定义。如果B和C节点都是右节点的话，只要将操作变成左旋就可以了。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



###### 插入操作-Case 3

case 3的操作是将C节点进行左旋，这样就从case 3转换成case 2了，然后针对case 2进行操作处理就行了。case 2操作做了一个右旋操作和颜色互换来达到目的。如果树的结构是下图的镜像结构，则只需要将对应的左旋变成右旋，右旋变成左旋即可。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



删除修复操作分为四种情况删除黑节点后删除黑节点后：

1. 待删除的节点的兄弟节点是红色的节点。
2. 待删除的节点的兄弟节点是黑色的节点，且兄弟节点的子节点都是黑色的。
3. 待调整的节点的兄弟节点是黑色的节点，且兄弟节点的左子节点是红色的，右节点是黑色的兄弟节点在右边兄弟节点在右边，如果兄弟节点在左边的话，就是兄弟节点的右子节点是红色的，左节点是黑色的。
4. 待调整的节点的兄弟节点是黑色的节点，且右子节点是是红色的兄弟节点在右边兄弟节点在右边，如果兄弟节点在左边，则就是对应的就是左节点是红色的。

###### 删除操作-Case 1

由于兄弟节点是红色节点的时候，无法借调黑节点，所以需要将兄弟节点提升到父节点，由于兄弟节点是红色的，根据RBTree的定义，兄弟节点的子节点是黑色的，就可以从它的子节点借调了。

case 1这样转换之后就会变成后面的case 2，case 3，或者case 4进行处理了。上升操作需要对C做一个左旋操作，如果是镜像结构的树只需要做对应的右旋操作即可。

之所以要做case 1操作是因为兄弟节点是红色的，无法借到一个黑节点来填补删除的黑节点。



![img](https://p1.meituan.net/travelcube/e32688e1ea6037abc1bd8afd645e3445115445.png@678w_310h_80q)

**img**



###### 删除操作-Case 2

case 2的删除操作是由于兄弟节点可以消除一个黑色节点，因为兄弟节点和兄弟节点的子节点都是黑色的，所以可以将兄弟节点变红，这样就可以保证树的局部的颜色符合定义了。这个时候需要将父节点A变成新的节点，继续向上调整，直到整颗树的颜色符合RBTree的定义为止。

case 2这种情况下之所以要将兄弟节点变红，是因为如果把兄弟节点借调过来，会导致兄弟的结构不符合RBTree的定义，这样的情况下只能是将兄弟节点也变成红色来达到颜色的平衡。当将兄弟节点也变红之后，达到了局部的平衡了，但是对于祖父节点来说是不符合定义4的。这样就需要回溯到父节点，接着进行修复操作。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



###### 删除操作-Case 3

case 3的删除操作是一个中间步骤，它的目的是将左边的红色节点借调过来，这样就可以转换成case 4状态了，在case 4状态下可以将D，E节点都阶段过来，通过将两个节点变成黑色来保证红黑树的整体平衡。

之所以说case-3是一个中间状态，是因为根据红黑树的定义来说，下图并不是平衡的，他是通过case 2操作完后向上回溯出现的状态。之所以会出现case 3和后面的case 4的情况，是因为可以通过借用侄子节点的红色，变成黑色来符合红黑树定义4。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



###### 删除操作-Case 4

Case 4的操作是真正的节点借调操作，通过将兄弟节点以及兄弟节点的右节点借调过来，并将兄弟节点的右子节点变成红色来达到借调两个黑节点的目的，这样的话，整棵树还是符合RBTree的定义的。

Case 4这种情况的发生只有在待删除的节点的兄弟节点为黑，且子节点不全部为黑，才有可能借调到两个节点来做黑节点使用，从而保持整棵树都符合红黑树的定义。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



### 4.topK问题

https://zhuanlan.zhihu.com/p/76734219

思路：只找到TopK，不排序TopK。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



先用前k个元素生成一个小顶堆，这个小顶堆用于存储，当前最大的k个元素。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



接着，从第k+1个元素开始扫描，和堆顶（堆中最小的元素）比较，如果被扫描的元素大于堆顶，则替换堆顶的元素，并调整堆，以保证堆内的k个元素，总是当前最大的k个元素。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



直到，扫描完所有n-k个元素，最终堆中的k个元素，就是猥琐求的TopK。

时间复杂度：On∗lg(kn∗lg(k)

画外音：n个元素扫一遍，假设运气很差，每次都入堆调整，调整时间复杂度为堆的高度，即lgkk，故整体时间复杂度是n*lgkk。

分析：堆，将冒泡的TopK排序优化为了TopK不排序，节省了计算资源。堆，是求TopK的经典算法，那还有没有更快的方案呢？

快速排序

与快排相比，两者唯一的不同是在对”分治”结果的使用上。我们知道，分治函数会返回一个position，在position左边的数都比第position个数小，在position右边的数都比第position大。我们不妨不断调用分治函数，直到它输出的position = K-1，此时position前面的K个数（0到K-1）就是要找的前K个数。

### 5.堆和栈的区别，为什么栈更快，用栈实现队列

首先, 栈是本着LIFO原则的存储机制, 对栈数据的定位相对比较快速, 而堆则是随机分配的空间, 处理的数据比较多, 无论如何, 至少要两次定位.

其次, 栈是由CPU提供指令支持的, 在指令的处理速度上, 对栈数据进行处理的速度自然要优于由操作系统支持的堆数据.

再者, 栈是在一级缓存中做缓存的, 而堆则是在二级缓存中, 两者在硬件性能上差异巨大.

最后, 各语言对栈的优化支持要优于对堆的支持, 比如swift语言中, 三个字及以内的struct结构, 可以在栈中内联, 从而达到更快的处理速度.

## 6.排序算法和其复杂度，稳定性

https://blog.csdn.net/pange1991/article/details/85460755



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



### 堆排序比快速排序时间复杂度好，为什么还用堆排序

第一、堆排序访问数据的方式没有快速排序友好。

**对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是跳着访问的。**比如，堆排序中，最重要的一个操作就是数据的堆化。比如下面这个例子，对堆顶进行堆化，会依次访问数组下标是1，2，4，8的元素，而不像快速排序那样，局部顺序访问，所以，这样对CPU缓存是不友好的。

第二、对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。

对于基于比较的排序算法来说，整个排序过程是由两个基本操作组成的，比较和交换。**快速排序交换的次数不会比逆序对多。但是堆排序的第一步是建堆，建堆的过程会打乱数据原有的相对选择顺序，导致数据有序度降低。比如对于一组已经有序的数据来说，经过建堆之后，数据反而变得更无序了**

# 原理理解

## 1 冒泡排序

### 1.1 过程

冒泡排序从小到大排序：一开始交换的区间为0~~N-1，将第1个数和第2个数进行比较，前面大于后面，交换两个数，否则不交换。再比较第2个数和第三个数，前面大于后面，交换两个数否则不交换。依次进行，最大的数会放在数组最后的位置。然后将范围变为0~~N-2，数组第二大的数会放在数组倒数第二的位置。依次进行整个交换过程，最后范围只剩一个数时数组即为有序。

### 1.2 动图



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



### 1.3 核心代码（函数）

```
//array[]为待排序数组，n为数组长度
void BubbleSort(int array[], int n)
{
    int i, j, k;
    for(i=0; i<n-1; i++)
        for(j=0; j<n-1-i; j++)
        {
            if(array[j]>array[j+1])
            {
                k=array[j];
                array[j]=array[j+1];
                array[j+1]=k;
            }
        }
}
```

## 2 直接选择排序

### 2.1 过程

选择排序从小到大排序：一开始从0~~n-1区间上选择一个最小值，将其放在位置0上，然后在1~~n-1范围上选取最小值放在位置1上。重复过程直到剩下最后一个元素，数组即为有序。

### 2.2 动图



![img](https://img-blog.csdnimg.cn/20201009100055434.gif)

**img**



### 2.3 核心代码（函数）

```
//array[]为待排序数组，n为数组长度
void selectSort(int array[], int n)
{
    int i, j ,min ,k;
    for( i=0; i<n-1; i++)
    {
        min=i; //每趟排序最小值先等于第一个数，遍历剩下的数
        for( j=i+1; j<n; j++) //从i下一个数开始检查
        {
            if(array[min]>array[j])
            {
                min=j;
            }
        }
        if(min!=i)
        {
            k=array[min];
            array[min]=array[i];
            array[i]=k;
        }
    }
}
```

## 3 直接插入排序

### 3.1 过程

插入排序从小到大排序：首先位置1上的数和位置0上的数进行比较，如果位置1上的数大于位置0上的数，将位置0上的数向后移一位，将1插入到0位置，否则不处理。位置k上的数和之前的数依次进行比较，如果位置K上的数更大，将之前的数向后移位，最后将位置k上的数插入不满足条件点，反之不处理。

### 3.2 动图



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



### 3.3 核心代码（函数）

```
//array[]为待排序数组，n为数组长度
void insertSort(int array[], int n)
{
    int i,j,temp;
    for( i=1;i<n;i++)
    {
        if(array[i]<array[i-1])
        {
            temp=array[i];
            for( j=i;array[j-1]>temp;j--)
            {
                array[j]=array[j-1];
            }
            array[j]=temp;
        }
    }
}
```

## 4 快速排序

### 4.1 过程

快速排序从小到大排序：在数组中随机选一个数（默认数组首个元素），数组中小于等于此数的放在左边部分，大于此数的放在右边部分，这个操作确保了这个数是处于正确位置的，再对左边部分数组和右边部分数组递归调用快速排序，重复这个过程。

### 4.2 动图



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



![img](https://img-blog.csdnimg.cn/img_convert/cb345b3b3dc2ae27314570d8aa9b1830.png)

**img**



### 4.3 核心代码（函数）

```
void quicksort(int a[], int left, int right) {
    int i, j, t, privotkey;
    if (left > right)   //（递归过程先写结束条件）
        return;

    privotkey = a[left]; //temp中存的就是基准数（枢轴）
    i = left;
    j = right;
    while (i < j) {
        //顺序很重要，要先从右边开始找（最后交换基准时换过去的数要保证比基准小，因为基准选取数组第一个数）
        while (a[j] >= privotkey && i < j) {
            j--;
        }
        a[i] = a[j];
        //再找左边的
        while (a[i] <= privotkey && i < j) {
            i++;
        }
        a[j] = a[i];
    }
    //最终将基准数归位
    a[i] = privotkey;

    quicksort(a, left, i - 1);//继续处理左边的，这里是一个递归的过程
    quicksort(a, i + 1, right);//继续处理右边的 ，这里是一个递归的过程
}
```

## 5 堆排序

### 5.1 过程

堆排序从小到大排序：首先将数组元素建成大小为n的大顶堆，堆顶（数组第一个元素）是所有元素中的最大值，将堆顶元素和数组最后一个元素进行交换，再将除了最后一个数的n-1个元素 建立成大顶堆，再将最大元素和数组倒数第二个元素进行交换，重复直至堆大小减为1。

- 注：完全二叉树

  假设二叉树深度为n，除了第n层外，n-1层节点都有两个孩子，第n层节点连续从左到右。如下图

  ![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

  **这里写图片描述**

- 注：大顶堆

  大顶堆是具有以下性质的完全二叉树：每个节点的值都大于或等于其左右孩子节点的值。

  即，根节点是堆中最大的值，按照层序遍历给节点从1开始编号，则节点之间满足如下关系：

  ![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

  **这里写图片描述**

  

  1<=i<=n/21<=i<=n/2

### 5.2 动图



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



### 5.3 核心代码（函数）



![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**



```
void heapSort(int array[], int n)
{
    int i;
    for (i=n/2;i>0;i--)
    {
        HeapAdjust(array,i,n);//从下向上，从右向左调整
    }
    for( i=n;i>1;i--)
    {
        swap(array, 1, i);
        HeapAdjust(array, 1, i-1);//从上到下，从左向右调整
    }
}
void HeapAdjust(int array[], int s, int n )
{
    int i,temp;
    temp = array[s];
    for(i=2*s;i<=n;i*=2)
    {
        if(i<n&&array[i]<array[i+1])
        {
            i++;
        }
        if(temp>=array[i])
        {
            break;
        }
        array[s]=array[i];
        s=i;
    }
    array[s]=temp;
}
void swap(int array[], int i, int j)
{
    int temp;

    temp=array[i];
    array[i]=array[j];
    array[j]=temp;
}
```

## 6 希尔排序

### 6.1 过程

希尔排序是插入排序改良的算法，希尔排序步长从大到小调整，第一次循环后面元素逐个和前面元素按间隔步长进行比较并交换，直至步长为1，步长选择是关键。

### 6.2 动图



![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



### 6.3 核心程序（函数）

```
//下面是插入排序
void InsertSort( int array[], int n)
{
    int i,j,temp;
    for( i=0;i<n;i++ )
    {
        if(array[i]<array[i-1])
        {
            temp=array[i];
            for( j=i-1;array[j]>temp;j--)
            {
                array[j+1]=array[j];
            }
            array[j+1]=temp;
        }
    }
}
//在插入排序基础上修改得到希尔排序
void SheelSort( int array[], int n)
{
    int i,j,temp;
    int gap=n; //~~~~~~~~~~~~~~~~~~~~~
    do{
        gap=gap/3+1;  //~~~~~~~~~~~~~~~~~~
        for( i=gap;i<n;i++ )
        {
            if(array[i]<array[i-gap])
            {
                temp=array[i];
                for( j=i-gap;array[j]>temp;j-=gap)
                {
                    array[j+gap]=array[j];
                }
                array[j+gap]=temp;
            }
        }
    }while(gap>1);  //~~~~~~~~~~~~~~~~~~~~~~

}
```

## 7.归并排序

### 7.1 过程

归并排序从小到大排序：首先让数组中的每一个数单独成为长度为1的区间，然后两两一组有序合并，得到长度为2的有序区间，依次进行，直到合成整个区间。

### 7.2 动图



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



### 7.3 核心代码（函数）

实现归并，并把数据都放在list1里面

```
void merging(int *list1, int list1_size, int *list2,  int list2_size)
{
    int i=0, j=0, k=0, m=0;
    int temp[MAXSIZE];

    while(i < list1_size && j < list2_size)
    {
        if(list1[i]<list2[j])
        {
            temp[k++] = list1[i++];
        }
        else
        {
            temp[k++] = list2[j++];
        }
    }
    while(i<list1_size)
    {
        temp[k++] = list1[i++];
    }
    while(j<list2_size)
    {
        temp[k++] = list2[j++];
    }

    for(m=0; m < (list1_size+list2_size); m++)
    {
        list1[m]=temp[m];
    }

}
//如果有剩下的，那么说明就是它是比前面的数组都大的，直接加入就可以了 
void mergeSort(int array[], int n)
{
    if(n>1)
    {
        int *list1 = array;
        int list1_size = n/2;
        int *list2 = array + n/2;
        int list2_size = n-list1_size;

        mergeSort(list1, list1_size);
        mergeSort(list2, list2_size);

        merging(list1, list1_size, list2, list2_size);
    }

}
//归并排序复杂度分析：一趟归并需要将待排序列中的所有记录  
//扫描一遍，因此耗费时间为O(n),而由完全二叉树的深度可知，  
//整个归并排序需要进行[log2n],因此，总的时间复杂度为  
//O(nlogn),而且这是归并排序算法中平均的时间性能  
//空间复杂度：由于归并过程中需要与原始记录序列同样数量级的  
//存储空间去存放归并结果及递归深度为log2N的栈空间，因此空间  
//复杂度为O(n+logN)  
//也就是说，归并排序是一种比较占内存，但却效率高且稳定的算法 
```

## 8 桶排序（基数排序和基数排序的思想）

### 8.1 过程

桶排序是计数排序的变种，把计数排序中相邻的m个”小桶”放到一个”大桶”中，在分完桶后，对每个桶进行排序（一般用快排），然后合并成最后的结果。

### 8.2 核心程序

```
#include <stdio.h>
int main()
{
    int a[11],i,j,t;
    for(i=0;i<=10;i++)
        a[i]=0;  //初始化为0

    for(i=1;i<=5;i++)  //循环读入5个数
    {
        scanf("%d",&t);  //把每一个数读到变量t中
        a[t]++;  //进行计数(核心行)
    }

    for(i=0;i<=10;i++)  //依次判断a[0]~a[10]
        for(j=1;j<=a[i];j++)  //出现了几次就打印几次
            printf("%d ",i);

    getchar();getchar(); 
    //这里的getchar();用来暂停程序，以便查看程序输出的内容
    //也可以用system("pause");等来代替
    return 0;

}
```

## 9 计数排序

### 9.1 过程

算法的步骤如下：
\- 找出待排序的数组中最大和最小的元素
\- 统计数组中每个值为i的元素出现的次数，存入数组C的第i项
\- 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）
\- 反向填充目标数组：将每个元素i放在新数组的第Cii项，每放一个元素就将Cii减去1

### 9.2 图解



![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**



### 9.3 核心程序（函数）

```
程序1：
#define NUM_RANGE (100)    //预定义数据范围上限，即K的值

void counting_sort(int *ini_arr, int *sorted_arr, int n)  //所需空间为 2*n+k
{  
       int *count_arr = (int *)malloc(sizeof(int) * NUM_RANGE);  
       int i, j, k;  

       //初始化统计数组元素为值为零 
       for(k=0; k<NUM_RANGE; k++){  
               count_arr[k] = 0;  
       }  
       //统计数组中，每个元素出现的次数    
       for(i=0; i<n; i++){  
               count_arr[ini_arr[i]]++;  
       }  

       //统计数组计数，每项存前N项和，这实质为排序过程
       for(k=1; k<NUM_RANGE; k++){  
               count_arr[k] += count_arr[k-1];  
       }  

       //将计数排序结果转化为数组元素的真实排序结果
       for(j=n-1 ; j>=0; j--){  
           int elem = ini_arr[j];          //取待排序元素
           int index = count_arr[elem]-1;  //待排序元素在有序数组中的序号
           sorted_arr[index] = elem;       //将待排序元素存入结果数组中
           count_arr[elem]--;              //修正排序结果，其实是针对算得元素的修正
       }  
       free(count_arr);  

}  

程序2：C++(最大最小压缩桶数)
public static void countSort(int[] arr) {
        if (arr == null || arr.length < 2) {
            return;
        }
        int min = arr[0];
        int max = arr[0];
        for (int i = 1; i < arr.length; i++) {
            min = Math.min(arr[i], min);
            max = Math.max(arr[i], max);
        }
        int[] countArr = new int[max - min + 1];
        for (int i = 0; i < arr.length; i++) {
            countArr[arr[i] - min]++;
        }
        int index = 0;
        for (int i = 0; i < countArr.length; i++) {
            while (countArr[i]-- > 0) {
                arr[index++] = i + min;
        }
}
```

### 10 基数排序

### 10.1 过程

基数排序是基于数据位数的一种排序算法。
它有两种算法
①LSD–Least Significant Digit first 从低位（个位）向高位排。
②MSD– Most Significant Digit first 从高位向低位（个位）排。
时间复杂度ON∗最大位数N∗最大位数。
空间复杂度ONN。

### 10.2 图解



![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**


对a[n]按照个位0~9进行桶排序：

![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**


对b[n]进行累加得到c[n]，用于b[n]中重复元素计数
！！！b[n]中的元素为temp中的位置！！！跳跃的用++补上：

![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**


temp数组为排序后的数组，写回a[n]。temp为按顺序倒出桶中的数据（联合b[n],c[n],a[n]得到），重复元素按顺序输出：

![这里写图片描述](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**这里写图片描述**



### 10.3 核心程序

```
 //基数排序  
//LSD  先以低位排，再以高位排  
//MSD  先以高位排，再以低位排  
void LSDSort(int *a, int n)  
{  
    assert(a);  //判断a是否为空，也可以a为空||n<2返回
    int digit = 0;   //最大位数初始化
    for (int i = 0; i < n; ++i)  
    {   //求最大位数
        while (a[i] > (pow(10,digit)))  //pow函数要包含头文件math.h，pow(10,digit)=10^digit
        {  
            digit++;  
        }  
    }  
    int flag = 1;   //位数
    for (int j = 1; j <= digit; ++j)  
    {  
        //建立数组统计每个位出现数据次数（Digit[n]为桶排序b[n]）  
        int Digit[10] = { 0 };  
        for (int i = 0; i < n; ++i)  
        {  
            Digit[(a[i] / flag)%10]++;  //flag=1时为按个位桶排序
        }  
         //建立数组统计起始下标（BeginIndex[n]为个数累加c[n]，用于记录重复元素位置
         //flag=1时，下标代表个位数值，数值代表位置，跳跃代表重复）
        int BeginIndex[10] = { 0 };  
        for (int i = 1; i < 10; ++i)  
        {  
            //累加个数
            BeginIndex[i] = BeginIndex[i - 1] + Digit[i - 1];  
        }  
        //建立辅助空间进行排序 
        //下面两条可以用calloc函数实现
        int *tmp = new int[n];  
        memset(tmp, 0, sizeof(int)*n);//初始化  
        //联合各数组求排序后的位置存在temp中
        for (int i = 0; i < n; ++i)  
        {  
            int index = (a[i] / flag)%10;  //桶排序和位置数组中的下标
            //计算temp相应位置对应a[i]中的元素，++为BeginIndex数组数值加1
            //跳跃间隔用++来补，先用再++
            tmp[BeginIndex[index]++] = a[i];  
        }  
        //将数据重新写回原空间  
        for (int i = 0; i < n; ++i)  
        {  
            a[i] = tmp[i];  
        }  
        flag = flag * 10;  
        delete[] tmp;  
    }  
}  
```

### 7.hash表的实现和如何处理冲突

https://zhuanlan.zhihu.com/p/29520044

### 8.加密算法

https://juejin.cn/post/6844903638117122056

### 9.LRU

https://leetcode-cn.com/problems/lru-cache/

### 10.洗牌算法

https://blog.csdn.net/qq_26399665/article/details/79831490

### 11.两个线程同时对int a=0进行a++一百次最后的结果可能为多少?

https://blog.csdn.net/qq_41594146/article/details/87433065?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control

### 12.给你一个 5L 和 3L 桶，水无限多，怎么到出 4L。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



### 13.一个饼，一把刀无限长，划9刀，最少多少块，最多多少块

n为刀数

最多块数：fn−1n−1 + n

最少：平行切

### 14.按行遍历和按列遍历的区分

**行优先（Row-major）或者列优先（Column-major）没有好坏，但其直接涉及到对内存中数据的最佳存储访问方式**。因为**在内存使用上，程序访问的内存地址之间连续性越好，程序的访问效率就越高；相应地，程序访问的内存地址之间连续性越差。所以，我们应该尽量在行优先机制的编译器**，

具体解释：

CPU高速缓存：在计算机系统中，CPU高速缓存（英语：CPU Cache，在本文中简称缓存）是用于减少处理器访问内存所需平均时间的部件。在金字塔式存储体系中它位于自顶向下的第二层，仅次于CPU寄存器。其容量远小于内存，但速度却可以接近处理器的频率。当处理器发出内存访问请求时，会先查看缓存内是否有请求数据。如果存在（命中），则不经访问内存直接返回该数据；如果不存在（失效），则要先把内存中的相应数据载入缓存，再将其返回处理器。缓存之所以有效，主要是因为程序运行时对内存的访问呈现局部性（Locality）特征。这种局部性既包括空间局部性（Spatial Locality），也包括时间局部性（Temporal Locality）。有效利用这种局部性，缓存可以达到极高的命中率。（百度百科解释）。

缓存从内存中抓取一般都是整个数据块，所以它的物理内存是连续的，几乎都是同行不同列的，而如果内循环以列的方式进行遍历的话，将会使整个缓存块无法被利用，而不得不从内存中读取数据，而从内存读取速度是远远小于从缓存中读取数据的。随着数组元素越来越多，按列读取速度也会越来越慢。

### 15.A*算法

A*算法通过下面这个函数来计算每个节点的优先级。



![img](https://pic3.zhimg.com/80/v2-3c1f00587f5f8994946cf1d224419bba_720w.png)

**img**



其中：

- fnn是节点n的综合优先级。当我们选择下一个要遍历的节点时，我们总会选取综合优先级最高（值最小）的节点。
- gnn 是节点n距离起点的代价。
- hnn是节点n距离终点的预计代价，这也就是A*算法的启发函数。关于启发函数我们在下面详细讲解。

A*算法在运算过程中，每次从优先队列中选取fnn值最小（优先级最高）的节点作为下一个待遍历的节点。

#### 启发函数

上面已经提到，启发函数会影响A*算法的行为。

- 在极端情况下，当启发函数hnn始终为0，则将由gnn决定节点的优先级，此时算法就退化成了Dijkstra算法。
- 如果hnn始终小于等于节点n到终点的代价，则A*算法保证一定能够找到最短路径。但是当hnn的值越小，算法将遍历越多的节点，也就导致算法越慢。
- 如果hnn完全等于节点n到终点的代价，则A*算法将找到最佳路径，并且速度很快。可惜的是，并非所有场景下都能做到这一点。因为在没有达到终点之前，我们很难确切算出距离终点还有多远。
- 如果hnn的值比节点n到终点的代价要大，则A*算法不能保证找到最短路径，不过此时会很快。
- 在另外一个极端情况下，如果hn相较于gnn大很多，则此时只有hnn产生效果，这也就变成了最佳优先搜索。

由上面这些信息我们可以知道，通过调节启发函数我们可以控制算法的速度和精确度。因为在一些情况，我们可能未必需要最短路径，而是希望能够尽快找到一个路径即可。这也是A*算法比较灵活的地方。

对于网格形式的图，有以下这些启发函数可以使用：

- 如果图形中只允许朝上下左右四个方向移动，则可以使用曼哈顿距离（Manhattan distance）。
- 如果图形中允许朝八个方向移动，则可以使用对角距离。
- 如果图形中允许朝任何方向移动，则可以使用欧几里得距离（Euclidean distance）。

## 16.原地归并算法

原地归并排序原理介绍，以下面的数组为例进行说明。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



开始时分别指向这个数组的两个有序子序列的第一个值，然后指针向后移动，直到找到比20大的值，即移动

到30，此时我们知道指针之前的值一定是两个子序列的最小的块，先用一个临时指针记录的位置。然后

把第二个序列的指针向后移动，直到找到比30大的值，即移动到55，即如下图所示：



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



这样，我们把和的内存块交换，再移动指针，移动长度为，得到



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



这样可以看出之前的都已经排好序，而以开始的子序列和以开始的子序列又是开始的问题模型，同样的操作进

行下去最终排序完成。

# 项目

#### **1.git merge和rebase的区别**



![clipboard.png](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**clipboard.png**



merge会进行一个三方合并，共同祖先和两个分支的最新提交，生成一个新的conmmit



![clipboard.png](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**clipboard.png**



rebase则会将刚刚提取的master分支指向目标分支的最新提交处，连城一条线



![clipboard.png](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**clipboard.png**



1. 可以看出merge结果能够体现出时间线，但是rebase会打乱时间线。
2. 而rebase看起来简洁，但是merge看起来不太简洁。
3. 最终结果是都把代码合起来了，所以具体怎么使用这两个命令看项目需要。

git可视化工具： gitkraken

#### **2.git的常用命令**



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



1.git init 初始化一个仓库

2.git clone 克隆一个项目

3.git add 添加当前目录到暂存区

4.git commit -m 提交暂存区到仓库区

5.git branch 列出本地所有分支

6.git checkout [branch-name] 切换到指定分支，撤销更改

7.git merge/rebase 合并

8.git log 查看信息

9.git push -u origin master 本地库所有内容推送到远程仓库

10.git remote 查看分支情况

11.git pull 远程获取代码版本

#### 3.linux， grep

grep指令用于找到文件中的相关匹配文本，并且能够接受正则表达式和通配符

格式：grep [选项] pattern [file]

#### 4.Linux调试程序：

- IDE设置断点进行调试
- Linux中没有IDE，可以打印log
- 打印中间结果
- 生成core文件

#### 5.遇到core dump怎么调试：

- 打开生成core文件的开关`ulimit -c unlimited`
- 输入`gdb core文件名 core`
- 输入`where`命令，可以找到错误的根源

#### 6.gdb调试C++

c++：

break 调用断点

使用`backtrace`命令查看调用栈：

| 命令（缩写）                            | 功 能                                                        |
| :-------------------------------------- | :----------------------------------------------------------- |
| run（r）                                | 启动或者重启一个程序。                                       |
| list（l）                               | 显示带有行号的源码。                                         |
| continue（c）                           | 让暂停的程序继续运行。                                       |
| next（n）                               | 单步调试程序，即手动控制代码一行一行地执行。                 |
| step（s）                               | 如果有调用函数，进入调用的函数内部；否则，和 next 命令的功能一样。 |
| until（u） until location（u location） | 当你厌倦了在一个循环体内单步跟踪时，单纯使用 until 命令，可以运行程序直到退出循环体。 until n 命令中，n 为某一行代码的行号，该命令会使程序运行至第 n 行代码处停止。 |
| finish（fi）                            | 结束当前正在执行的函数，并在跳出函数后暂停程序的执行。       |
| return（return）                        | 结束当前调用函数并返回指定值，到上一层函数调用处停止程序执行。 |
| jump（j）                               | 使程序从当前要执行的代码处，直接跳转到指定位置处继续执行后续的代码。 |
| print（p）                              | 打印指定变量的值。                                           |
| quit（q）                               | 退出 GDB 调试器。                                            |

http://senlinzhan.github.io/2017/12/31/gdb/

死锁：

http://senlinzhan.github.io/2018/01/01/gdb-on-multithreaded/

多线程调试

#### 7.Valgrind检测用法：

http://senlinzhan.github.io/2017/12/31/valgrind/

#### 8.linux命令：

##### top

概述：top命令是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。
可以直接使用top命令后，查看%MEM的内容。可以选择按进程查看或者按用户查看，如想查看oracle用户的进程内存使用情况的话可以使用如下的命令：

##### pmap

可以根据进程查看进程相关信息占用的内存情况，进程号可以通过ps查看进程号可以通过ps查看如下所示：
　　pmap -d 14596

##### ps

作用：ps显示瞬间进程 processprocess 的动态，使用权限是所有使用者。

##### grep

概要：

**grep**全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。
**grep**命令可以**指定文件中搜索特定的内容，并将含有这些内容的行标准输出。**

##### awk

概要：**把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理**。

##### sed

概要：Linux sed命令是利用script来处理文本文件。
sed可依照script的指令，来处理、编辑文本文件。
Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。

##### awk、sed、grep对比

- grep 更适合单纯的查找或匹配文本
- sed 更适合编辑匹配到的文本
- awk 更适合格式化文本，对文本进行较复杂格式处理

##### rm

概要：删除一个目录中的一个或多个文件或目录，如果

##### wcl

-c **统计**字节**数**。

-l **统计行数**。

-w **统计**字数。

### netast命令，现实当前网络状况：

-a allall显示所有选项，默认不显示LISTEN相关
-t tcptcp仅显示tcp相关选项
-u udpudp仅显示udp相关选项
-n 拒绝显示别名，能显示数字的全部转化成数字。
-l 仅列出有在 Listen 监听监听 的服務状态

-p 显示建立相关链接的程序名
-r 显示路由信息，路由表
-e 显示扩展信息，例如uid等
-s 按各个协议进行统计
-c 每隔一个固定时间，执行该netstat命令。

# 设计模式：

#### **1.单例模式，多线程安全问题，怎么保证只有唯一实例**

https://steverao.github.io/2018/12/12/Singleton-Pattern-in-Multithreaded/

[https://neo00.top/archives/%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F](https://neo00.top/archives/创建型模式之单例模式)

```c++
#include 
#include 
using namespace std;

class  SingleIton
{
public:
    static mutex mtx;//静态成员变量类内声明，类外定义
    static SingleIton *GetInstance() 
    {
        if (Instance == NULL)//第一次检索是为判断是否对象已经存在，存在就不进入同步
        {
            unique_lock m_locker(mtx);//进入线程同步，互斥量防止多个线程同时生成多个实例
            if (Instance == NULL)//第二次检索是为了判断是否对象已经存在，不存在就生成
                Instance = new SingleIton();
        }
        return Instance;
    }
private:
    static SingleIton *Instance;
    SingleIton() {};//构造函数私有，防止默认构造函数实例对象
};
mutex SingleIton::mtx;
SingleIton* SingleIton::Instance = NULL;


int main()
{
    SingleIton  *s1 = SingleIton::GetInstance();
    SingleIton  *s2 = SingleIton::GetInstance();
    cout << s1 << "  " << s2;
    return 0;
}
```

#### **2.工厂模式**

https://design-patterns.readthedocs.io/zh_CN/latest/creational_patterns/factory_method.html

#### 3.观察者模式

https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/observer.html

#### 4.装饰器模式

https://design-patterns.readthedocs.io/zh_CN/latest/structural_patterns/decorator.html

## 5.MVC, MVP, MVVM模式

### MVC

MVC模式的意思是，软件可以分成三个部分。



![img](http://www.ruanyifeng.com/blogimg/asset/2015/bg2015020104.png)

**img**



> - 视图（View）：用户界面。
> - 控制器（Controller）：业务逻辑
> - 模型（Model）：数据保存

各部分之间的通信方式如下。



![img](http://www.ruanyifeng.com/blogimg/asset/2015/bg2015020105.png)

**img**



> 1. View 传送指令到 Controller
> 2. Controller 完成业务逻辑后，要求 Model 改变状态
> 3. Model 将新的数据发送到 View，用户得到反馈

所有通信都是单向的。

接受用户指令时，MVC 可以分成两种方式。一种是通过 View 接受指令，传递给 Controller。



![img](http://www.ruanyifeng.com/blogimg/asset/2015/bg2015020106.png)

**img**



另一种是直接通过controller接受指令。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



### MVP

MVP 模式将 Controller 改名为 Presenter，同时改变了通信方向。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



1. 各部分之间的通信，都是双向的。
2. View 与 Model 不发生联系，都通过 Presenter 传递。
3. View 非常薄，不部署任何业务逻辑，称为”被动视图”（Passive View），即没有任何主动性，而 Presenter非常厚，所有逻辑都部署在那里。

### MVVM

MVVM 模式将 Presenter 改名为 ViewModel，基本上与 MVP 模式完全一致。



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



唯一的区别是，它采用双向绑定（data-binding）：View的变动，自动反映在 ViewModel，反之亦然。[Angular](https://angularjs.org/) 和 [Ember](http://emberjs.com/) 都采用这种模式。

低耦合，视图（View）可以独立于Model变化和修改，一个ViewModel可以绑定到不同的”View”上，当View变化的时候Model可以不变，当Model变化的时候View也可以不变。

可重用性，可以把一些视图逻辑放在一个ViewModel里面，让很多view重用这段视图逻辑。

独立开发，开发人员可以专注于业务逻辑和数据的开发（ViewModel），设计人员可以专注于页面设计，使用Expression Blend可以很容易设计界面并生成xml代码。

可测试，界面向来是比较难于测试的，而现在测试可以针对ViewModel来写。

# 智力问题

#### 1.将1000个数装进10个十个抽屉，要求能用不同的抽屉组合出任意1-1000之间的数。

二进制思想，分别放入1，2，4，8，16，32，64，128，256，512

#### 2.一个路口观察十分钟有车经过的概率是99%，那么五分钟能观察到有车经过的概率是多少

设5分钟观察到有车的概率为p，则没有车经过为1−p1−p
$$
2*p*1−p1−p + 1*p^2 = 0.99 → p
$$

#### 3.一个项链三种颜色红白蓝的珠子，从一个地方剪短，从剪断的左边一直取相同颜色的珠子，从右边也取，求最多能取多少个珠子。（白色可以变两种颜色）

Acwing题目1342：

https://www.acwing.com/problem/content/1344/

2倍数组模拟循环结构，两端两种情况计算珠子大小

#### 4.从n大小的数据中等概率取出m个数

**蓄水池抽样算法**

**步骤：**

1.当i <= m时，将其放入蓄水池

2.当i >m的时候，从[1, i]中取随机数d，如果d<=m，则用第i个数代替第d个数据

**证明**：

当i>m的时候，以m+1为例，m+1，会以m/m+1m/m+1的概率进入蓄水池，

替换掉某个数的概率是m/m+1m/m+1 x 1/m1/m=1/m+11/m+1

不被替换的概率m/m+1m/m+1

而m+2个数替换掉第某个数的概率为1/m+21/m+2

第N个数替换掉某个数的概率为1/N 不被替换的概率为 （N - 1 )/N

i<=m的这些数一直留在蓄水池的概率是m/m+1m/m+1 x m+1/m+2m+1/m+2 … x N−1/NN−1/N = m/N

对于i>m的这些数进入蓄水池的概率是m/i，而之后一直留在蓄水池的概率是m/im/i x i/i+1i/i+1 … x N−1/NN−1/N = m/N

**代码：**

```c++
void func(vector vec, int n,int m)
{
    int random;
    srand(time(NULL));//设置利用系统时间来返回随机数
    for (int i = 0; i < m; i++)
    {
        random = (rand() % (n - i)) + i;//产生i~n-1范围的随机数
        swap(vec[i], vec[random]);
        cout << vec[i] << " ";
    }
}
```

## 5.猜帽子颜色：

一群人开舞会，每人头上都戴着一顶帽子。帽子只有黑白两种，黑的至少有一顶。每个人都能看到其它人帽子的颜色，却看不到自己的。主持人先让大家看看别人头上戴的是什幺帽子，然 后关灯，如果有人认为自己戴的是黑帽子，就打自己一个耳光。第一次关灯，没有声音。于是再开灯，大家再看一遍，关灯时仍然鸦雀无声。一直到第三次关灯，才 有劈劈啪啪打耳光的声音响起。问有多少人戴着黑帽子？

### 解析：

若有一顶，以戴黑帽子人的视角：都是白帽，黑帽至少一顶，故我是黑帽。第一次关灯后就会有巴掌声。

若有两顶，以戴黑帽子人的视角：除我之外有一个戴黑帽子的，如果我是白帽子，则第一次关灯后就应该有巴掌声，故我是黑帽。第二次关灯后有巴掌声。

若有三顶，以戴黑帽子人的视角：除我之外有两个人带黑帽子，如果我是白帽，则第二次关灯后就应该有巴掌声，故我是黑帽。第三次关灯后有巴掌声。

## 6.一个人向南走一公里，向东走一公里，向北走一公里回到了原点，地球上有多少这样点：

找到某一个纬度所有点皆是，即纬度圈长度为1公里

北极点也是

## 7.四个队伍循环赛，胜利3分，平各得一分，输不得分，请问最低多少分保证一定出线，最低多少分仍有机会可出现

7分 和 2分

## 8.小明离家有50米，每走一米吃一个苹果，起点有100个苹果，每次最多背50个苹果，请问最多可以拿回家多少苹果？

每走一步少三个苹果，走16步时用了48个苹果，然后剩下52个，直接背起50个苹果，剩下34步吃34个，即剩下16个

# 安卓

## 1.Android打包原理：

1.通过AAPT工具进行资源文件（包括AndroidManifest.xml、布局文件、各种xml资源等）的打包，生成R.java文件。
2.通过AIDL工具处理AIDL文件，生成相应的Java文件。
3.通过Javac工具编译项目源码，生成Class文件。
4.通过DX工具将所有的Class文件转换成DEX文件，该过程主要完成Java字节码转换成Dalvik字节码，压缩常量池以及清除冗余信息等工作。
5.通过ApkBuilder工具将资源文件、DEX文件打包生成APK文件。
6.利用KeyStore对生成的APK文件进行签名。
7.如果是正式版的APK，还会利用ZipAlign工具进行对齐处理，对齐的过程就是将APK文件中所有的资源文件举例文件的起始距离都偏移4字节的整数倍，这样通过内存映射访问APK文件的速度会更快。

## 2.Okhttp内容：

### 1.Okhttp 基本实现原理

> OkHttp 主要是通过 5 个拦截器和 3 个双端队列（2 个异步队列，1 个同步队列）工作。内部实现通过一个责任链模式完成，将网络请求的各个阶段封装到各个链条中，实现了各层的解耦。

> OkHttp 的底层是通过 Socket 发送 HTTP 请求与接受响应，但是 OkHttp 实现了连接池的概念，即对于同一主机的多个请求，可以公用一个 Socket 连接，而不是每次发送完 HTTP 请求就关闭底层的 Socket，这样就实现了连接池的概念。而 OkHttp 对 Socket 的读写操作使用的 OkIo 库进行了一层封装。

执行流程：

- 通过构建者构建出OkHttpClient对象,再通过newCall方法获得RealCall请求对象.
- 通过RealCall发起同步或异步请求,而决定是异步还是同步请求的是由线程分发器dispatcher来决定.
- 当发起同步请求时会将请求加入到同步队列中依次执行,所以会阻塞UI线程,需要开启子线程执行.
- 当发起异步请求时会创建一个线程池,并且判断请求队列是否大于最大请求队列64,请求主机数是否大于5,如果大于请求添加到异步等待队列中,否则添加到异步执行队列,并执行任务.执行流程：

### 2.Okhttp 网络缓存如何实现？

OKHttp 默认只支持 get 请求的缓存。

- 第一次拿到响应后根据头信息决定是否缓存。
- 下次请求时判断是否存在本地缓存，是否需要使用对比缓存、封装请求头信息等等。
- 如果缓存失效或者需要对比缓存则发出网络请求，否则使用本地缓存。

### 3.Okhttp 网络连接怎么实现复用？

HttpEngine 在发起请求之前，会先调用nextConnection来获取一个Connection对象，如果可以从ConnectionPool中获取一个Connection对象，就不会新建，如果无法获取，就会调用createnextConnection来新建一个Connection对象，这就是 Okhttp 多路复用的核心，不像之前的网络框架，无论有没有，都会新建Connection对象。



![image](https://img-blog.csdnimg.cn/20210312101552861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTA2OTAzNA==,size_16,color_FFFFFF,t_70)

**image**



### 4.Dispatcher 的功能是什么？

Dispatcher中文是分发器的意思，和拦截器不同的是分发器不做事件处理，只做事件流向。他负责将每一次Requst进行分发，压栈到自己的线程池，并通过调用者自己不同的方式进行异步和同步处理。 通俗的讲就是主要维护任务队列的作用。

- 记录同步任务、异步任务及等待执行的异步任务。
- 调度线程池管理异步任务。
- 发起/取消网络请求 API：execute、enqueue、cancel。

Dispatcher 类，该类中维护了三个双端队列（Deque） ：

> readyAsyncCalls：准备运行的异步请求
> runningAsyncCalls：正在运行的异步请求
> runningSyncCalls：正在运行的同步请求

OkHttp 设置了默认的最大并发请求量 maxRequests = 64 和单个 Host 主机支持的最大并发量 maxRequestsPerHost = 5

#### 5.Okhttp 拦截器的作用是什么？

1、应用拦截器

拿到的是原始请求，可以添加一些自定义header、通用参数、参数加密、网关接入等等。

- RetryAndFollowUpInterceptor 处理错误重试和重定向
- BridgeInterceptor 应用层和网络层的桥接拦截器，主要工作是为请求添加cookie、添加固定的header，比如Host、Content-Length、Content-Type、User-Agent等等，然后保存响应结果的cookie，如果响应使用gzip压缩过，则还需要进行解压。
- CacheInterceptor 缓存拦截器，如果命中缓存则不会发起网络请求。
- ConnectInterceptor 连接拦截器，内部会维护一个连接池，负责连接复用、创建连接（三次握手等等）、释放连接以及创建连接上的socket流。

2、网络拦截器

用户自定义拦截器，通常用于监控网络层的数据传输。

- CallServerInterceptor 请求拦截器，在前置准备工作完成后，真正发起了网络请求。

#### 6、Okhttp 运用了哪些设计模式？

Okhttp 运用了六种设计模式：

- 构造者模式（OkhttpClient,Request 等各种对象的创建）
- 工厂模式（在 Call 接口中，有一个内部工厂 Factory 接口。）
- 单例模式（Platform 类，已经使用 Okhttp 时使用单例）
- 策略模式（在 CacheInterceptor 中，在响应数据的选择中使用了策略模式，选择缓存数据还是选择网络访问。）
- 责任链模式（拦截器的链式调用）
- 享元模式（Dispatcher 的线程池中，不限量的线程池实现了对象复用）

# 框架

### 1.RPC框架

#### 什么是 RPC ？

> - RPC RemoteProcedureCallRemoteProcedureCall即**远程过程调用**，是分布式系统常见的一种通信方法。它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。
> - 除 RPC 之外，常见的多系统数据交互方案还有分布式消息队列、HTTP 请求调用、数据库和分布式缓存等。
> - 其中 RPC 和 HTTP 调用是没有经过中间件的，它们是端到端系统的直接数据交互。

**简单的说**

- RPC就是从一台机器（客户端）上通过参数传递的方式调用另一台机器（服务器）上的一个函数或方法（可以统称为服务）并得到返回的结果。
- RPC会隐藏底层的通讯细节（不需要直接处理Socket通讯或Http通讯）。
- 客户端发起请求，服务器返回响应（类似于Http的工作方式）RPC在使用形式上像调用本地函数（或方法）一样去调用远程的函数（或方法）。

#### 为什么我们要用RPC?

RPC 的主要目标是让**构建分布式应用更容易**，在提供强大的远程调用能力时不损失本地调用的语义简洁性。为实现该目标，RPC 框架需提供一种透明调用机制让使用者不必显式的区分本地调用和远程调用。

#### **RPC需要解决的三个问题**

> RPC要达到的目标：远程调用时，要能够像本地调用一样方便，让调用者感知不到远程调用的逻辑。

1. **Call ID映射**。我们怎么告诉远程机器我们要**调用哪个函数呢**？在本地调用中，函数体是直接通过函数指针来指定的，我们调用具体函数，编译器就自动帮我们调用它相应的函数指针。但是在远程调用中，是无法调用函数指针的，因为两个进程的地址空间是完全不一样。所以，在RPC中，**所有的函数都必须有自己的一个ID**。这个ID在所有进程中都是唯一确定的。客户端在做远程过程调用时，必须附上这个ID。然后我们还需要在客户端和服务端分别维护一个 {函数 <–> Call ID} 的对应表。两者的表不一定需要完全相同，但相同的函数对应的Call ID必须相同。当客户端需要进行远程调用时，它就查一下这个表，找出相应的Call ID，然后把它传给服务端，服务端也通过查表，来确定客户端需要调用的函数，然后执行相应函数的代码。
2. **序列化和反序列化**。客户端怎么把参数值传给远程的函数呢？在本地调用中，我们只需要把参数压到栈里，然后让函数自己去栈里读就行。但是在远程过程调用时，客户端跟服务端是不同的进程，**不能通过内存来传递参数**。甚至有时候客户端和服务端使用的都**不是同一种语言**（比如服务端用C++，客户端用Java或者Python）。这时候就需要客户端把参数先转成一个字节流，传给服务端后，再把字节流转成自己能读取的格式。这个过程叫序列化和反序列化。同理，从服务端返回的值也需要序列化反序列化的过程。
3. **网络传输**。远程调用往往是基于网络的，客户端和服务端是通过网络连接的。所有的数据都需要通过网络传输，因此就需要有一个网络传输层。网络传输层需要把Call ID和序列化后的参数字节流传给服务端，然后再把序列化后的调用结果传回客户端。只要能完成这两者的，都可以作为传输层使用。因此，它所使用的协议其实是不限的，能完成传输就行。尽管大部分RPC框架都使用TCP协议，但其实UDP也可以，而gRPC干脆就用了HTTP2。Java的Netty也属于这层的东西。

#### **实现高可用RPC框架需要考虑到的问题**

> 要实现一个RPC不算难，难的是实现一个高性能高可靠的RPC框架

1. 既然系统采用分布式架构，那一个服务势必会有多个实例，要解决**如何获取实例的问题**。所以需要一个服务注册中心，比如在Dubbo中，就可以使用Zookeeper作为注册中心，在调用时，从Zookeeper获取服务的实例列表，再从中选择一个进行调用；
2. 如何选择实例呢？就要考虑负载均衡，例如dubbo提供了4种负载均衡策略；
3. 如果每次都去注册中心查询列表，效率很低，那么就要加缓存；
4. 客户端总不能每次调用完都等着服务端返回数据，所以就要支持异步调用；
5. 服务端的接口修改了，老的接口还有人在用，这就需要版本控制；
6. 服务端总不能每次接到请求都马上启动一个线程去处理，于是就需要线程池；

#### 理论结构模型



![img](https://zeyyyy.top/medias/loading.gif#%20eg%20./images/loading.gif)

**img**



image

RPC 服务端通过RpcServer去导出（export）远程接口方法，而客户端通过RpcClient去导入（import）远程接口方法。客户端像调用本地方法一样去调用远程接口方法，**RPC 框架提供接口的代理实现**，实际的调用将委托给代理RpcProxy。代理封装调用信息并将调用转交给RpcInvoker去实际执行。在客户端的RpcInvoker通过连接器RpcConnector去维持与服务端的通道RpcChannel，并使用RpcProtocol执行协议编码（encode）并将编码后的请求消息通过通道发送给服务端。

RPC 服务端接收器RpcAcceptor接收客户端的调用请求，同样使用RpcProtocol执行协议解码（decode）。

解码后的调用信息传递给RpcProcessor去控制处理调用过程，最后再委托调用给RpcInvoker去实际执行并返回调用结果。

| 组 件        | 功 能                                                        |
| :----------- | :----------------------------------------------------------- |
| PrcServer    | 负责导出（export）远程接口                                   |
| RpcClient    | 负责导入（import）远程接口的代理实现                         |
| RpcProxy     | 远程接口的代理实现                                           |
| RpcInvoker   | 客户端：负责编码调用信息和发送调用请求到服务端并等待调用结果返回;服务端：负责调用服务端接口的具体实现并返回调用结果 |
| RpcProtocol  | 负责协议编/解码                                              |
| RpcConnector | 负责维持客户端和服务端的连接通道和发送数据到服务端           |
| RpcAcceptor  | 负责接收客户端请求并返回请求结果                             |
| RpcProcessor | 负责在服务端控制调用过程，包括管理调用线程池、超时时间等     |
| RpcChannel   | 数据传输通道                                                 |
| ————————     | -                                                            |

#### 主流的RPC框架

##### 服务治理型

- **dubbo**：是阿里巴巴公司开源的一个Java高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和 Spring框架无缝集成。dubbo 已经与12年年底停止维护升级。
- **dubbox**：是当当团队基于dubbo升级的一个版本。是一个分布式的服务架构，可直接用于生产环境作为SOA服务框架。[dubbox资源链接](http://dubbo.apache.org/en-us/)
- **motan**：是新浪微博开源的一个Java框架。它诞生的比较晚，起于2013年，2016年5月开源。Motan 在微博平台中已经广泛应用，每天为数百个服务完成近千亿次的调用。[motan资源链接](https://github.com/weibocom/motan/wiki/zh_quickstart)

##### 多语言型

- **gRPC**:是Google开发的高性能、通用的开源RPC框架，其由Google主要面向移动应用开发并基于HTTP/2协议标准而设计，基于ProtoBufProtocolBuffersProtocolBuffers序列化协议开发，且支持众多开发语言。本身它不是分布式的，所以要实现上面的框架的功能需要进一步的开发。[gRPC资源链接](http://doc.oschina.net/grpc?t=60134)
- **thrift**：是Apache的一个跨语言的高性能的服务框架，也得到了广泛的应用。[Thrift文档](http://thrift.apache.org/docs/)

## 2.Protobuf简单内容

序列化serialization、marshallingserialization、marshalling的过程是指将数据结构或者对象的状态转换成可以存储比如文件、内存比如文件、内存或者传输的格式比如网络比如网络。反向操作就是反序列化deserialization、unmarshallingdeserialization、unmarshalling的过程。

## 3.卡尔曼滤波

## 卡尔曼滤波怎么做的？

我们先回顾总结下直观理解中是怎么做的。

1. 根据**上一秒导弹的位置** 和 **导弹的的速度**估计出**当前时刻导弹的位置粗略估计值**。
2. 将**雷达测得导弹位置测量值**和我们**计算出的导弹位置粗略估计值**根据这两种数据可信度来进行线性加权和得到准确的导弹位置估计值**。**

在前面我们也提到了导弹的位置和雷达测量值都是有误差的。所以卡尔曼想用概率来衡量数据的可信度。

比如：雷达测量的数据它就不只是一个数字了。而是说测量发现导弹有0.8的概率在7m那个位置，有0.1的概率在7.2m那个位置。有0.1的概率在6.9m那个位置。这些数据就叫做概率分布。概率分布的意思就是很多个值还有它们各自出现的概率多大所组成的数据就叫做概率分布。

**卡尔曼认为导弹速度、导弹位置、雷达测距的测量值这些都服从正态分布（对就是高中学的那个正态分布）**。

将几个概率分布通过一些方法相乘相乘合成一个新的正态分布来计算

# 4.分布式框架系列：

https://zhuanlan.zhihu.com/p/147691411

https://blog.csdn.net/qq_22172133/article/details/104459452

# 项目整理

## 1.智能看护项目：

1.预定义函数指针类型，并定义函数指针数组，可根据功能号来调用不同功能函数

2.自定义协议函数来解决TCP粘包问题

3.使用socket短链接，因为没有大规模提取

4.数据库使用Mysql，因为考虑未来的多用户使用，并且大面积写入

5.优化：线程池等

## 2.跌倒检测系统算法研究

- 各种滤波算法：小波滤波，一阶滤波等

- 图像识别算法：C3D

  ##### **3.1 一般概念**

  由于视频只是比图像多了一个时间维度，所以一个比较直接的思路即是将时间维度和空间维度一起卷积，即使用更高维的卷积核与更高维的池化算子来构建卷积网络。对于传统RGB图像的卷积，包含通道数在内其输入维度是3维，卷积核是4维，输出维度是3维，如下图所示：

  

  ![img](https://pic3.zhimg.com/80/v2-f86345f82e3f3de15a65f0da25e538ca_720w.jpg)

  **img**

  图1 2D CNN 示意图

  

  如果输入向量是[112, 112, 3]，分别代表[图像宽、图像高、输入通道数]，卷积核是[3, 3, 3, 2]，分别代表[卷积核宽、卷积核高、输入通道数、输出通道数]；，那么输出向量是[112, 112, 2]，分别代表[feature map宽、feature map高、输出通道数]，其中输出通道数等同于卷积核个数。

  对于视频的卷积，从公式上来说也是加了一层卷积运算，则输入维度是4维，卷积核是5维，输出维度是4维，如下图所示

  

  ![img](https://pic4.zhimg.com/80/v2-e460986607c750b87d11cbf5a4e6749f_720w.jpg)

  **img**

  图2 3D CNN示意图

  

  即输入向量是[16, 112, 112, 3]，分别代表[视频帧数时域深度时域深度、视频图像宽、视频图像高、输入通道数]，卷积核是[3, 3, 3, 3, 64]，分别代表[卷积核深、卷积核宽、卷积核高、输入通道数、输出通道数]，则输出向量是[16, 112, 112, 64]，分别代表[feature map深、feature map宽、feature map高、输出通道数]，其中输出通道数等同于卷积核个数。以下方法都是基于这种思路展开的。

  （注：图中每个frame都有RGB 3通道，通道不展开画）

  ##### **3.2 C3D**

  ###### 3.2.1 算法原理

  C3D的算法来源于论文《Learning spatiotemporal features with 3d convolutional networks.》，关于算法介绍可以参见[论文笔记——基于的视频行为识别/动作识别算法笔记三三](https://zhuanlan.zhihu.com/p/41659502)。

  ###### 3.2.2 算法网络结构

  

  ![img](https://pic2.zhimg.com/80/v2-c14cd63d37cbea049e88bba96a70b671_720w.jpg)

  **img**

  图3 C3D网络

  

## 3.拼图小游戏

- 切图：将一个图片根据等级进行切图，可采用sort函数来实现乱序，也要记得对图片进行压缩

  ```java
  public int compare(ImagePiece a, ImagePiece b) {
      return Math.random() > 0.5 ? 1 : -1;// 返回一个随机数
  }
  ```

- 错误情况的排除，自定义一个check函数来检测当前拼图是否有解

  方法原理为：

  设两个矩阵A和B。将矩阵从左到右，从上到下排成一个一维数组，设其逆序对的个数加上空白格在原矩阵所在的行列号之和P。若PAA与PBB的奇偶性相同，则两个矩阵可以通过拼图游戏进行转换。因此只要计算当前矩阵和正确矩阵的P值判断一下即可。

- 手势交互：

  - 一种是看附近有没有空白块，有则移动
  - 另外一种是选择两个图片交换

- 判断成功条件：

  会记录拼图的初始条件（例如顺序），然后观察最后的位置是否和原相同，相同则成功

- 当点击图片时，隐藏点击的图片并记录，然后生成动画层，在动画层上生成大小位置一样的图片，然后在动画层上实现图片交换的效果。
  动画完成之后可以在onAnimationEnd中隐藏动画层，移除掉动画层中的ImageView，将记录好的两个ImageView一些属性的交换，比如说bitmap的交换，index的交换。
  注意：效果看起来像两个ImageView互换了位置，实际上只是bitmap相互替换了。普通模式中的空白图片默认就是记录好的一张图片。

- Handler机制：Handler是安卓提供的一种消息机制。通常用于接受子线程发送的数据,并用此数据配合主线程更新UI。可用来记录时间

- 自定义图片的方式：在工程中存储一些图片，或者在SD卡获取权限或者获取拍照权限来实现。